<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>王敏的博客</title>
  <subtitle>按照自己的方式去度过人生</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ynuwm.github.io/"/>
  <updated>2017-12-01T09:03:47.000Z</updated>
  <id>http://ynuwm.github.io/</id>
  
  <author>
    <name>王敏</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>把信送给加西亚</title>
    <link href="http://ynuwm.github.io/2017/11/24/%E6%8A%8A%E4%BF%A1%E9%80%81%E7%BB%99%E5%8A%A0%E8%A5%BF%E4%BA%9A/"/>
    <id>http://ynuwm.github.io/2017/11/24/把信送给加西亚/</id>
    <published>2017-11-24T12:25:17.000Z</published>
    <updated>2017-12-01T09:03:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>在所有与古巴有关的事情中，有一个人常常令我无法忘怀。<br>美西战争爆发以后，美国必须马上与西班牙反抗军首领加西亚将军取得联系。加西亚将军隐藏在古巴辽阔的崇山峻岭中——没有人知道确切的地点，因而无法送信给他。但是，美国总统必须尽快地与他建立合作关系。怎么办呢？<br>有人对总统推荐说：“有一个名叫罗文的人，如果有人能找到加西亚将军，那个人一定就是他。”<br>于是，他们将罗文找来，交给他一封信——写给加西亚的信。关于那个名叫罗文的人，如何拿了信，将它装进一个油纸袋里，打封，吊在胸口藏好，如何在3个星期之后，徒步穿越一个危机四伏的国家，将信交到加西亚手上——这些细节都不是我想说明的，我要强调的重点是：<br>美国总统将一封写给加西亚的信交给了罗文，罗文接过信后，并没有问：“他在哪里？”</p>
<p>喜欢原文中这句话：年轻人所需要的不只是学习书本上的知识，也不只是聆听他人种种的指导，而是更需要一种敬业精神，对上级的托付，立即采取行动，全心全意去完成任务——把信送给加西亚。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在所有与古巴有关的事情中，有一个人常常令我无法忘怀。&lt;br&gt;美西战争爆发以后，美国必须马上与西班牙反抗军首领加西亚将军取得联系。加西亚将军隐藏在古巴辽阔的崇山峻岭中——没有人知道确切的地点，因而无法送信给他。但是，美国总统必须尽快地与他建立合作关系。怎么办呢？&lt;br&gt;有人对
    
    </summary>
    
      <category term="读书笔记" scheme="http://ynuwm.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="职场" scheme="http://ynuwm.github.io/tags/%E8%81%8C%E5%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>南航求职经历</title>
    <link href="http://ynuwm.github.io/2017/11/24/%E5%8D%97%E8%88%AA%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/"/>
    <id>http://ynuwm.github.io/2017/11/24/南航面试经历/</id>
    <published>2017-11-24T08:33:01.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>我是3,4月份投的简历，那个时候来软院宣讲，研究生很少，去的基本是软院大三的应届生，估计的话收的简历有三四十份。那次说的是要招暑假实习生，但是投完简历就没消息了。直到10月份，突然接到一个广州的电话，但是我没接到，打过去一直在忙，就没怎么管。十月份自己脚崴了，一直在休息，拄着拐杖。10月22号那周的周三也就是10月18号突然收到短信，说我的简历通过了，叫我参加笔试，叫我准备一下，考试时间就是10月22（周日），准备个屁啊，时间太紧我直接裸考的，上学期投完简历我是有买托业资料，做了一段时间没收到消息也就没做了。</p>
<p>说说笔试吧。21号的晚上就去了市区，拄着拐杖太不方便了，地上是湿的刚刚下完雨。在小菜园立交桥那个地方就迷路了，道路太复杂，以前又没怎么去过，边打听边走。走了一个多小时才找到住的地方，累的全身都出汗了，想着就去打打酱油吧，本来就没准备。第二天考试就下大雨了，比较幸运是等我到考点后下起来的。考试的话，30个人只去了13个。行测比较坑，上面显示的是38道题，等提交的时候发现后面还有题，大家好像都没做。英语很简单，偏应用，听力也简单，提前十多分钟做完了。考完直接回来了。然后就等通知。</p>
<p>10月30号收到面试通知说11月1号要过去面试，再次打电话确认我参加不，这次电话又没有接到，不过我打过去了说参加的。然后就是面试，我是6号，一共有9个通过了笔试，其中一个没来，也就是只剩8个，有两个女生报的行销，跟我们不是一个岗位，剩下6个全是男生都是信息开发岗。面试分两轮：第一轮小组讨论，全部6个人坐在一起，讨论15分钟，最后派一个代表总结。抽到的题目是做一个网页系统，同时要面对海量用户，要综合考虑数据的准确性和及时性，问要用到哪些技术。其实面试官这里是不太管你们讨论出什么结果的，他们看的是你的思路，你在小组里扮演的角色。我自己的话，就没说很多话，但是每次我说的都是他们讨论之外的东西，<br>他们讨论热烈的时候，我就说海量用户，那用户数据安全性怎么设计，并发执行的话我们有哪些要注意的等等。 然后小组派一个人总结了。<br>第二轮，多对一面试，有三个面试官，一个是技术，一个是行销的，另一个是其他的。在我前面面完的人，有一个出来我们就问他都问了哪些问题，他说数据结构什么的，最后他说还讲了毕业设计。我当时想这哥们应该没戏了，都研究生了还在讲毕业设计，不显得很low吗，而且面试你给人家讲毕设不觉得这个很普遍吗，研究生阶段很多东西可以讲啊，局限在毕设在我看来已经失败了。刚进去做自我介绍，问我的第一个问题是我的两篇论文，我讲了很多估计他们也不太懂，神经网络LSTM什么的。接着问我说进去之后要用Java我说学过，他说你现在Python用的多，这两个语言的区别，我又balala说了一堆。第三个问题我是想继续读还是出来工作，我想都没想说不想读书了<br>我要工作。好像就没问别的问题了，另外两个人也没有问，当时真的很慌，面试我的时间我觉得是最短的，而且另外两个都没问问题这就完了？然后就出来了。对了，其他7个人都穿正装了，就我没穿，短信上没有说，招聘官网说要穿，由于没有懒得借就没穿。对了笔试的时候碰到了同班同学，他们是最近一个月投的简历。</p>
<p>11月9号收到MAP职业性格测试。认识一起面试的两个，应该是所有面试的都收到了这个测试。</p>
<p>11月22收到Offer。那个说毕设的哥们跟我预测的一样，没收到短信。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我是3,4月份投的简历，那个时候来软院宣讲，研究生很少，去的基本是软院大三的应届生，估计的话收的简历有三四十份。那次说的是要招暑假实习生，但是投完简历就没消息了。直到10月份，突然接到一个广州的电话，但是我没接到，打过去一直在忙，就没怎么管。十月份自己脚崴了，一直在休息，拄
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="面试" scheme="http://ynuwm.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>SemEval2017-Task4前几名思路与技巧</title>
    <link href="http://ynuwm.github.io/2017/11/18/SemEval2017%E5%89%8D%E5%87%A0%E5%90%8D%E6%80%9D%E8%B7%AF%E4%B8%8E%E6%8A%80%E5%B7%A7/"/>
    <id>http://ynuwm.github.io/2017/11/18/SemEval2017前几名思路与技巧/</id>
    <published>2017-11-18T07:14:50.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="No-1-BB-twtr"><a href="#No-1-BB-twtr" class="headerlink" title="No.1 BB_twtr"></a>No.1 BB_twtr</h2><p>第一，预处理<br>url to ‘url’<br>emotions to ‘smile’,’sadness’…<br>‘sooooo’ to ‘soo’<br>lowercased</p>
<p>第二，100million unlabeled tweets 预训练词向量(Twitter API)</p>
<p>第三，模型(参考论文Ye Zhang and Byron Wallace,2015)<br>卷积核大小２,3,4，每种有２个，对同一个句子卷积，得到６个univariate vectors,然后concat在一起，再经全连接层和softmax层分类<br>LSTM 类似的处理<br>第四，数据<br>Task-A(49693　labeled)<br>Task-BD(30849)<br>Task-CE(18948)</p>
<p>第五，ensemble<br>10 CNNs and 10 LSTMs together through soft voting</p>
<h2 id="No-2-DataStories"><a href="#No-2-DataStories" class="headerlink" title="No.2 DataStories"></a>No.2 DataStories</h2><p>第一，自己写的文本分词器<br>第二，TaskA两层双向LSTM+Att(基于message)<br>第三，TaskBCDE句子和话题分别通过BiLSTM然后concat+att-context(基于话题)</p>
<h2 id="No-3-LIA"><a href="#No-3-LIA" class="headerlink" title="No.3 LIA"></a>No.3 LIA</h2><p>ensemble CNN 和 LSTM<br>第一，Word Embedding<br>Lexical embedding<br>Sentiment embeddings(Multitask-learning)<br>Sentiment embeddings(distant-supervision)<br>Sentiment embeddings(negative-sampling)</p>
<p>第二，句子层特征提取<br>Lexicons:MPQA+NRC<br>Emoticons:number of emoticons grouped in pos,neg,neu<br>All-caps:number of words in all-caps<br>Elongated units:words in which characters are repeated more than wtice(eg,looooool)<br>Punctuation:number of contiguous sequences of severl periods.exclaimation marks and question marks</p>
<h2 id="No-4-Senti17"><a href="#No-4-Senti17" class="headerlink" title="No.4 Senti17"></a>No.4 Senti17</h2><p>第一，HappyTokenizer 处理文本<br>第二，十个卷积网络投票，每个网络训练数据一样，词向量一样，不同的是初始权重</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;No-1-BB-twtr&quot;&gt;&lt;a href=&quot;#No-1-BB-twtr&quot; class=&quot;headerlink&quot; title=&quot;No.1 BB_twtr&quot;&gt;&lt;/a&gt;No.1 BB_twtr&lt;/h2&gt;&lt;p&gt;第一，预处理&lt;br&gt;url to ‘url’&lt;br&gt;emot
    
    </summary>
    
      <category term="深度学习" scheme="http://ynuwm.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="semeval" scheme="http://ynuwm.github.io/tags/semeval/"/>
    
  </entry>
  
  <entry>
    <title>这几天的云大</title>
    <link href="http://ynuwm.github.io/2017/11/15/%E8%BF%99%E5%87%A0%E5%A4%A9%E7%9A%84%E4%BA%91%E5%A4%A7/"/>
    <id>http://ynuwm.github.io/2017/11/15/这几天的云大/</id>
    <published>2017-11-15T04:14:44.000Z</published>
    <updated>2017-12-04T12:37:08.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2017/11/15/这几天的云大/psb1.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb2.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb3.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb4.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb5.jpg" alt="img"><br><img src="/2017/11/15/这几天的云大/psb6.jpg" alt="img"> </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2017/11/15/这几天的云大/psb1.jpg&quot; alt=&quot;img&quot;&gt;&lt;br&gt;&lt;img src=&quot;/2017/11/15/这几天的云大/psb2.jpg&quot; alt=&quot;img&quot;&gt;&lt;br&gt;&lt;img src=&quot;/2017/11/15/这几天的云大/ps
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="照片" scheme="http://ynuwm.github.io/tags/%E7%85%A7%E7%89%87/"/>
    
      <category term="云大" scheme="http://ynuwm.github.io/tags/%E4%BA%91%E5%A4%A7/"/>
    
  </entry>
  
  <entry>
    <title>综述自然语言处理NLP</title>
    <link href="http://ynuwm.github.io/2017/11/15/%E7%BB%BC%E8%BF%B0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP/"/>
    <id>http://ynuwm.github.io/2017/11/15/综述自然语言处理NLP/</id>
    <published>2017-11-15T02:42:33.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>前言</p>
<p>自然语言处理是文本挖掘的研究领域之一，是人工智能和语言学领域的分支学科。在此领域中探讨如何处理及运用自然语言。</p>
<p>对于自然语言处理的发展历程，可以从哲学中的经验主义和理性主义说起。基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处理是哲学中的理性主义。在哲学领域中经验主义与理性主义的斗争一直是此消彼长，这种矛盾与斗争也反映在具体科学上，如自然语言处理。</p>
<p>早期的自然语言处理具有鲜明的经验主义色彩。如 1913 年马尔科夫提出马尔科夫随机过程与马尔科夫模型的基础就是“手工查频”，具体说就是统计了《欧根·奥涅金》长诗中元音与辅音出现的频度；1948 年香农把离散马尔科夫的概率模型应用于语言的自动机，同时采用手工方法统计英语字母的频率。</p>
<p>然而这种经验主义到了乔姆斯基时出现了转变。</p>
<p>1956 年乔姆斯基借鉴香农的工作，把有限状态机用作刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型。这些工作非常伟大，为自然语言和形式语言找到了一种统一的数学描述理论，一个叫做“形式语言理论”的新领域诞生了。这个时代，“经验主义”被全盘否定，“理性主义”算是完胜。</p>
<p>然而在 20 世纪 50 年代末到 60 年代中期，经验主义东山再起了。多数学者普遍认为只有详尽的历史语料才能带来靠谱的结论。于是一些比较著名的理论与算法就诞生了，如贝叶斯方法（Bayesian Method）、隐马尔可夫、最大熵、Viterbi 算法、支持向量机之类。世界上第一个联机语料库也是在那个时候的 Brown University 诞生的。</p>
<p>但是总的来说，这个时代依然是基于规则的理性主义的天下，经验主义虽然取得了不俗的成就，却依然没有受到太大的重视。但是金子总会发光的。</p>
<p>90 年代以来，基于统计的自然语言处理就开始大放异彩了。首先是在机器翻译领域取得了突破，因为引入了许多基于语料库的方法（哈钦斯，英国著名学者）。1990 年在芬兰赫尔辛基举办的第 13 届国际计算语言学会议确定的主题是“处理大规模真实文本的理论、方法与工具”，大家的重心开始转向大规模真实文本了，传统的仅仅基于规则的自然语言处理显然力不从心了。学者们认为，大规模语料至少是对基于规则方法有效的补充。</p>
<p>到了 1994~1999 年，经验主义就开始空前繁荣了。如句法剖析、词类标注、参照消解、话语处理的算法几乎把“概率”与“数据”作为标准方法，成为了自然语言处理的主流。 </p>
<p>总之，理性主义在自然语言处理的发展史上是有重要地位的，也辉煌了几十年，历史事物常常是此消彼长的，至于谁好谁坏，不是固定的，取决于不同时代的不同历史任务。总的来说，基于规则的理性主义在这个时代被提及得比较少，用的也比较少，主要是由于以下几个缺陷：</p>
<p>• 鲁棒性差，过于严格的规则导致对非本质错误的零容忍（这一点在最近的一些新的剖析技术上有所改善）；</p>
<p>• 研究强度大，泛化能力差。一个研究要语言学家、语音学家和各种领域的专家配合，在当前大规模文本处理的时间、资源要求下太不划算。且机器学习的方法很难应用，难以普及；</p>
<p>• 实践性差。基于统计的经验主义方法可以根据数据集不断对参数进行优化，而基于规则的方法就不可以，这在当前数据量巨大的情况下，影响是致命的，因为前者常常可以通过增大训练集来获得更好的效果，后者则死板许多，结果往往不尽人意。</p>
<p>但理性主义还是有很多优点的，同样经验主义也有很多缺陷，算是各有所长、各有所短。不同学科有不同学科的研究角度，只能说某些角度在某个特定的历史时期对提高生产力“更有用”，所以重视的人更多。但“有用”不代表胜利，暂时的“无用”更不能说是科学层面上的“失败”。尤其是在当前中文自然语言处理发展还不甚成熟的时期，私以为基于统计的方法在很多方面并不完美，“理性主义”的作用空间还很大，需要更多的人去关注、助力。<br>——《统计自然语言处理》宗成庆</p>
<p>自然语言处理涉及的范畴如下（维基百科）：</p>
<p>• 中文自动分词（Chinese word segmentation）<br>• 词性标注（Part-of-speech tagging）<br>• 句法分析（Parsing）<br>• 自然语言生成（Natural language generation）<br>• 文本分类（Text categorization）<br>• 信息检索（Information retrieval）<br>• 信息抽取（Information extraction）<br>• 文字校对（Text-proofing）<br>• 问答系统（Question answering）<br>• 机器翻译（Machine translation）<br>• 自动摘要（Automatic summarization）</p>
<p>本文针对其中几个主要领域的研究现状和进展，通过论文、博客等资料，结合自身的学习和实践经历进行浅显地介绍。由于个人实践经验不足，除中文分词、自动文摘、文本分类、情感分析和话题模型方面进行过实际业务的实践，其他方面经验欠缺，若有不当之处，欢迎童鞋们批评指正！</p>
<p>目录</p>
<p><img src="/2017/11/15/综述自然语言处理NLP/000.png" alt="img"></p>
<p>一. 中文分词</p>
<p>中文分词主要包括词的歧义切分和未登录词识别，主要可以分为基于词典和基于统计的方法，最新的方法是多种方法的混合。从目前汉语分词研究的总体水平看，F1 值已经达到 95% 左右，主要分词错误是由新词造成的，尤其对领域的适应性较差。下面主要介绍一下中文分词存在的主要问题和分词方法。</p>
<ol>
<li>问题</li>
</ol>
<p>1.1 歧义切分</p>
<p>切分歧义处理包括两部分内容：</p>
<p>• 切分歧义的检测；</p>
<p>• 切分歧义的消解。</p>
<p>这两部分在逻辑关系上可分成两个相对独立的步骤。</p>
<p>• 切分歧义的检测。“最大匹配法”（精确的说法应该叫“最长词优先匹配法”） 是最早出现、同时也是最基本的汉语自动分词方法。依扫描句子的方向，又分正向最大匹配 MM（从左向右）和逆向最大匹配 RMM（从右向左）两种。</p>
<p>最大匹配法实际上将切分歧义检测与消解这两个过程合二为一，对输入句子给出唯一的切分可能性，并以之为解。从最大匹配法出发导出了“双向最大匹配法”，即 MM＋ RMM。双向最大匹配法存在着切分歧义检测盲区。</p>
<p>针对切分歧义检测，另外两个有价值的工作是“最少分词法”，这种方法歧义检测能力较双向最大匹配法要强些，产生的可能切分个数仅略有增加；和“全切分法”，这种方法穷举所有可能的切分，实现了无盲区的切分歧义检测，但代价是导致大量的切分“垃圾”。</p>
<p>• 切分歧义的消解。典型的方法包括句法统计和基于记忆的模型。句法统计将自动分词和基于 Markov 链的词性自动标注技术结合起来，利用从人工标注语料库中提取出的词性二元统计规律来消解切分歧义，基于记忆的模型对伪歧义型高频交集型歧义切分，可以把它们的正确（唯一）切分形式预先记录在一张表中，其歧义消解通过直接查表即可实现。</p>
<p>1.2 未登录词识别</p>
<p>未登录词大致包含两大类：</p>
<p>• 新涌现的通用词或专业术语等；</p>
<p>• 专有名词。如中国人名、外国译名、地名、机构名（泛指机关、团体和其它企事业单位）等。</p>
<p>前一种未登录词理论上是可预期的，能够人工预先添加到词表中（但这也只是理想状态，在真实环境下并不易做到）；后一种未登录词则完全不可预期，无论词表多么庞大，也无法囊括。</p>
<p>真实文本中（即便是大众通用领域），未登录词对分词精度的影响超过了歧义切分。未登录词处理在实用型分词系统中占的份量举足轻重。</p>
<p>• 新涌现的通用词或专业术语。对这类未登录词的处理，一般是在大规模语料库的支持下，先由机器根据某种算法自动生成一张候选词表（无监督的机器学习策略），再人工筛选出其中的新词并补充到词表中。</p>
<p>鉴于经过精加工的千万字、甚至亿字级的汉语分词语料库目前还是水月镜花，所以这个方向上现有的研究无一不以从极大规模生语料库中提炼出的 n 元汉字串之分布（n≥2）为基础。其中汉字之间的结合力通过全局统计量包括互信息、t- 测试差、卡方统计量、字串频等来表示。</p>
<p>• 专有名词。对专有名词的未登录词的处理，首先依据从各类专有名词库中总结出的统计知识 （如姓氏用字及其频度）和人工归纳出的专有名词的某些结构规则，在输入句子中猜测可能成为专有名词的汉字串并给出其置信度，之后利用对该类专有名词有标识意义的紧邻上下文信息（如称谓），以及全局统计量和局部统计量（局部统计量是相对全局统计量而言的，是指从当前文章得到且其有效范围一般仅限于该文章的统计量，通常为字串频），进行进一步的鉴定。</p>
<p>已有的工作涉及了四种常见的专有名词：中国人名的识别、外国译名的识别、中国地名的识别及机构名的识别。</p>
<p>从各家报告的实验结果来看，外国译名的识别效果最好，中国人名次之，中国地名再次之，机构名最差。而任务本身的难度实质上也是遵循这个顺序由小增大。 沈达阳、孙茂松等（1997b）特别强调了局部统计量在未登录词处理中的价值。</p>
<ol>
<li>方法</li>
</ol>
<p>2.1 基于词典的方法</p>
<p>在基于词典的方法中，对于给定的词，只有词典中存在的词语能够被识别，其中最受欢迎的方法是最大匹配法（MM），这种方法的效果取决于词典的覆盖度，因此随着新词不断出现，这种方法存在明显的缺点。</p>
<p>2.2 基于统计的方法</p>
<p>基于统计的方法由于使用了概率或评分机制而非词典对文本进行分词而被广泛应用。这种方法主要有三个缺点：</p>
<p>一是这种方法只能识别 OOV（out-of-vocabulary）词而不能识别词的类型，比如只能识别为一串字符串而不能识别出是人名；二是统计方法很难将语言知识融入分词系统，因此对于不符合语言规范的结果需要额外的人工解析；三是在许多现在分词系统中，OOV 词识别通常独立于分词过程。</p>
<p>二. 词性标注</p>
<p>词性标注是指为给定句子中的每个词赋予正确的词法标记，给定一个切好词的句子，词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记（part-of-speech tag），比如，名词（noun）、动词（verb）、形容词（adjective）等。</p>
<p>它是自然语言处理中重要的和基础的研究课题之一，也是其他许多智能信息处理技术的基础，已被广泛的应用于机器翻译、文字识别、语音识别和信息检索等领域。</p>
<p>词性标注对于后续的自然语言处理工作是一个非常有用的预处理过程，它的准确程度将直接影响到后续的一系列分析处理任务的效果。 </p>
<p>长期以来，兼类词的词性歧义消解和未知词的词性识别一直是词性标注领域需要解决的热点问题。当兼类词的词性歧义消解变得困难时，词性的标注就出现了不确定性的问题。而对那些超出了词典收录范围的词语或者新涌现的词语的词性推测，也是一个完整的标注系统所应具备的能力。</p>
<ol>
<li>词性标注方法</li>
</ol>
<p>词性标注是一个非常典型的序列标注问题。最初采用的方法是隐马尔科夫生成式模型， 然后是判别式的最大熵模型、支持向量机模型，目前学术界通常采用结构感知器模型和条件随机场模型。</p>
<p>近年来，随着深度学习技术的发展，研究者们也提出了很多有效的基于深层神经网络的词性标注方法。</p>
<p>迄今为止，词性标注主要分为基于规则的和基于统计的方法。</p>
<p>• 规则方法能准确地描述词性搭配之间的确定现象，但是规则的语言覆盖面有限，庞大的规则库的编写和维护工作则显得过于繁重，并且规则之间的优先级和冲突问题也不容易得到满意的解决。</p>
<p>• 统计方法从宏观上考虑了词性之间的依存关系，可以覆盖大部分的语言现象，整体上具有较高的正确率和稳定性，不过其对词性搭配确定现象的描述精度却不如规则方法。</p>
<p>针对这样的情况，如何更好地结合利用统计方法和规则处理手段，使词性标注任务既能够有效地利用语言学家总结的语言规则，又可以充分地发挥统计处理的优势成为了词性标注研究的焦点。</p>
<ol>
<li>词性标注研究进展</li>
</ol>
<p>• 词性标注和句法分析联合建模：研究者们发现，由于词性标注和句法分析紧密相关，词性标注和句法分析联合建模可以同时显著提高两个任务准确率。</p>
<p>• 异构数据融合：汉语数据目前存在多个人工标注数据，然而不同数据遵守不同的标注规范，因此称为多源异构数据。近年来，学者们就如何利用多源异构数据提高模型准确率，提出了很多有效的方法，如基于指导特征的方法、基于双序列标注的方法、以及基于神经网络共享表示的方法。</p>
<p>• 基于深度学习的方法：传统词性标注方法的特征抽取过程主要是将固定上下文窗口的词进行人工组合，而深度学习方法能够自动利用非线性激活函数完成这一目标。进一步，如果结合循环神经网络如双向 LSTM，则抽取到的信息不再受到固定窗口的约束，而是考虑整个句子。</p>
<p>除此之外，深度学习的另一个优势是初始词向量输入本身已经刻画了词语之间的相似度信息，这对词性标注非常重要。</p>
<p>三. 句法分析</p>
<p>语言语法的研究有非常悠久的历史，可以追溯到公元前语言学家的研究。不同类型的句法分析体现在句法结构的表示形式不同，实现过程的复杂程度也有所不同。因此，科研人员采用不同的方法构建符合各个语法特点的句法分析系统。其主要分类如下图所示：</p>
<p><img src="/2017/11/15/综述自然语言处理NLP/00１.png" alt="img"></p>
<p>下文主要对句法分析技术方法和研究现状进行总结分析：</p>
<ol>
<li>依存句法分析</li>
</ol>
<p>依存语法存在一个共同的基本假设：句法结构本质上包含词和词之间的依存（修饰）关系。一个依存关系连接两个词，分别是核心词（head）和依存词（dependent）。依存关系可以细分为不同的类型，表示两个词之间的具体句法关系。</p>
<p>目前研究主要集中在数据驱动的依存句法分析方法，即在训练实例集合上学习得到依存句法分析器，而不涉及依存语法理论的研究。数据驱动的方法的主要优势在于给定较大规模的训练数据，不需要过多的人工干预，就可以得到比较好的模型。因此，这类方法很容易应用到新领域和新语言环境。</p>
<p>数据驱动的依存句法分析方法主要有两种主流方法：基于图（ graph-based）的分析方法和基于转移（ transition-based）的分析方法。</p>
<p>2.1 基于图的依存句法分析方法</p>
<p>基于图的方法将依存句法分析问题看成从完全有向图中寻找最大生成树的问题。一棵依存树的分值由构成依存树的几种子树的分值累加得到。</p>
<p>根据依存树分值中包含的子树的复杂度，基于图的依存分析模型可以简单区分为一阶和高阶模型。高阶模型可以使用更加复杂的子树特征，因此分析准确率更高，但是解码算法的效率也会下降。</p>
<p>基于图的方法通常采用基于动态规划的解码算法，也有一些学者采用柱搜索（beam search）来提高效率。学习特征权重时，通常采用在线训练算法，如平均感知器（averaged perceptron）。</p>
<p>2.2 基于转移的依存句法分析方法</p>
<p>基于转移的方法将依存树的构成过程建模为一个动作序列，将依存分析问题转化为寻找最优动作序列的问题。早期，研究者们使用局部分类器（如支持向量机等）决定下一个动作。近年来，研究者们采用全局线性模型来决定下一个动作，一个依存树的分值由其对应的动作序列中每一个动作的分值累加得到。</p>
<p>特征表示方面，基于转移的方法可以充分利用已形成的子树信息，从而形成丰富的特征，以指导模型决策下一个动作。模型通过贪心搜索或者柱搜索等解码算法找到近似最优的依存树。和基于图的方法类似，基于转移的方法通常也采用在线训练算法学习特征权重。</p>
<p>2.3 多模型融合的依存句法分析方法</p>
<p>基于图和基于转移的方法从不同的角度解决问题，各有优势。基于图的模型进行全局搜索但只能利用有限的子树特征，而基于转移的模型搜索空间有限但可以充分利用已构成的子树信息构成丰富的特征。详细比较发现，这两种方法存在不同的错误分布。</p>
<p>因此，研究者们使用不同的方法融合两种模型的优势，常见的方法有：stacked learning；对多个模型的结果加权后重新解码（re-parsing）；从训练语料中多次抽样训练多个模型（bagging）。 </p>
<ol>
<li>短语结构句法分析</li>
</ol>
<p>分词，词性标注技术一般只需对句子的局部范围进行分析处理，目前已经基本成熟，其标志就是它们已经被成功地用于文本检索、文本分类、信息抽取等应用之中，而句法分析、语义分析技术需要对句子进行全局分析，目前，深层的语言分析技术还没有达到完全实用的程度。</p>
<p>短语结构句法分析的研究基于上下文无关文法（Context Free Grammar，CFG）。上下文无关文法可以定义为四元组，其中 T 表示终结符的集合（即词的集合），N 表示非终结符的集合（即文法标注和词性标记的集合），S 表示充当句法树根节点的特殊非终结符，而 R 表示文法规则的集合，其中每条文法规则可以表示为 Ni®g ，这里的 g 表示由非终结符与终结符组成的一个序列（允许为空）。</p>
<p>根据文法规则的来源不同，句法分析器的构建方法总体来说可以分为两大类：</p>
<p>• 人工书写规则</p>
<p>• 从数据中自动学习规则</p>
<p>人工书写规则受限于规则集合的规模：随着书写的规则数量的增多，规则与规则之间的冲突加剧，从而导致继续添加规则变得困难。</p>
<p>与人工书写规模相比，自动学习规则的方法由于开发周期短和系统健壮性强等特点，加上大规模人工标注数据，比如宾州大学的多语种树库的推动作用，已经成为句法分析中的主流方法。</p>
<p>而数据驱动的方法又推动了统计方法在句法分析领域中的大量应用。为了在句法分析中引入统计信息，需要将上下文无关文法扩展成为概率上下文无关文法（Probabilistic Context Free Grammar，PCFG），即为每条文法规则指定概率值。</p>
<p>概率上下文无关文法与非概率化的上下文无关文法相同，仍然表示为四元组，区别在于概率上下文无关文法中的文法规则必须带有概率值。</p>
<p>获得概率上下文无关文法的最简单的方法是直接从树库中读取规则，利用最大似然估计（Maximum Likelihood Estimation，MLE）计算得到每条规则的概率值。使用该方法得到的文法可以称为简单概率上下文无关文法。在解码阶段，CKY 10 等解码算法就可以利用学习得到的概率上下文无关文法搜索最优句法树。</p>
<p>虽然基于简单概率上下文无关文法的句法分析器的实现比较简单，但是这类分析器的性能并不能让人满意。</p>
<p>性能不佳的主要原因在于上下文无关文法采取的独立性假设过强：一条文法规则的选择只与该规则左侧的非终结符有关，而与任何其它上下文信息无关。文法中缺乏其它信息用于规则选择的消歧。因此后继研究工作的出发点大都基于如何弱化上下文无关文法中的隐含独立性假设。</p>
<ol>
<li>总结</li>
</ol>
<p>分词，词性标注技术一般只需对句子的局部范围进行分析处理，目前已经基本成熟，其标志就是它们已经被成功地用于文本检索、文本分类、信息抽取等应用之中，而句法分析、语义分析技术需要对句子进行全局分析，目前，深层的语言分析技术还没有达到完全实用的程度。</p>
<p>四. 文本分类</p>
<p>文本分类是文本挖掘的核心任务，一直以来倍受学术界和工业界的关注。文本分类（Text Classification）的任务是根据给定文档的内容或主题，自动分配预先定义的类别标签。</p>
<p>对文档进行分类，一般需要经过两个步骤：</p>
<p>• 文本表示</p>
<p>• 学习分类</p>
<p>文本表示是指将无结构化的文本内容转化成结构化的特征向量形式，作为分类模型的输入。在得到文本对应的特征向量后，就可以采用各种分类或聚类模型，根据特征向量训练分类器或进行聚类。因此，文本分类或聚类的主要研究任务和相应关键科学问题如下：</p>
<ol>
<li>任务</li>
</ol>
<p>1.1 构建文本特征向量</p>
<p>构建文本特征向量的目的是将计算机无法处理的无结构文本内容转换为计算机能够处理的特征向量形式。文本内容特征向量构建是决定文本分类和聚类性能的重要环节。</p>
<p>为了根据文本内容生成特征向量，需要首先建立特征空间。其中典型代表是文本词袋（Bag of Words）模型，每个文档被表示为一个特征向量，其特征向量每一维代表一个词项。所有词项构成的向量长度一般可以达到几万甚至几百万的量级。</p>
<p>这样高维的特征向量表示如果包含大量冗余噪音，会影响后续分类聚类模型的计算效率和效果。</p>
<p>因此，我们往往需要进行特征选择（Feature Selection）与特征提取（Feature Extraction），选取最具有区分性和表达能力的特征建立特征空间，实现特征空间降维；或者，进行特征转换（Feature Transformation），将高维特征向量映射到低维向量空间。特征选择、提取或转换是构建有效文本特征向量的关键问题。</p>
<p>1.2 建立分类或聚类模型</p>
<p>在得到文本特征向量后，我们需要构建分类或聚类模型，根据文本特征向量进行分类或聚类。</p>
<p>其中，分类模型旨在学习特征向量与分类标签之间的关联关系，获得最佳的分类效果； 而聚类模型旨在根据特征向量计算文本之间语义相似度，将文本集合划分为若干子集。 分类和聚类是机器学习领域的经典研究问题。</p>
<p>我们一般可以直接使用经典的模型或算法解决文本分类或聚类问题。例如，对于文本分类，我们可以选用朴素贝叶斯、决策树、k-NN、逻辑回归（Logistic Regression）、支持向量机（Support Vector Machine, SVM）等分类模型。 </p>
<p>对于文本聚类，我们可以选用 k-means、层次聚类或谱聚类（spectral clustering）等聚类算法。 这些模型算法适用于不同类型的数据而不仅限于文本数据。</p>
<p>但是，文本分类或聚类会面临许多独特的问题，例如，如何充分利用大量无标注的文本数据，如何实现面向文本的在线分类或聚类模型，如何应对短文本带来的表示稀疏问题，如何实现大规模带层次分类体系的分类功能，如何充分利用文本的序列信息和句法语义信息，如何充分利用外部语言知识库信息，等等。这些问题都是构建文本分类和聚类模型所面临的关键问题。</p>
<ol>
<li>模型</li>
</ol>
<p>2.1 文本分类模型</p>
<p>近年来，文本分类模型研究层出不穷，特别是随着深度学习的发展，深度神经网络模型 也在文本分类任务上取得了巨大进展。我们将文本分类模型划分为以下三类：</p>
<p>• 基于规则的分类模型</p>
<p>基于规则的分类模型旨在建立一个规则集合来对数据类别进行判断。这些规则可以从训练样本里自动产生，也可以人工定义。给定一个测试样例，我们可以通过判断它是否满足某 些规则的条件，来决定其是否属于该条规则对应的类别。</p>
<p>典型的基于规则的分类模型包括决策树（Decision Tree）、随机森林（Random Forest）、 RIPPER 算法等。</p>
<p>• 基于机器学习的分类模型</p>
<p>典型的机器学习分类模型包括贝叶斯分类器（Naïve Bayes）、线性分类器（逻辑回归）、 支持向量机（Support Vector Machine, SVM）、最大熵分类器等。</p>
<p>SVM 是这些分类模型中比较有效、使用较为广泛的分类模型。它能够有效克服样本分布不均匀、特征冗余以及过拟合等问题，被广泛应用于不同的分类任务与场景。通过引入核函数，SVM 还能够解决原始特征空间线性不可分的问题。</p>
<p>除了上述单分类模型，以 Boosting 为代表的分类模型组合方法能够有效地综合多个弱分类模型的分类能力。在给定训练数据集合上同时训练这些弱分类模型，然后通过投票等机制综合多个分类器的预测结果，能够为测试样例预测更准确的类别标签。</p>
<p>• 基于神经网络的方法</p>
<p>以人工神经网络为代表的深度学习技术已经在计算机视觉、语音识别等领域取得了巨大成功，在自然语言处理领域，利用神经网络对自然语言文本信息进行特征学习和文本分类，也成为文本分类的前沿技术。</p>
<p>前向神经网络：多层感知机（Multilayer Perceptron, MLP）是一种典型的前向神经网络。它能够自动学习多层神经网络，将输入特征向量映射到对应的类别标签上。</p>
<p>通过引入非线性激活层，该模型能够实现非线性的分类判别式。包括多层感知机在内的文本分类模型均使用了词袋模型假设，忽略了文本中词序和结构化信息。对于多层感知机模型来说，高质量的初始特征表示是实现有效分类模型的必要条件。</p>
<p>为了更加充分地考虑文本词序信息，利用神经网络自动特征学习的特点，研究者后续提出了卷积神经网络（Convolutional Neural Network, CNN）和循环神经网络（Recurrent Neural Network, RNN）进行文本分类。</p>
<p>基于 CNN 和 RNN 的文本分类模型输入均为原始的词序列，输出为该文本在所有类别上的概率分布。这里，词序列中的每个词项均以词向量的形式作为输入。</p>
<p>卷积神经网络（CNN）：卷积神经网络文本分类模型的主要思想是，对词向量形式的文本输入进行卷积操作。CNN 最初被用于处理图像数据。与图像处理中选取二维域进行卷积操作不同，面向文本的卷积操作是针对固定滑动窗口内的词项进行的。</p>
<p>经过卷积层、 池化层和非线性转换层后，CNN 可以得到文本特征向量用于分类学习。CNN 的优势在于在计算文本特征向量过程中有效保留有用的词序信息。</p>
<p>针对 CNN 文本分类模型还有许多改进工作， 如基于字符级 CNN 的文本分类模型、将词位置信息加入到词向量。</p>
<p>循环神经网络（RNN）：循环神经网络将文本作为字符或词语序列{x0 , … , xN}，对于第 t 时刻输入的字符或词语 xt，都会对应产生新的低维特征向量 st。如图 3 所示，st 的取值会受到 xt 和上个时刻特征向量 st-1 的共同影响，st 包含了文本序列从 x0 到 xt 的语义信息。因此，我们可以利用 sN 作为该文本序列的特征向量，进行文本分类学习。</p>
<p>与 CNN 相比，RNN 能够更自然地考虑文本的词序信息，是近年来进行文本表示最流行的方案之一。</p>
<p>为了提升 RNN 对文本序列的语义表示能力，研究者提出很多扩展模型。</p>
<p>例如，长短时记忆网络（LSTM）提出记忆单元结构，能够更好地处理文本序列中的长程依赖，克服循环神经网络梯度消失问题。如图 4 是 LSTM 单元示意图，其中引入了三个门（input gate, output gate, forget gate）来控制是否输入输出以及记忆单元更新。</p>
<p>提升 RNN 对文本序列的语义表示能力的另外一种重要方案是引入选择注意力机制 (Selective Attention)，可以让模型根据具体任务需求对文本序列中的词语给予不同的关注度。</p>
<ol>
<li>应用</li>
</ol>
<p>文本分类技术在智能信息处理服务中有着广泛的应用。例如，大部分在线新闻门户网站（如新浪、搜狐、腾讯等）每天都会产生大量新闻文章，如果对这些新闻进行人工整理非常耗时耗力，而自动对这些新闻进行分类，将为新闻归类以及后续的个性化推荐等都提供巨大帮助。</p>
<p>互联网还有大量网页、论文、专利和电子图书等文本数据，对其中文本内容进行分类，是实现对这些内容快速浏览与检索的重要基础。此外，许多自然语言分析任务如观点挖掘、垃圾邮件检测等，也都可以看作文本分类或聚类技术的具体应用。</p>
<p>对文档进行分类，一般需要经过两个步骤：（1）文本表示，以及（2）学习。文本表示是指将无结构化的文本内容转化成结构化的特征向量形式，作为分类模型的输入。在得到文本对应的特征向量后，就可以采用各种分类或聚类模型，根据特征向量训练分类器</p>
<p>五. 信息检索</p>
<p>信息检索（Information Retrieval, IR）是指将信息按一定的方式加以组织，并通过信息查找满足用户的信息需求的过程和技术。</p>
<p>1951 年，Calvin Mooers 首次提出了“信息检索”的概念，并给出了信息检索的主要任务：协助信息的潜在用户将信息需求转换为一张文献来源列表，而这些文献包含有对其有用的信息。</p>
<p>信息检索学科真正取得长足发展是在计算机诞生并得到广泛应用之后，文献数字化使得信息的大规模共享及保存成为现实，而检索就成为了信息管理与应用中必不可少的环节。</p>
<p>互联网的出现和计算机硬件水平的提高使得人们存储和处理信息的能力得到巨大的提高，从而加速了信息检索研究的进步，并使其研究对象从图书资料和商用数据扩展到人们生活的方方面面。</p>
<p>伴随着互联网及网络信息环境的迅速发展，以网络信息资源为主要组织对象的信息检索系统：搜索引擎应运而生，成为了信息化社会重要的基础设施。</p>
<p>2016 年初，中文搜索引擎用户数达到 5.66 亿人，这充分说明搜索引擎在应用层次取得的巨大成功，也使得信息检索，尤其是网络搜索技术的研究具有了重要的政治、经济和社会价值。</p>
<ol>
<li>内容结构</li>
</ol>
<p>检索用户、信息资源和检索系统三个主要环节组成了信息检索应用环境下知识获取与信息传递的完整结构，而当前影响信息获取效率的因素也主要体现在这几个环节，即：</p>
<p>• 检索用户的意图表达</p>
<p>• 信息资源（尤其是网络信息资源）的质量度量</p>
<p>• 需求与资源的合理匹配</p>
<p>具体而言，用户有限的认知能力导致其知识结构相对大数据时代的信息环境而言往往存在缺陷，进而影响信息需求的合理组织和清晰表述；数据资源的规模繁杂而缺乏管理，在互联网“注意力经济”盛行的环境下，不可避免地存在欺诈作弊行为，导致检索系统难以准确感知其质量；用户与资源提供者的知识结构与背景不同，对于相同或者相似事物的描述往往存在较大差异，使得检索系统传统的内容匹配技术难以很好应对，无法准确度量资源与需求的匹配程度。</p>
<p>上述技术挑战互相交织，本质上反映了用户个体有限的认知能力与包含近乎无限信息的数据资源空间之间的不匹配问题。</p>
<p>概括地讲，当前信息检索的研究包括如下四个方面的研究内容及相应的关键科学问题：</p>
<p>1.1 信息需求理解</p>
<p>面对复杂的泛在网络空间，用户有可能无法准确表达搜索意图；即使能够准确表达，搜索引擎也可能难以正确理解；即使能够正确理解，也难以与恰当的网络资源进行匹配。这使得信息需求理解成为了影响检索性能提高的制约因素，也构成了检索技术发展面临的第一个关键问题。</p>
<p>1.2 资源质量度量</p>
<p>资源质量管理与度量在传统信息检索研究中并非处于首要的位置，但随着互联网信息资源逐渐成为检索系统的主要查找对象，网络资源特有的缺乏编审过程、内容重复度高、质量参差不齐等问题成为了影响检索质量的重要因素。</p>
<p>目前，搜索引擎仍旧面临着如何进行有效的资源质量度量的挑战，这构成了当前信息检索技术发展面临的第二个关键问题。</p>
<p>1.3 结果匹配排序</p>
<p>近年来，随着网络技术的进步，信息检索系统（尤其是搜索引擎）涉及的数据对象相应 的变得多样化、异质化，这也造成了传统的以文本内容匹配为主要手段的结果排序方法面临着巨大的挑战。</p>
<p>高度动态繁杂的泛在网络内容使得文本相似度计算方法无法适用；整合复杂异构网络资源作为结果使得基于同质性假设构建的用户行为模型难以应对；多模态的交互方式则使得传统的基于单一维度的结果分布规律的用户行为假设大量失效。</p>
<p>因此，在大数据时代信息进一步多样化、异质化的背景下，迫切需要构建适应现代信息资源环境的检索结果匹配排序方法，这是当前信息检索技术发展面临的第三个关键问题。</p>
<p>1.4 信息检索评价</p>
<p>信息检索评价是信息检索和信息获取领域研究的核心问题之一。信息检索和信息获取系统核心的目标是帮助用户获取到满足他们需求的信息，而评价系统的作用是帮助和监督研究开发人员向这一核心目标前进，以逐步开发出更好的系统，进而缩小系统反馈和用户需求之间的差距，提高用户满意度。</p>
<p>因此，如何设计合理的评价框架、评价手段、评价指标，是当前信息检索技术发展面临的第四个关键问题。</p>
<ol>
<li>个性化搜索</li>
</ol>
<p>现有的主要个性化搜索算法可分为基于内容分析的算法、基于链接分析的方法和基于协作过滤的算法。</p>
<p>• 基于内容的个性化搜索算法通过比较用户兴趣爱好和结果文档的内容相似性来对文档的用户相关性进行判断进而对搜索结果进行重排。</p>
<p>用户模型一般表述为关键词或主题向量或层次的形式。个性化算法通过比较用户模型和文档的相似性，判断真实的搜索意图，并估计文档对用户需求的匹配程度。</p>
<p>• 基于链接分析的方法主要是利用互联网上网页之间的链接关系，并假设用户点击和访问过的网页为用户感兴趣的网页，通过链接分析算法进行迭代最终计算出用户对每个网页的喜好度。</p>
<p>• 基于协作过滤的个性化搜索算法主要借鉴了基于协作过滤的推荐系统的思想，这种方法考虑到能够收集到的用户的个人信息有限，因此它不仅仅利用用户个人的信息，还利用与用户相似的其它用户或群组的信息，并基于用户群组和相似用户的兴趣偏好来个性化当前用户的搜索结果。用户之间的相似性可以通过用户的兴趣爱好、历史查询、点击过的网页等内容计算得出。</p>
<ol>
<li>语义搜索技术</li>
</ol>
<p>随着互联网信息的爆炸式增长，传统的以关键字匹配为基础的搜索引擎，已越来越难以满足用户快速查找信息的需求。同时由于没有知识引导及对网页内容的深入整理，传统网页搜索返回的网页结果也不能精准给出所需信息。</p>
<p>针对这些问题，以知识图谱为代表的语义搜索（Semantic Search）将语义 Web 技术和传统的搜索引擎技术结合，是一个很有研究价值 但还处于初期阶段的课题。</p>
<p>在未来的一段时间，结合互联网应用需求的实际和技术、产品运营能力的实际发展水平，语义搜索技术的发展重点将有可能集中在以各种情境的垂直搜索资源为基础，知识化推理为检索运行方式，自然语言多媒体交互为手段的智能化搜索与推荐技术。</p>
<p>首先将包括各类垂直搜索资源在内的深度万维网数据源整合成为提供搜索服务的资源池；随后利用广泛分布在公众终端计算设备上的浏览器作为客户端载体，通过构建的复杂情境知识库来开发多层次查询技术，并以此管理、调度、整合搜索云端的搜索服务资源，满足用户的多样化、多模态查询需求；最后基于面向情境体验的用户行为模型构建，以多模态信息推荐的形式实现对用户信息需求的主动满足。</p>
<p>六. 信息抽取</p>
<p>信息抽取（Information Extraction）是指从非结构化/半结构化文本（如网页、新闻、 论文文献、微博等）中提取指定类型的信息（如实体、属性、关系、事件、商品记录等）， 并通过信息归并、冗余消除和冲突消解等手段将非结构化文本转换为结构化信息的一项综合技术。例如:</p>
<p>• 从相关新闻报道中抽取出恐怖事件信息：时间、地点、袭击者、受害人、袭击 目标、后果等；</p>
<p>• 从体育新闻中抽取体育赛事信息：主队、客队、赛场、比分等；</p>
<p>• 从论文和医疗文献中抽取疾病信息：病因、病原、症状、药物等</p>
<p>被抽取出来的信息通常以结构化的形式描述，可以为计算机直接处理，从而实现对海量非结构化数据的分析、组织、管理、计算、 查询和推理，并进一步为更高层面的应用和任务（如自然语言理解、知识库构建、智能问答系统、舆情分析系统）提供支撑。</p>
<p>目前信息抽取已被广泛应用于舆情监控、网络搜索、智能问答等多个重要领域。与此同时，信息抽取技术是中文信息处理和人工智能的核心技术，具有重要的科学意义。</p>
<p>一直以来，人工智能的关键核心部件之一是构建可支撑类人推理和自然语言理解的大规模常识知识库。然而，由于人类知识的复杂性、开放性、多样性和巨大的规模，目前仍然无法构建满足上述需求的大规模知识库。</p>
<p>信息抽取技术通过结构化自然语言表述的语义知识，并整合来自海量文本中的不同语义知识，是构建大规模知识库最有效的技术之一。</p>
<p>每一段文本内所包含的寓意可以描述为其中的一组实体以及这些实体相互之间的关联和交互，因此抽取文本中的实体和它们之间的语义关系也就成为了理解文本意义的基础。</p>
<p>信息抽取可以通过抽取实体和实体之间的语义关系，表示这些语义关系承载的信息，并基于这些信息进行计算和推理来有效的理解一段文本所承载的语义。</p>
<ol>
<li>命名实体识别</li>
</ol>
<p>命名实体识别的目的是识别文本中指定类别的实体，主要包括人名、地名、机构名、专有名词等的任务。</p>
<p>命名实体识别系统通常包含两个部分：实体边界识别和实体分类。</p>
<p>其中实体边界识别判断一个字符串是否是一个实体，而实体分类将识别出的实体划分到预先给定的不同类别中去。</p>
<p>命名实体识别是一项极具实用价值的技术，目前中英文上通用命名实体识别（人名、地名、机构名）的 F1 值都能达到 90% 以上。命名实体识别的主要难点在于表达不规律、且缺乏训练语料的开放域命名实体类别（如电影、歌曲名）等。</p>
<ol>
<li>关系抽取</li>
</ol>
<p>关系抽取指的是检测和识别文本中实体之间的语义关系，并将表示同一语义关系的提及（mention）链接起来的任务。关系抽取的输出通常是一个三元组（实体 1，关系类别，实体 2），表示实体 1 和实体 2 之间存在特定类别的语义关系。</p>
<p>例如，句子“北京是中国的首都、政治中心和文化中心”中表述的关系可以表示为（中国，首都，北京），（中国，政治中心，北京）和（中国，文化中心，北京）。语义关系类别可以预先给定（如 ACE 评测中的七大类关系），也可以按需自动发现（开放域信息抽取）。</p>
<p>关系抽取通常包含两个核心模块：关系检测和关系分类。</p>
<p>其中关系检测判断两个实体之间是否存在语义关系，而关系分类将存在语义关系的实体对划分到预先指定的类别中。</p>
<p>在某些场景和任务下，关系抽取系统也可能包含关系发现模块，其主要目的是发现实体和实体之间存在的语义关系类别。例如，发现人物和公司之间存在雇员、CEO、CTO、创始人、董事长等关系类别。</p>
<ol>
<li>事件抽取</li>
</ol>
<p>事件抽取指的是从非结构化文本中抽取事件信息，并将其以结构化形式呈现出来的任务。</p>
<p>例如，从“毛泽东 1893 年出生于湖南湘潭”这句话中抽取事件{类型：出生， 人物：毛泽东，时间：1893 年，出生地：湖南湘潭}。</p>
<p>事件抽取任务通常包含事件类型识别和事件元素填充两个子任务。</p>
<p>事件类型识别判断一句话是否表达了特定类型的事件。事件类型决定了事件表示的模板，不同类型的事件具有不同的模板。</p>
<p>例如出生事件的模板是{人物， 时间，出生地}，而恐怖袭击事件的模板是{地点，时间，袭击者，受害者，受伤人数,…}。 事件元素指组成事件的关键元素，事件元素识别指的是根据所属的事件模板，抽取相应的元素，并为其标上正确元素标签的任务。</p>
<ol>
<li>信息集成</li>
</ol>
<p>实体、关系和事件分别表示了单篇文本中不同粒度的信息。在很多应用中，需要将来自不同数据源、不同文本的信息综合起来进行决策，这就需要研究信息集成技术。</p>
<p>目前，信息抽取研究中的信息集成技术主要包括共指消解技术和实体链接技术。</p>
<p>共指消解指的是检测同一实体/关系/事件的不同提及，并将其链接在一起的任务，例如，识别“乔布斯是苹果的创始人之一，他经历了苹果公司几十年的起落与兴衰”这句话中的“乔布斯”和“他”指的是同一实体。</p>
<p>实体链接的目的是确定实体名所指向的真实世界实体。例如识别上一句话中的“苹果”和“乔布斯”分别指向真实世界中的苹果公司和其 CEO 史蒂夫·乔布斯。</p>
<p>七. 问答系统</p>
<p>自动问答（Question Answering, QA）是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。不同于现有搜索引擎，问答系统是信息服务的一种高级形式，系统返回用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。</p>
<p>近年来，随着人工智能的飞速发展，自动问答已经成为倍受关注且发展前景广泛的研究方向。自动问答的研究历史可以溯源到人工智能的原点。</p>
<p>1950 年，人工智能之父阿兰图灵（Alan M. Turing）在《Mind》上发表文章《Computing Machinery and Intelligence》，文章开篇提出通过让机器参与一个模仿游戏（Imitation Game）来验证“机器”能否“思考”，进而提出了经典的图灵测试（Turing Test），用以检验机器是否具备智能。</p>
<p>同样，在自然语言处理研究领域，问答系统被认为是验证机器是否具备自然语言理解能力的四个任务之一（其它三个是机器翻译、复述和文本摘要）。</p>
<p>自动问答研究既有利于推动人工智能相关学科的发展，也具有非常重要的学术意义。从应用上讲，现有基于关键词匹配和浅层语义分析的信息服务技术已经难以满足用户日益增长的精准化和智能化信息需求，已有的信息服务范式急需一场变革。</p>
<p>2011 年，华盛顿大学图灵中心主任 Etzioni 在 Nature 上发表的《Search Needs a Shake-Up》中明确指出：在万维网诞生 20 周年之际，互联网搜索正处于从简单关键词搜索走向深度问答的深刻变革的风口浪尖上。以直接而准确的方式回答用户自然语言提问的自动问答系统将构成下一代搜索引擎的基本形态。</p>
<p>同一年，以深度问答技术为核心的 IBM Watson 自动问答机器人在美国智力竞赛节目 Jeopardy 中战胜人类选手，引起了业内的巨大轰动。Watson 自动问答系统让人们看到已有信息服务模式被颠覆的可能性，成为了问答系统发展的一个里程碑。</p>
<p>此外，随着移动互联网崛起与发展，以苹果公司 Siri、Google Now、微软 Cortana 等为代表的移动生活助手爆发式涌现，上述系统都把以自然语言为基本输入方式的问答系统看作是下一代信息服务的新形态和突破口，并均加大人员、资金的投入，试图在这一次人工智能浪潮中取得领先。</p>
<ol>
<li>关键问题</li>
</ol>
<p>自动问答系统在回答用户问题时，需要正确理解用户所提的自然语言问题，抽取其中的关键语义信息，然后在已有语料库、知识库或问答库中通过检索、匹配、推理的手段获取答案并返回给用户。</p>
<p>上述过程涉及词法分析、句法分析、语义分析、信息检索、逻辑推理、知识工程、语言生成等多项关键技术。传统自动问答多集中在限定领域，针对限定类型的问题进行回答。伴随着互联网和大数据的飞速发展，现有研究趋向于开放域、面向开放类型问题的自动问答。概括地讲，自动问答的主要研究任务和相应关键科学问题如下。</p>
<p>1.1 问句理解</p>
<p>给定用户问题，自动问答首先需要理解用户所提问题。用户问句的语义理解包含词法分析、句法分析、语义分析等多项关键技术，需要从文本的多个维度理解其中包含的语义内容。</p>
<p>在词语层面，需要在开放域环境下，研究命名实体识别（Named Entity Recognition）、术语识别（Term Extraction）、词汇化答案类型词识别（Lexical Answer Type Recognition）、 实体消歧（Entity Disambiguation）、关键词权重计算（Keyword Weight Estimation）、答案集中词识别（Focused Word Detection）等关键问题。</p>
<p>在句法层面，需要解析句子中词与词之间、短语与短语之间的句法关系，分析句子句法结构。在语义层面，需要根据词语层面、句法层面的分析结果，将自然语言问句解析成可计算、结构化的逻辑表达形式（如一阶谓词逻辑表达式）。</p>
<p>1.2 文本信息抽取</p>
<p>给定问句语义分析结果，自动问答系统需要在已有语料库、知识库或问答库中匹配相关的信息，并抽取出相应的答案。</p>
<p>传统答案抽取构建在浅层语义分析基础之上，采用关键词匹配策略，往往只能处理限定类型的答案，系统的准确率和效率都难以满足实际应用需求。为保证信息匹配以及答案抽取的准确度，需要分析语义单元之间的语义关系，抽取文本中的结构化知识。</p>
<p>早期基于规则模板的知识抽取方法难以突破领域和问题类型的限制，远远不能满足开放领域自动问答的知识需求。为了适应互联网实际应用的需求，越来越多的研究者和开发者开始关注开放域知识抽取技术，其特点在于：</p>
<p>• 文本领域开放：处理的文本是不限定领域的网络文本</p>
<p>• 内容单元类型开放：不限定所抽取的内容单元类型，而是自动地从网络中挖掘内容单元的类型，例如实体类型、事件类型和关系类型等。</p>
<p>1.3 知识推理</p>
<p>自动问答中，由于语料库、知识库和问答库本身的覆盖度有限，并不是所有问题都能直 接找到答案。这就需要在已有的知识体系中，通过知识推理的手段获取这些隐含的答案。</p>
<p>例如，知识库中可能包括了一个人的“出生地”信息，但是没包括这个人的“国籍”信息，因此无法直接回答诸如“某某人是哪国人?”这样的问题。但是一般情况下，一个人的“出生地”所属的国家就是他（她）的“国籍”。</p>
<p>在自动问答中，就需要通过推理的方式学习到这样的模式。传统推理方法采用基于符号的知识表示形式，通过人工构建的推理规则得到答案。</p>
<p>但是面对大规模、开放域的问答场景，如何自动进行规则学习，如何解决规则冲突仍然是亟待解决的难点问题。目前，基于分布式表示的知识表示学习方法能够将实体、概念以及它们之间的语义关系表示为低维空间中的对象（向量、矩阵等），并通过低维空间中的数值计算完成知识推理任务。</p>
<p>虽然这类推理的效果离实用还有距离，但是我们认为这是值得探寻的方法，特别是如何将已有的基于符号表示的逻辑推理与基于分布式表示的数值推理相结合，研究融合符号逻辑和表示学习的知识推理技术，是知识推理任务中的关键科学问题。</p>
<ol>
<li>技术方法</li>
</ol>
<p>根据目标数据源的不同，已有自动问答技术大致可以分为三类：</p>
<p>• 检索式问答；<br>• 社区问答;<br>• 知识库问答。</p>
<p>以下分别就这几个方面对研究现状进行简要阐述。</p>
<p>2.1 检索式问答</p>
<p>检索式问答研究伴随搜索引擎的发展不断推进。1999 年，随着 TREC QA 任务的发起， 检索式问答系统迎来了真正的研究进展。TREC QA 的任务是给定特定 WEB 数据集，从中找到能够回答问题的答案。这类方法是以检索和答案抽取为基本过程的问答系统，具体过程包括问题分析、篇章检索和答案抽取。</p>
<p>根据抽取方法的不同，已有检索式问答可以分为基于模式匹配的问答方法和基于统计文本信息抽取的问答方法。</p>
<p>• 基于模式匹配的方法往往先离线地获得各类提问答案的模式。在运行阶段，系统首先判断当前提问属于哪一类，然后使用这类提问的模式来对抽取的候选答案进行验证。同时为了提高问答系统的性能，人们也引入自然语言处理技术。由于自然语言处理的技术还未成熟，现有大多数系统都基于浅层句子分析。</p>
<p>• 基于统计文本信息抽取的问答系统的典型代表是美国 Language Computer Corporation 公司的 LCC 系统。该系统使用词汇链和逻辑形式转换技术，把提问句和答案句转化成统一的逻辑形式（Logic Form），通过词汇链，实现答案的推理验证。</p>
<p>LCC 系统在 TREC QA Track 2001 ~ 2004 连续三年的评测中以较大领先优势获得第一名的成绩。 2011 年，IBM 研发的问答机器人 Watson 在美国智力竞赛节目《危险边缘 Jeopardy!》中战胜人类选手，成为问答系统发展的一个里程碑。</p>
<p>Watson 的技术优势大致可以分为以下三个方面：</p>
<p>• 强大的硬件平台：包括 90 台 IBM 服务器，分布式计算环境；</p>
<p>• 强大的知识资源：存储了大约 2 亿页的图书、新闻、电影剧本、辞海、文选和《世界图书百科全书》等资料；</p>
<p>• 深层问答技术（DeepQA）：涉及统计机器学习、句法分析、主题分析、信息抽取、 知识库集成和知识推理等深层技术。</p>
<p>然而，Watson 并没有突破传统问答式检索系统的局限性，使用的技术主要还是检索和匹配，回答的问题类型大多是简单的实体或词语类问题，而推理能力不强。</p>
<p>2.2 社区问答</p>
<p>随着 Web2.0 的兴起，基于用户生成内容（User-Generated Content, UGC）的互联网服务越来越流行，社区问答系统应运而生，例如 Yahoo! Answers、百度知道等。</p>
<p>问答社区的出现为问答技术的发展带来了新的机遇。据统计 2010 年 Yahoo! Answers 上已解决的问题量达到 10 亿，2011 年“百度知道”已解决的问题量达到 3 亿，这些社区问答数据覆盖了方方面面的用户知识和信息需求。</p>
<p>此外，社区问答与传统自动问答的另一个显著区别是：社区问答系统有大量的用户参与，存在丰富的用户行为信息，例如用户投票信息、用户评价信息、回答者的问题采纳率、用户推荐次数、页面点击次数以及用户、问题、答案之间的相互关联信息等等，这些用户行为信息对于社区中问题和答案的文本内容分析具有重要的价值。</p>
<p>一般来讲，社区问答的核心问题是从大规模历史问答对数据中找出与用户提问问题语义相似的历史问题并将其答案返回提问用户。</p>
<p>假设用户查询问题为 q0,用于检索的问答对数据为 SQ,A = {(q1 , a1 ), (q2 , a2 )}, … , (qn, an)}}，相似问答对检索的目标是从 SQ,A 中检索出能够解答问题 q0 的问答对 (qi , ai)。 针对这一问题，传统的信息检索模型，如向量空间模型、语言模型等，都可以得到应用。</p>
<p>但是，相对于传统的文档检索，社区问答的特点在于：用户问题和已有问句相对来说都非常短，用户问题和已有问句之间存在“词汇鸿沟”问题，基于关键词匹配的检索模型很难达到较好的问答准确度。</p>
<p>目前，很多研究工作在已有检索框架中针对这一问题引入单语言翻译概率模型，通过 IBM 翻译模型，从海量单语问答语料中获得同种语言中两个不同词语之间的语义转换概率，从而在一定程度上解决词汇语义鸿沟问题。</p>
<p>例如和“减肥”对应的概率高的相关词有“瘦身”、“跑步”、“饮食”、“健康”、“远动”等等。 除此之外，也有许多关于问句检索中词重要性的研究和基于句法结构的问题匹配研究。</p>
<p>2.3 知识库问答</p>
<p>检索式问答和社区问答尽管在某些特定领域或者商业领域有所应用，但是其核心还是关键词匹配和浅层语义分析技术，难以实现知识的深层逻辑推理，无法达到人工智能的高级目标。</p>
<p>因此，近些年来，无论是学术界或工业界，研究者们逐步把注意力投向知识图谱或知识库（Knowledge Graph）。其目标是把互联网文本内容组织成为以实体为基本语义单元（节点）的图结构，其中图上的边表示实体之间语义关系。</p>
<p>目前互联网中已有的大规模知识库包括 DBpedia、Freebase、YAGO 等。这些知识库多是以“实体-关系-实体”三元组为基本单元所组成的图结构。</p>
<p>基于这样的结构化知识，问答系统的任务就是要根据用户问题的语义直接在知识库上查找、推理出相匹配的答案，这一任务称为面向知识库的问答系统或知识库问答。要完成在结构化数据上的查询、匹配、推理等操作，最有效的方式是利用结构化的查询语句，例如：SQL、SPARQL 等。</p>
<p>然而，这些语句通常是由专家编写，普通用户很难掌握并正确运用。对普通用户来说，自然语言仍然是最自然的交互方式。因此，如何把用户的自然语言问句转化为结构化的查询语句是知识库问答的核心所在，其关键是对于自然语言问句进行语义理解。</p>
<p>目前，主流方法是通过语义分析，将用户的自然语言问句转化成结构化的语义表示，如范式和 DCS-Tree。相对应的语义解析语法或方法包括组合范畴语法（ Category Compositional Grammar, CCG ）以 及 依 存 组 合 语 法（ Dependency-based Compositional Semantics, DCS）等。</p>
<p>八. 机器翻译</p>
<ol>
<li>理论应用</li>
</ol>
<p>机器翻译（machine translation，MT）是指利用计算机实现从一种自然语言到另外一种自然语言的自动翻译。被翻译的语言称为源语言（source language），翻译到的语言称作目标语言（target language）。 </p>
<p>简单地讲，机器翻译研究的目标就是建立有效的自动翻译方法、模型和系统，打破语言壁垒，最终实现任意时间、任意地点和任意语言的自动翻译，完成人们无障碍自由交流的梦想。</p>
<p>人们通常习惯于感知（听、看和读）自己母语的声音和文字，很多人甚至只能感知自己的母语，因此，机器翻译在现实生活和工作中具有重要的社会需求。</p>
<p>从理论上讲，机器翻译涉及语言学、计算语言学、人工智能、机器学习，甚至认知语言学等多个学科，是一个典型的多学科交叉研究课题，因此开展这项研究具有非常重要的理论意义，既有利于推动相关学科的发展，揭示人脑实现跨语言理解的奥秘，又有助于促进其他自然语言处理任务，包括中文信息处理技术的快速发展。</p>
<p>从应用上讲，无论是社会大众、政府企业还是国家机构，都迫切需要机器翻译技术。特别是在“互联网+”时代，以多语言多领域呈现的大数据已成为我们面临的常态问题，机器翻译成为众多应用领域革新的关键技术之一。</p>
<p>例如，在商贸、体育、文化、旅游和教育等各个领域，人们接触到越来越多的外文资料，越来越频繁地与持各种语言的人通信和交流，从而对机器翻译的需求越来越强烈；在国家信息安全和军事情报领域，机器翻译技术也扮演着非常重要的角色。</p>
<p>可以说离开机器翻译，基于大数据的多语言信息获取、挖掘、分析和决策等其他应用都将成为空中楼阁。</p>
<p>尤其值得提出的是，在未来很长一段时间里，建立于丝绸之路这一历史资源之上的“一带一路”将是我国与周边国家发展政治、经济，进行文化交流的主要战略。据统计，“一带一路”涉及 60 多个国家、44 亿人口、53 种语言，可见机器翻译是“一带一路”战略实施中不可或缺的重要技术。</p>
<ol>
<li>技术现状</li>
</ol>
<p>基于规则的机器翻译方法需要人工设计和编纂翻译规则，统计机器翻译方法能够自动获取翻译规则，但需要人工定义规则的形式，而端到端的神经网络机器翻译方法可以直接通过编码网络和解码网络自动学习语言之间的转换算法。</p>
<p>从某种角度讲，其自动化程度和智能化程度在不断提升，机器翻译质量也得到了显著改善。机器翻译技术的研究现状可从欧盟组织的国际机器翻译评测（WMT）的结果中窥得一斑。</p>
<p>该评测主要针对欧洲语言之间的互译，2006 年至 2016 年每年举办一次。对比法语到英语历年的机器翻译评测结果可以发现，译文质量已经在自动评价指标 BLEU 值上从最初小于 0.3 到目前接近 0.4（大量的人工评测对比说明，BLEU 值接近 0.4 的译文能够达到人类基本可以理解的程度）。</p>
<p>另外，中国中文信息学会组织的全国机器翻译评测（CWMT）每两年组织一次， 除了英汉、日汉翻译评测以外，CWMT 还关注我国少数民族语言（藏、蒙、维）和汉语之间的翻译。</p>
<p>相对而言，由于数据规模和语言复杂性的问题，少数民族与汉语之间的翻译性能要低于汉英、汉日之间的翻译性能。虽然机器翻译系统评测的分值呈逐年增长的趋势，译文质量越来越好，但与专业译员的翻译结果相比，机器翻译还有很长的路要走，可以说，在奔向“信、达、雅”翻译目标的征程上，目前的机器翻译基本挣扎在“信”的阶段，很多理论和技术问题仍有待于更深入的研究和探索。</p>
<p>九. 自动摘要</p>
<p>自动文摘（又称自动文档摘要）是指通过自动分析给定的一篇文档或多篇文档，提炼、总结其中的要点信息，最终输出一篇长度较短、可读性良好的摘要（通常包含几句话或数百字），该摘要中的句子可直接出自原文，也可重新撰写所得。</p>
<p>简言之，文摘的目的是通过对原文本进行压缩、提炼，为用户提供简明扼要的文字描述。用户可以通过阅读简短的摘要而知晓原文中所表达的主要内容，从而大幅节省阅读时间。</p>
<p>自动文摘研究的目标是建立有效的自动文摘方法与模型，实现高性能的自动文摘系统。近二十年来，业界提出了各类自动文摘方法与模型，用于解决各类自动摘要问题，在部分自动摘要问题的研究上取得了明显的进展，并成功将自动文摘技术应用于搜索引擎、新闻阅读 等产品与服务中。</p>
<p>例如谷歌、百度等搜索引擎均会为每项检索结果提供一个短摘要，方便用 户判断检索结果相关性。在新闻阅读软件中，为新闻事件提供摘要也能够方便用户快速了解 该事件。2013 年雅虎耗资 3000 万美元收购了一项自动新闻摘要应用 Summly，则标志着自动文摘技术的应用走向成熟。</p>
<p>自动文摘的研究在图书馆领域和自然语言处理领域一直都很活跃，最早的应用需求来自于图书馆。图书馆需要为大量文献书籍生成摘要，而人工摘要的效率很低，因此亟需自动摘要方法取代人工高效地完成文献摘要任务。</p>
<p>随着信息检索技术的发展，自动文摘在信息检索系统中的重要性越来越大，逐渐成为研究热点之一。经过数十年的发展，同时在 DUC 与 TAC 等自动文摘国际评测的推动下，文本摘要技术已经取得长足的进步。国际上自动文摘方面比较著名的几个系统包括 ISI 的 NeATS 系统，哥伦比亚大学的 NewsBlaster 系统，密歇根大学的 NewsInEssence 系统等。</p>
<ol>
<li>方法</li>
</ol>
<p>自动文摘所采用的方法从实现上考虑可以分为抽取式摘要（extractive summarization） 和生成式摘要（abstractive summarization）。</p>
<p>抽取式方法相对比较简单，通常利用不同方法对文档结构单元（句子、段落等）进行评价，对每个结构单元赋予一定权重，然后选择最重要的结构单元组成摘要。而生成式方法通常需要利用自然语言理解技术对文本进行语法、 语义分析，对信息进行融合，利用自然语言生成技术生成新的摘要句子。</p>
<p>目前的自动文摘方法主要基于句子抽取，也就是以原文中的句子作为单位进行评估与选取。抽取式方法的好处是易于实现，能保证摘要中的每个句子具有良好的可读性。</p>
<p>为解决如前所述的要点筛选和文摘合成这两个关键科学问题，目前主流自动文摘研究工作大致遵循如下技术框架： 内容表示 → 权重计算 → 内容选择 → 内容组织。</p>
<p>首先将原始文本表示为便于后续处理的表达方式，然后由模型对不同的句法或语义单元 进行重要性计算，再根据重要性权重选取一部分单元，经过内容上的组织形成最后的摘要。</p>
<p>1.1 内容表示与权重计算</p>
<p>原文档中的每个句子由多个词汇或单元构成，后续处理过程中也以词汇等元素为基本单位，对所在句子给出综合评价分数。</p>
<p>以基于句子选取的抽取式方法为例，句子的重要性得分由其组成部分的重要性衡量。由于词汇在文档中的出现频次可以在一定程度上反映其重要性， 我们可以使用每个句子中出现某词的概率作为该词的得分，通过将所有包含词的概率求和得到句子得分。</p>
<p>也有一些工作考虑更多细节，利用扩展性较强的贝叶斯话题模型，对词汇本身的话题相关性概率进行建模。一些方法将每个句子表示为向量，维数为总词表大小。通常使用加权频数作为句子向量相应维上的取值。加权频数的定义可以有多种，如信息检索中常用的词频-逆文档频率 （TF-IDF）权重。</p>
<p>也有研究工作考虑利用隐语义分析或其他矩阵分解技术，得到低维隐含语义表示并加以利用。得到向量表示后计算两两之间的某种相似度（例如余弦相似度）。随后根据计算出的相似度构建带权图，图中每个节点对应每个句子。</p>
<p>在多文档摘要任务中，重要的句子可能和更多其他句子较为相似，所以可以用相似度作为节点之间的边权，通过迭代求解基于图的排序算法来得到句子的重要性得分。</p>
<p>也有很多工作尝试捕捉每个句子中所描述的概念，例如句子中所包含的命名实体或动词。</p>
<p>出于简化考虑，现有工作中更多将二元词（bigram）作为概念。近期则有工作提出利用频繁图挖掘算法从文档集中挖掘得到深层依存子结构作为语义表示单元。</p>
<p>另一方面，很多摘要任务已经具备一定数量的公开数据集，可用于训练有监督打分模型。</p>
<p>例如对于抽取式摘要，我们可以将人工撰写的摘要贪心匹配原文档中的句子或概念，从而得到不同单元是否应当被选作摘要句的数据。然后对各单元人工抽取若干特征，利用回归模型或排序学习模型进行有监督学习，得到句子或概念对应的得分。</p>
<p>文档内容描述具有结构性，因此也有利用隐马尔科夫模型（HMM）、条件随机场（CRF）、结构化支持向量机（Structural SVM）等常见序列标注或一般结构预测模型进行抽取式摘要有监督训练的工作。</p>
<p>所提取的特征包括所在位置、包含词汇、与邻句的相似度等等。对特定摘要任务一般也会引入与具体设定相关的特征，例如查询相关摘要任务中需要考虑与查询的匹配或相似程度。</p>
<p>1.2 内容选择</p>
<p>无论从效果评价还是从实用性的角度考虑，最终生成的摘要一般在长度上会有限制。在获取到句子或其他单元的重要性得分以后，需要考虑如何在尽可能短的长度里容纳尽可能多的重要信息，在此基础上对原文内容进行选取。内容选择方法包括贪心选择和全局优化。</p>
<ol>
<li>技术现状</li>
</ol>
<p>相比机器翻译、自动问答、知识图谱、情感分析等热门领域，自动文摘在国内并没有受到足够的重视。</p>
<p>国内早期的基础资源与评测举办过中文单文档摘要的评测任务，但测试集规模比较小，而且没有提供自动化评价工具。2015 年 CCF 中文信息技术专委会组织了 NLPCC 评测，其中包括了面向中文微博的新闻摘要任务，提供了规模相对较大的样例数据和测试数据，并采用自动评价方法，吸引了多支队伍参加评测，目前这些数据可以公开获得。</p>
<p>但上述中文摘要评测任务均针对单文档摘要任务，目前还没有业界认可的中文多文档摘要数据，这在事实上阻碍了中文自动摘要技术的发展。</p>
<p>近些年，市面上出现了一些文本挖掘产品，能够提供中文文档摘要功能（尤其是单文档 摘要），例如方正智思、拓尔思（TRS），海量科技等公司的产品。百度等搜索引擎也能为检索到的文档提供简单的单文档摘要。这些文档摘要功能均被看作是系统的附属功能，其实现方法均比较简单。</p>
<p>十. 学习资料</p>
<ol>
<li>书籍</li>
</ol>
<p>1.1 李航《统计学习方法》<br>这本经典书值得反复读，从公式推导到定理证明逻辑严谨，通俗易懂。<br>推荐指数：★★★★★</p>
<p>1.1  宗成庆《统计自然语言处理》<br>推荐指数：★★★★☆</p>
<ol>
<li>博客</li>
</ol>
<p>斯坦福 cs224d：<br><a href="http://cs224d.stanford.edu/syllabus.html" target="_blank" rel="external">http://cs224d.stanford.edu/syllabus.html</a> </p>
<ol>
<li>会议</li>
</ol>
<p>ACL 2015:<br><a href="http://acl2015.org/accepted_papers.html" target="_blank" rel="external">http://acl2015.org/accepted_papers.html</a><br>ACL 2016:<br><a href="http://acl2016.org/index.php?article_id=13#long_papers" target="_blank" rel="external">http://acl2016.org/index.php?article_id=13#long_papers</a><br>EMNLP 2015:<br><a href="http://www.emnlp2015.org/accepted-papers.html" target="_blank" rel="external">http://www.emnlp2015.org/accepted-papers.html</a> </p>
<ol>
<li>实践案例</li>
</ol>
<p><a href="https://github.com/carpedm20/lstm-char-cnn-tensorflow" target="_blank" rel="external">https://github.com/carpedm20/lstm-char-cnn-tensorflow</a><br><a href="https://github.com/zoneplus/DL4NLP" target="_blank" rel="external">https://github.com/zoneplus/DL4NLP</a><br><a href="https://github.com/HIT-SCIR/scir-training-day" target="_blank" rel="external">https://github.com/HIT-SCIR/scir-training-day</a> </p>
<p>十一. 进一步学习</p>
<p>论文下载地址：</p>
<p><a href="http://ccl.pku.edu.cn/alcourse/nlp/LectureNotes/An%20Overview%20on%20Chinese%20Word%20Segmentation%20(Sun%20Maosong).pdf" target="_blank" rel="external">http://ccl.pku.edu.cn/alcourse/nlp/LectureNotes/An%20Overview%20on%20Chinese%20Word%20Segmentation%20(Sun%20Maosong).pdf</a><br><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/cl-05.gao_.pdf" target="_blank" rel="external">https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/cl-05.gao_.pdf</a><br><a href="http://www.voidcn.com/blog/forever1dreamsxx/article/p-1295137.html" target="_blank" rel="external">http://www.voidcn.com/blog/forever1dreamsxx/article/p-1295137.html</a><br><a href="http://cleanbugs.com/item/the-syntactic-structure-of-nlp-three-chinese-syntactic-structure-cips2016-413620.html" target="_blank" rel="external">http://cleanbugs.com/item/the-syntactic-structure-of-nlp-three-chinese-syntactic-structure-cips2016-413620.html</a><br><a href="http://cips-upload.bj.bcebos.com/cips2016.pdf" target="_blank" rel="external">http://cips-upload.bj.bcebos.com/cips2016.pdf</a></p>
<p>本文经授权转载自公众号「数据派THU」</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前言&lt;/p&gt;
&lt;p&gt;自然语言处理是文本挖掘的研究领域之一，是人工智能和语言学领域的分支学科。在此领域中探讨如何处理及运用自然语言。&lt;/p&gt;
&lt;p&gt;对于自然语言处理的发展历程，可以从哲学中的经验主义和理性主义说起。基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处
    
    </summary>
    
      <category term="深度学习" scheme="http://ynuwm.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="NLP" scheme="http://ynuwm.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Faker采访的时候说</title>
    <link href="http://ynuwm.github.io/2017/10/30/Faker%E9%87%87%E8%AE%BF%E7%9A%84%E6%97%B6%E5%80%99%E8%AF%B4/"/>
    <id>http://ynuwm.github.io/2017/10/30/Faker采访的时候说/</id>
    <published>2017-10-30T08:15:19.000Z</published>
    <updated>2017-12-04T12:43:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>10月28日，半决SKT VS RNG赛后Faker接受采访说：我们的目标一直都是冠军。如果在总决赛上我们输给了对面，那亚军对我来说毫无意义。<br><img src="/2017/10/30/Faker采访的时候说/000.jpg" alt="img"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;10月28日，半决SKT VS RNG赛后Faker接受采访说：我们的目标一直都是冠军。如果在总决赛上我们输给了对面，那亚军对我来说毫无意义。&lt;br&gt;&lt;img src=&quot;/2017/10/30/Faker采访的时候说/000.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;

    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="LOL" scheme="http://ynuwm.github.io/tags/LOL/"/>
    
  </entry>
  
  <entry>
    <title>Learning PyTorch With Examples</title>
    <link href="http://ynuwm.github.io/2017/10/11/Learning-PyTorch-With-Examples/"/>
    <id>http://ynuwm.github.io/2017/10/11/Learning-PyTorch-With-Examples/</id>
    <published>2017-10-11T11:22:56.000Z</published>
    <updated>2017-12-04T12:31:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><h2 id="Warm-up-numpy"><a href="#Warm-up-numpy" class="headerlink" title="Warm-up:numpy"></a>Warm-up:numpy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = np.random.randn(N, D_in)</div><div class="line">y = np.random.randn(N, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = np.random.randn(D_in, H)</div><div class="line">w2 = np.random.randn(H, D_out)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.dot(w1)</div><div class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</div><div class="line">    y_pred = h_relu.dot(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = np.square(y_pred - y).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</div><div class="line">    grad_h = grad_h_relu.copy()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.T.dot(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure>
<h2 id="PyTorch-Tensor"><a href="#PyTorch-Tensor" class="headerlink" title="PyTorch:Tensor"></a>PyTorch:Tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random input and output data</span></div><div class="line">x = torch.randn(N, D_in).type(dtype)</div><div class="line">y = torch.randn(N, D_out).type(dtype)</div><div class="line"></div><div class="line"><span class="comment"># Randomly initialize weights</span></div><div class="line">w1 = torch.randn(D_in, H).type(dtype)</div><div class="line">w2 = torch.randn(H, D_out).type(dtype)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y</span></div><div class="line">    h = x.mm(w1)</div><div class="line">    h_relu = h.clamp(min=<span class="number">0</span>)</div><div class="line">    y_pred = h_relu.mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss)</div><div class="line"></div><div class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></div><div class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</div><div class="line">    grad_w2 = h_relu.t().mm(grad_y_pred)</div><div class="line">    grad_h_relu = grad_y_pred.mm(w2.t())</div><div class="line">    grad_h = grad_h_relu.clone()</div><div class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">    grad_w1 = x.t().mm(grad_h)</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1 -= learning_rate * grad_w1</div><div class="line">    w2 -= learning_rate * grad_w2</div></pre></td></tr></table></figure>
<h1 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h1><h2 id="PyTorch-Variables-and-autograd"><a href="#PyTorch-Variables-and-autograd" class="headerlink" title="PyTorch:Variables and autograd"></a>PyTorch:Variables and autograd</h2><p>PyTorch中所有的神经网络都来自于autograd包<br>在上面的例子中，我们必须手动实现神经网络的向前和向后遍。手动实施后向传递对于小型双层网络来说不是一件大事，但是对于大型复杂网络来说，可以很快地得到很多毛病。</p>
<p>幸运的是，我们可以使用自动区分 来自动计算神经网络中的向后遍。PyTorch中的 autograd包提供了这个功能。使用自动格式时，网络的正向传递将定义一个 计算图 ; 图中的节点将是Tensors，边缘将是从输入Tensors生成输出Tensors的函数。通过此图反向传播，您可以轻松地计算渐变。</p>
<p>这听起来很复杂，在实践中使用起来很简单。我们将PyTorch Tensors包装在可变对象中; 变量表示计算图中的节点。如果x是变量，则x.data是Tensor，并且x.grad是另一个变量，x其相对于某个标量值保存渐变 。</p>
<p>PyTorch变量与PyTorch Tensors具有相同的API（几乎）您可以在Tensor上执行的任何操作也适用于变量; 区别在于使用变量定义计算图，允许您自动计算渐变。</p>
<p>这里我们使用PyTorch变量和自动调整来实现我们的两层网络; 现在我们不再需要手动实现向后通过网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=False indicates that we do not need to compute gradients</span></div><div class="line"><span class="comment"># with respect to these Variables during the backward pass.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line"><span class="comment"># Setting requires_grad=True indicates that we want to compute gradients with</span></div><div class="line"><span class="comment"># respect to these Variables during the backward pass.</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; these</span></div><div class="line">    <span class="comment"># are exactly the same operations we used to compute the forward pass using</span></div><div class="line">    <span class="comment"># Tensors, but we do not need to keep references to intermediate values since</span></div><div class="line">    <span class="comment"># we are not implementing the backward pass by hand.</span></div><div class="line">    y_pred = x.mm(w1).clamp(min=<span class="number">0</span>).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss using operations on Variables.</span></div><div class="line">    <span class="comment"># Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape</span></div><div class="line">    <span class="comment"># (1,); loss.data[0] is a scalar value holding the loss.</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass. This call will compute the</span></div><div class="line">    <span class="comment"># gradient of loss with respect to all Variables with requires_grad=True.</span></div><div class="line">    <span class="comment"># After this call w1.grad and w2.grad will be Variables holding the gradient</span></div><div class="line">    <span class="comment"># of the loss with respect to w1 and w2 respectively.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent; w1.data and w2.data are Tensors,</span></div><div class="line">    <span class="comment"># w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are</span></div><div class="line">    <span class="comment"># Tensors.</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure></p>
<h2 id="PyTorch-Defining-new-autograd-functions"><a href="#PyTorch-Defining-new-autograd-functions" class="headerlink" title="PyTorch:Defining new autograd functions"></a>PyTorch:Defining new autograd functions</h2><p>在PyTorch中，我们可以通过定义一个子类torch.autograd.Function并实现forward 和backward函数来轻松地定义自己的autograd运算符。然后，我们可以通过构造一个实例并将其称为函数，传递包含输入数据的变量来使用我们的新的自动格式运算符。</p>
<p>在这个例子中，我们定义了我们自己的自定义自整定函数来执行ReLU非线性，并用它来实现我们的两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    We can implement our own custom autograd Functions by subclassing</div><div class="line">    torch.autograd.Function and implementing the forward and backward passes</div><div class="line">    which operate on Tensors.</div><div class="line">    """</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward pass we receive a Tensor containing the input and return a</div><div class="line">        Tensor containing the output. You can cache arbitrary Tensors for use in the</div><div class="line">        backward pass using the save_for_backward method.</div><div class="line">        """</div><div class="line">        self.save_for_backward(input)</div><div class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the backward pass we receive a Tensor containing the gradient of the loss</div><div class="line">        with respect to the output, and we need to compute the gradient of the loss</div><div class="line">        with respect to the input.</div><div class="line">        """</div><div class="line">        input, = self.saved_tensors</div><div class="line">        grad_input = grad_output.clone()</div><div class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></div><div class="line">        <span class="keyword">return</span> grad_input</div><div class="line"></div><div class="line"></div><div class="line">dtype = torch.FloatTensor</div><div class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></div><div class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></div><div class="line">    relu = MyReLU()</div><div class="line"></div><div class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></div><div class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></div><div class="line">    y_pred = relu(x.mm(w1)).mm(w2)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update weights using gradient descent</span></div><div class="line">    w1.data -= learning_rate * w1.grad.data</div><div class="line">    w2.data -= learning_rate * w2.grad.data</div><div class="line"></div><div class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></div><div class="line">    w1.grad.data.zero_()</div><div class="line">    w2.grad.data.zero_()</div></pre></td></tr></table></figure></p>
<h2 id="TensorFlow-Static-Graphs"><a href="#TensorFlow-Static-Graphs" class="headerlink" title="TensorFlow: Static Graphs"></a>TensorFlow: Static Graphs</h2><p>PyTorch autograd看起来很像TensorFlow：在两个框架中我们定义一个计算图，并使用自动差分来计算梯度。两者之间的最大区别在于TensorFlow的计算图是静态的，PyTorch使用 动态计算图。</p>
<p>在TensorFlow中，我们定义了一次计算图，然后一遍又一遍地执行相同的图，可能会将不同的输入数据提供给图形。在PyTorch中，每个前进传递定义了一个新的计算图。</p>
<p>静态图是很好的，因为你可以优化前面的图形; 例如，框架可能决定融合一些图形操作以获得效率，或者提出一种将图形分布在多个GPU或许多机器上的策略。如果您一遍又一遍地重复使用相同的图表，那么这个潜在的昂贵的前期优化可以被分摊，因为同一个图表一遍又一遍地重新运行。</p>
<p>静态和动态图不同的一个方面是控制流程。对于某些型号，我们可能希望对每个数据点执行不同的计算; 例如，对于每个数据点，可以展开不同数量的时间步长的循环网络; 这个展开可以被实现为循环。使用静态图形，循环构造需要是图形的一部分; 由于这个原因，TensorFlow提供了诸如tf.scan将循环嵌入图中的操作符。使用动态图表，情况更简单：由于我们为每个示例动态构建图表，因此我们可以使用正常的命令式流程控制来执行每个输入不同的计算。</p>
<p>为了与上面的PyTorch autograd示例进行对比，我们使用TensorFlow来简化两层网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># First we set up the computational graph:</span></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create placeholders for the input and target data; these will be filled</span></div><div class="line"><span class="comment"># with real data when we execute the graph.</span></div><div class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_in))</div><div class="line">y = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, D_out))/</div><div class="line"></div><div class="line"><span class="comment"># Create Variables for the weights and initialize them with random data.</span></div><div class="line"><span class="comment"># A TensorFlow Variable persists its value across executions of the graph.</span></div><div class="line">w1 = tf.Variable(tf.random_normal((D_in, H)))</div><div class="line">w2 = tf.Variable(tf.random_normal((H, D_out)))</div><div class="line"></div><div class="line"><span class="comment"># Forward pass: Compute the predicted y using operations on TensorFlow Tensors.</span></div><div class="line"><span class="comment"># Note that this code does not actually perform any numeric operations; it</span></div><div class="line"><span class="comment"># merely sets up the computational graph that we will later execute.</span></div><div class="line">h = tf.matmul(x, w1)</div><div class="line">h_relu = tf.maximum(h, tf.zeros(<span class="number">1</span>))</div><div class="line">y_pred = tf.matmul(h_relu, w2)</div><div class="line"></div><div class="line"><span class="comment"># Compute loss using operations on TensorFlow Tensors</span></div><div class="line">loss = tf.reduce_sum((y - y_pred) ** <span class="number">2.0</span>)</div><div class="line"></div><div class="line"><span class="comment"># Compute gradient of the loss with respect to w1 and w2.</span></div><div class="line">grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])</div><div class="line"></div><div class="line"><span class="comment"># Update the weights using gradient descent. To actually update the weights</span></div><div class="line"><span class="comment"># we need to evaluate new_w1 and new_w2 when executing the graph. Note that</span></div><div class="line"><span class="comment"># in TensorFlow the the act of updating the value of the weights is part of</span></div><div class="line"><span class="comment"># the computational graph; in PyTorch this happens outside the computational</span></div><div class="line"><span class="comment"># graph.</span></div><div class="line">learning_rate = <span class="number">1e-6</span></div><div class="line">new_w1 = w1.assign(w1 - learning_rate * grad_w1)</div><div class="line">new_w2 = w2.assign(w2 - learning_rate * grad_w2)</div><div class="line"></div><div class="line"><span class="comment"># Now we have built our computational graph, so we enter a TensorFlow session to</span></div><div class="line"><span class="comment"># actually execute the graph.</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># Run the graph once to initialize the Variables w1 and w2.</span></div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">    <span class="comment"># Create numpy arrays holding the actual data for the inputs x and targets</span></div><div class="line">    <span class="comment"># y</span></div><div class="line">    x_value = np.random.randn(N, D_in)</div><div class="line">    y_value = np.random.randn(N, D_out)</div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">        <span class="comment"># Execute the graph many times. Each time it executes we want to bind</span></div><div class="line">        <span class="comment"># x_value to x and y_value to y, specified with the feed_dict argument.</span></div><div class="line">        <span class="comment"># Each time we execute the graph we want to compute the values for loss,</span></div><div class="line">        <span class="comment"># new_w1, and new_w2; the values of these Tensors are returned as numpy</span></div><div class="line">        <span class="comment"># arrays.</span></div><div class="line">        loss_value, _, _ = sess.run([loss, new_w1, new_w2],</div><div class="line">                                    feed_dict=&#123;x: x_value, y: y_value&#125;)</div><div class="line">        print(loss_value)</div></pre></td></tr></table></figure></p>
<h1 id="nn-module"><a href="#nn-module" class="headerlink" title="nn module"></a>nn module</h1><h2 id="PyTorch-nn"><a href="#PyTorch-nn" class="headerlink" title="PyTorch: nn"></a>PyTorch: nn</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model as a sequence of layers. nn.Sequential</span></div><div class="line"><span class="comment"># is a Module which contains other Modules, and applies them in sequence to</span></div><div class="line"><span class="comment"># produce its output. Each Linear Module computes output from input using a</span></div><div class="line"><span class="comment"># linear function, and holds internal Variables for its weight and bias.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line"></div><div class="line"><span class="comment"># The nn package also contains definitions of popular loss functions; in this</span></div><div class="line"><span class="comment"># case we will use Mean Squared Error (MSE) as our loss function.</span></div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model. Module objects</span></div><div class="line">    <span class="comment"># override the __call__ operator so you can call them like functions. When</span></div><div class="line">    <span class="comment"># doing so you pass a Variable of input data to the Module and it produces</span></div><div class="line">    <span class="comment"># a Variable of output data.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss. We pass Variables containing the predicted and true</span></div><div class="line">    <span class="comment"># values of y, and the loss function returns a Variable containing the</span></div><div class="line">    <span class="comment"># loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero the gradients before running the backward pass.</span></div><div class="line">    model.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to all the learnable</span></div><div class="line">    <span class="comment"># parameters of the model. Internally, the parameters of each Module are stored</span></div><div class="line">    <span class="comment"># in Variables with requires_grad=True, so this call will compute gradients for</span></div><div class="line">    <span class="comment"># all learnable parameters in the model.</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Update the weights using gradient descent. Each parameter is a Variable, so</span></div><div class="line">    <span class="comment"># we can access its data and gradients like we did before.</span></div><div class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</div><div class="line">        param.data -= learning_rate * param.grad.data</div></pre></td></tr></table></figure>
<h2 id="PyTorch-optim"><a href="#PyTorch-optim" class="headerlink" title="PyTorch: optim"></a>PyTorch: optim</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables.</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the nn package to define our model and loss function.</span></div><div class="line">model = torch.nn.Sequential(</div><div class="line">    torch.nn.Linear(D_in, H),</div><div class="line">    torch.nn.ReLU(),</div><div class="line">    torch.nn.Linear(H, D_out),</div><div class="line">)</div><div class="line">loss_fn = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Use the optim package to define an Optimizer that will update the weights of</span></div><div class="line"><span class="comment"># the model for us. Here we will use Adam; the optim package contains many other</span></div><div class="line"><span class="comment"># optimization algoriths. The first argument to the Adam constructor tells the</span></div><div class="line"><span class="comment"># optimizer which Variables it should update.</span></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: compute predicted y by passing x to the model.</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss.</span></div><div class="line">    loss = loss_fn(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Before the backward pass, use the optimizer object to zero all of the</span></div><div class="line">    <span class="comment"># gradients for the variables it will update (which are the learnable weights</span></div><div class="line">    <span class="comment"># of the model)</span></div><div class="line">    optimizer.zero_grad()</div><div class="line"></div><div class="line">    <span class="comment"># Backward pass: compute gradient of the loss with respect to model</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    loss.backward()</div><div class="line"></div><div class="line">    <span class="comment"># Calling the step function on an Optimizer makes an update to its</span></div><div class="line">    <span class="comment"># parameters</span></div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
<h2 id="PyTorch-Custom-nn-Modules"><a href="#PyTorch-Custom-nn-Modules" class="headerlink" title="PyTorch: Custom nn Modules"></a>PyTorch: Custom nn Modules</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we instantiate two nn.Linear modules and assign them as</div><div class="line">        member variables.</div><div class="line">        """</div><div class="line">        super(TwoLayerNet, self).__init__()</div><div class="line">        self.linear1 = torch.nn.Linear(D_in, H)</div><div class="line">        self.linear2 = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the forward function we accept a Variable of input data and we must return</div><div class="line">        a Variable of output data. We can use Modules defined in the constructor as</div><div class="line">        well as arbitrary operators on Variables.</div><div class="line">        """</div><div class="line">        h_relu = self.linear1(x).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.linear2(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = TwoLayerNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. The call to model.parameters()</span></div><div class="line"><span class="comment"># in the SGD constructor will contain the learnable parameters of the two</span></div><div class="line"><span class="comment"># nn.Linear modules which are members of the model.</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
<h2 id="PyTorch-Control-Flow-Weight-Sharing"><a href="#PyTorch-Control-Flow-Weight-Sharing" class="headerlink" title="PyTorch: Control Flow + Weight Sharing"></a>PyTorch: Control Flow + Weight Sharing</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">import</span> torch</div><div class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DynamicNet</span><span class="params">(torch.nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        In the constructor we construct three nn.Linear instances that we will use</div><div class="line">        in the forward pass.</div><div class="line">        """</div><div class="line">        super(DynamicNet, self).__init__()</div><div class="line">        self.input_linear = torch.nn.Linear(D_in, H)</div><div class="line">        self.middle_linear = torch.nn.Linear(H, H)</div><div class="line">        self.output_linear = torch.nn.Linear(H, D_out)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3</div><div class="line">        and reuse the middle_linear Module that many times to compute hidden layer</div><div class="line">        representations.</div><div class="line"></div><div class="line">        Since each forward pass builds a dynamic computation graph, we can use normal</div><div class="line">        Python control-flow operators like loops or conditional statements when</div><div class="line">        defining the forward pass of the model.</div><div class="line"></div><div class="line">        Here we also see that it is perfectly safe to reuse the same Module many</div><div class="line">        times when defining a computational graph. This is a big improvement from Lua</div><div class="line">        Torch, where each Module could be used only once.</div><div class="line">        """</div><div class="line">        h_relu = self.input_linear(x).clamp(min=<span class="number">0</span>)</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(random.randint(<span class="number">0</span>, <span class="number">3</span>)):</div><div class="line">            h_relu = self.middle_linear(h_relu).clamp(min=<span class="number">0</span>)</div><div class="line">        y_pred = self.output_linear(h_relu)</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></div><div class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></div><div class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></div><div class="line"></div><div class="line"><span class="comment"># Create random Tensors to hold inputs and outputs, and wrap them in Variables</span></div><div class="line">x = Variable(torch.randn(N, D_in))</div><div class="line">y = Variable(torch.randn(N, D_out), requires_grad=<span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># Construct our model by instantiating the class defined above</span></div><div class="line">model = DynamicNet(D_in, H, D_out)</div><div class="line"></div><div class="line"><span class="comment"># Construct our loss function and an Optimizer. Training this strange model with</span></div><div class="line"><span class="comment"># vanilla stochastic gradient descent is tough, so we use momentum</span></div><div class="line">criterion = torch.nn.MSELoss(size_average=<span class="keyword">False</span>)</div><div class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-4</span>, momentum=<span class="number">0.9</span>)</div><div class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</div><div class="line">    <span class="comment"># Forward pass: Compute predicted y by passing x to the model</span></div><div class="line">    y_pred = model(x)</div><div class="line"></div><div class="line">    <span class="comment"># Compute and print loss</span></div><div class="line">    loss = criterion(y_pred, y)</div><div class="line">    print(t, loss.data[<span class="number">0</span>])</div><div class="line"></div><div class="line">    <span class="comment"># Zero gradients, perform a backward pass, and update the weights.</span></div><div class="line">    optimizer.zero_grad()</div><div class="line">    loss.backward()</div><div class="line">    optimizer.step()</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Tensor&quot;&gt;&lt;a href=&quot;#Tensor&quot; class=&quot;headerlink&quot; title=&quot;Tensor&quot;&gt;&lt;/a&gt;Tensor&lt;/h1&gt;&lt;h2 id=&quot;Warm-up-numpy&quot;&gt;&lt;a href=&quot;#Warm-up-numpy&quot; class=&quot;he
    
    </summary>
    
      <category term="PyTorch" scheme="http://ynuwm.github.io/categories/PyTorch/"/>
    
    
      <category term="Python" scheme="http://ynuwm.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>BAT机器学习面试1000题系列</title>
    <link href="http://ynuwm.github.io/2017/10/10/BAT%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%951000%E9%A2%98%E7%B3%BB%E5%88%97/"/>
    <id>http://ynuwm.github.io/2017/10/10/BAT机器学习面试1000题系列/</id>
    <published>2017-10-10T12:03:00.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="机器学习" scheme="http://ynuwm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="面试" scheme="http://ynuwm.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>知乎看山杯夺冠记(转)</title>
    <link href="http://ynuwm.github.io/2017/10/08/%E7%9F%A5%E4%B9%8E%E7%9C%8B%E5%B1%B1%E6%9D%AF%E5%A4%BA%E5%86%A0%E8%AE%B0-%E8%BD%AC/"/>
    <id>http://ynuwm.github.io/2017/10/08/知乎看山杯夺冠记-转/</id>
    <published>2017-10-08T14:14:51.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/28923961" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/28923961</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/28923961&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://zhuanlan.zhihu.com/p/28923961&lt;/a&gt;&lt;/p&gt;

    
    </summary>
    
      <category term="深度学习" scheme="http://ynuwm.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文本分类" scheme="http://ynuwm.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>913周会小结</title>
    <link href="http://ynuwm.github.io/2017/09/13/913%E5%91%A8%E4%BC%9A%E5%B0%8F%E7%BB%93/"/>
    <id>http://ynuwm.github.io/2017/09/13/913周会小结/</id>
    <published>2017-09-13T02:20:19.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>1,好的人脉和环境真的使人看起来不一样。<br>脚踏实地的钻研的同时也不要忘了交流的重要性。信息的获取不能局限于一个小的范围内。真的，跟一群整天安于现状不求上进的人在一起，自己也会逐渐磨灭前进的意志，在无声无息中。玩游戏,娱乐肯定是必须的，但如果不追求段位的提升，技术的精湛，那多少年过去了，你有何资格说曾经哥也玩过这个游戏。人始终是环境产物，你不明白为何进入传销的人再也走不出来，其实就像此时此刻的你坚信的一些观念，习惯等等其实也就是你所处大环境的东西。要做的不受外界环境和周围人的影响是难的，这大抵就是成大事者必备的不同常人的意志和信念。为什么我很不喜欢云南更多的是因为这里的环境不是我想要的，平台不够好，资源不够多。</p>
<p>２，</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1,好的人脉和环境真的使人看起来不一样。&lt;br&gt;脚踏实地的钻研的同时也不要忘了交流的重要性。信息的获取不能局限于一个小的范围内。真的，跟一群整天安于现状不求上进的人在一起，自己也会逐渐磨灭前进的意志，在无声无息中。玩游戏,娱乐肯定是必须的，但如果不追求段位的提升，技术的精湛
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="开会记录" scheme="http://ynuwm.github.io/tags/%E5%BC%80%E4%BC%9A%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>深度学习和神经网络－吴恩达2017</title>
    <link href="http://ynuwm.github.io/2017/09/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%8D%E5%90%B4%E6%81%A9%E8%BE%BE2017/"/>
    <id>http://ynuwm.github.io/2017/09/09/深度学习和神经网络－吴恩达2017/</id>
    <published>2017-09-09T13:47:45.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第一周-深度学习概论"><a href="#第一周-深度学习概论" class="headerlink" title="第一周　深度学习概论"></a>第一周　深度学习概论</h1><h2 id="欢迎来到深度学习工程微专业"><a href="#欢迎来到深度学习工程微专业" class="headerlink" title="欢迎来到深度学习工程微专业"></a>欢迎来到深度学习工程微专业</h2><h2 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h2><h2 id="用神经网络进行监督学习"><a href="#用神经网络进行监督学习" class="headerlink" title="用神经网络进行监督学习"></a>用神经网络进行监督学习</h2><h2 id="为什么深度学习会兴起"><a href="#为什么深度学习会兴起" class="headerlink" title="为什么深度学习会兴起"></a>为什么深度学习会兴起</h2><h2 id="关于这门课"><a href="#关于这门课" class="headerlink" title="关于这门课"></a>关于这门课</h2><p><img src="/2017/09/09/深度学习和神经网络－吴恩达2017/0001.jpg" alt="img"></p>
<p><img src="/2017/09/09/深度学习和神经网络－吴恩达2017/0000.jpg" alt="img"></p>
<h1 id="第二周-神经网络基础"><a href="#第二周-神经网络基础" class="headerlink" title="第二周　神经网络基础"></a>第二周　神经网络基础</h1><h2 id="二分分类"><a href="#二分分类" class="headerlink" title="二分分类"></a>二分分类</h2><h2 id="logistic-回归"><a href="#logistic-回归" class="headerlink" title="logistic 回归"></a>logistic 回归</h2><h2 id="logistic-回归损失函数"><a href="#logistic-回归损失函数" class="headerlink" title="logistic 回归损失函数"></a>logistic 回归损失函数</h2><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><h2 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h2><h2 id="更多导数的例子"><a href="#更多导数的例子" class="headerlink" title="更多导数的例子"></a>更多导数的例子</h2><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><h2 id="计算图的导数计算"><a href="#计算图的导数计算" class="headerlink" title="计算图的导数计算"></a>计算图的导数计算</h2><h2 id="logistic-回归中的梯度下降法"><a href="#logistic-回归中的梯度下降法" class="headerlink" title="logistic 回归中的梯度下降法"></a>logistic 回归中的梯度下降法</h2><h2 id="m个样本的梯度下降"><a href="#m个样本的梯度下降" class="headerlink" title="m个样本的梯度下降"></a>m个样本的梯度下降</h2><h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><h2 id="向量化的更多例子"><a href="#向量化的更多例子" class="headerlink" title="向量化的更多例子"></a>向量化的更多例子</h2><h2 id="向量化logistic-回归"><a href="#向量化logistic-回归" class="headerlink" title="向量化logistic 回归"></a>向量化logistic 回归</h2><h2 id="向量化logistic-回归的梯度输出"><a href="#向量化logistic-回归的梯度输出" class="headerlink" title="向量化logistic 回归的梯度输出"></a>向量化logistic 回归的梯度输出</h2><h2 id="python中的广播"><a href="#python中的广播" class="headerlink" title="python中的广播"></a>python中的广播</h2><h2 id="关于Python-Numpy-向量的说明"><a href="#关于Python-Numpy-向量的说明" class="headerlink" title="关于Python/Numpy 向量的说明"></a>关于Python/Numpy 向量的说明</h2><h2 id="Jupyter-Ipython笔记本的快速指南"><a href="#Jupyter-Ipython笔记本的快速指南" class="headerlink" title="Jupyter/Ipython笔记本的快速指南"></a>Jupyter/Ipython笔记本的快速指南</h2><h2 id="选修-logistic-损失函数的解释"><a href="#选修-logistic-损失函数的解释" class="headerlink" title="(选修)logistic 损失函数的解释"></a>(选修)logistic 损失函数的解释</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;第一周-深度学习概论&quot;&gt;&lt;a href=&quot;#第一周-深度学习概论&quot; class=&quot;headerlink&quot; title=&quot;第一周　深度学习概论&quot;&gt;&lt;/a&gt;第一周　深度学习概论&lt;/h1&gt;&lt;h2 id=&quot;欢迎来到深度学习工程微专业&quot;&gt;&lt;a href=&quot;#欢迎来到深度学习
    
    </summary>
    
      <category term="深度学习" scheme="http://ynuwm.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="神经网络，机器学习" scheme="http://ynuwm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>今宵酒醒何处</title>
    <link href="http://ynuwm.github.io/2017/09/06/%E4%BB%8A%E5%AE%B5%E9%85%92%E9%86%92%E4%BD%95%E5%A4%84/"/>
    <id>http://ynuwm.github.io/2017/09/06/今宵酒醒何处/</id>
    <published>2017-09-06T07:42:20.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="530" height="86" src="//music.163.com/outchain/player?type=2&id=427142762&auto=1&height=66"></iframe>

<p>大雨滂沱整整一个下午<br>打落思念的果实<br>手机突然响了<br>飞机就要航向 远方<br>那时候凝望青藏铁道窗外<br>地中海的蔚蓝 如今你在何方<br>异国早晨一个我慢慢走<br>Ah bon vin bon vin bon vin boire bon vin<br>Ah pas trop pas trop pas trop pas boire trop<br>遗憾是少年时<br>爱与暧昧分不清楚<br>火会熄灭因为风的缘故<br>思念的旅人今宵酒醒何处<br>潇洒是一首诗<br>饮下寂寞在夜深处<br>人会辛苦因为爱的缘故<br>陌生的恋人今宵酒醒何处<br>Ah bon vin bon vin bon vin boire bon vin<br>都门帐饮无绪<br>Ah pas trop pas trop pas trop pas boire trop<br>良辰好景虚设<br>那时候凝望青藏铁道窗外<br>地中海的蔚蓝 如今你在何方<br>异国早晨一个我慢慢走<br>遗憾是少年时<br>爱与暧昧分不清楚<br>火会熄灭因为风的缘故<br>思念的旅人今宵酒醒何处<br>潇洒是一首诗<br>饮下寂寞在夜深处<br>人会辛苦因为爱的缘故<br>陌生的恋人今宵酒醒何处<br>遗憾是少年时<br>爱与暧昧分不清楚<br>火会熄灭因为风的缘故<br>思念的旅人今宵酒醒何处<br>潇洒是一首诗<br>饮下寂寞在夜深处<br>人会辛苦因为爱的缘故<br>陌生的恋人今宵酒醒何处<br>Ah bon vin bon vin bon vin boire bon vin<br>都门帐饮无绪<br>Ah pas trop pas trop pas trop pas boire trop<br>良辰好景虚设<br>Ah bon vin bon vin bon vin boire bon vin<br>都门帐饮无绪<br>Ah pas trop pas trop pas trop pas boire trop<br>良辰好景虚设<br>大雨滂沱整整一个下午<br>打落思念的果实<br>手机突然响了<br>飞机就要航向 远方</p>
]]></content>
    
    <summary type="html">
    
      &lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=&quot;530&quot; height=&quot;86&quot; src=&quot;//music.163.com/outchain/player?type=2&amp;id=
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="周传雄" scheme="http://ynuwm.github.io/tags/%E5%91%A8%E4%BC%A0%E9%9B%84/"/>
    
  </entry>
  
  <entry>
    <title>Welcome back</title>
    <link href="http://ynuwm.github.io/2017/08/25/Welcome-back/"/>
    <id>http://ynuwm.github.io/2017/08/25/Welcome-back/</id>
    <published>2017-08-25T01:39:44.000Z</published>
    <updated>2017-12-01T09:03:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome back VG</p>
<p><img src="/2017/08/25/Welcome-back/000.jpg" alt="img"> </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome back VG&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2017/08/25/Welcome-back/000.jpg&quot; alt=&quot;img&quot;&gt; &lt;/p&gt;

    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="LOL" scheme="http://ynuwm.github.io/tags/LOL/"/>
    
  </entry>
  
  <entry>
    <title>用理工科思维理解世界</title>
    <link href="http://ynuwm.github.io/2017/07/31/%E7%94%A8%E7%90%86%E5%B7%A5%E7%A7%91%E6%80%9D%E7%BB%B4%E7%90%86%E8%A7%A3%E4%B8%96%E7%95%8C/"/>
    <id>http://ynuwm.github.io/2017/07/31/用理工科思维理解世界/</id>
    <published>2017-07-31T04:16:51.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Part-One-反常识思维"><a href="#Part-One-反常识思维" class="headerlink" title="Part One　反常识思维"></a>Part One　反常识思维</h1><h2 id="反常识思维"><a href="#反常识思维" class="headerlink" title="反常识思维"></a>反常识思维</h2><p>有时候他们把自己的价值判断称为“常识”，因为这写判断本来就是从人的原始思维而来的，然而现代社会产生了另一种思维，却是“反常识”的。</p>
<p>Tradeoff：我们不得不在生活中做出各种取舍，而很多的烦恼恰恰来自不愿意或者不知道取舍。</p>
<p>人脑有两套思维系统,”系统１”,”系统２”，前者起自动作用，能对事物给出一个很难被改变的第一印象；而后者费力而缓慢，需要我们集中注意力进行复杂的计算。系统２根本不是计算机的对手，而系统１却比计算机强大的多（谷歌识别猫脸实验）。文人思维是系统１的集大成者，而理工科思维则是系统２的产物。</p>
<p>Tradeoff 要求量化输入和预计输出，这也是理工科思维的最根本的方法。现在到了用了理工科思维取代文人思维的时候了。传统的文人腔已经越来越少出现在主流媒体上，一篇正经讨论现实问题的文章总要做点计算才说的过去。</p>
<h2 id="“舌战群儒”的技术分析"><a href="#“舌战群儒”的技术分析" class="headerlink" title="“舌战群儒”的技术分析"></a>“舌战群儒”的技术分析</h2><p>诸葛亮前往东吴说服孙权抗曹。这一仗是打还是不打，正确的讨论方法是摆事实讲道理，推演各种选择的最可能结局。利弊分析，再做决策。但是舌战群儒这场辩论的主题却不是打不打的问题，而是一种从气势上压过对方一头。　　舌战群儒的技术不是证明对方的结论不对，而是证明对方这个“人”，或者对方代表的势力，不行。表面上是说具体问题，而实际上都是说人。<br>舌战群儒技术一共三招：<br>（１）列举事实证据，暗示对方能力不行。<br>（２）如果比不过事实，比境界<br>（３）你别说我如何如何不堪，著名英雄XXX也曾经如此过</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Part-One-反常识思维&quot;&gt;&lt;a href=&quot;#Part-One-反常识思维&quot; class=&quot;headerlink&quot; title=&quot;Part One　反常识思维&quot;&gt;&lt;/a&gt;Part One　反常识思维&lt;/h1&gt;&lt;h2 id=&quot;反常识思维&quot;&gt;&lt;a href=&quot;#
    
    </summary>
    
      <category term="读书笔记" scheme="http://ynuwm.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="思维" scheme="http://ynuwm.github.io/tags/%E6%80%9D%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>719周老师开会记录</title>
    <link href="http://ynuwm.github.io/2017/07/20/719%E5%91%A8%E8%80%81%E5%B8%88%E5%BC%80%E4%BC%9A%E8%AE%B0%E5%BD%95/"/>
    <id>http://ynuwm.github.io/2017/07/20/719周老师开会记录/</id>
    <published>2017-07-20T00:02:50.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>1，做科研其实是很简单的事，掌握一定的手段，发现未知问题，用现有手段去解决问题。在此过程中掌握新手段，再发现新问题，解决新问题。</p>
<p>2，人生的高度取决于你的能力。人可能有很多想法，但是能实现的有多少。做论文是一样的，想法很多，手段单一是弄不成的。</p>
<p>3，为什么总想着比下有余，却不想想比上不足，。为什么不向更优秀的人看齐。</p>
<p>4，有什么事情一定要立刻去做。用最少的时间做最多的事情。</p>
<p>5，别人能做到的你也要做到。有时候不是事情太难，而是我们根本没花多少时间去钻研。</p>
<p>6，都要努力咯。没论文以后申请项目就难。硬件就不要想了。</p>
<p>7，计量党建学，重点实验室。</p>
<p><font color="#E10602/font"><strong><br>——— 来自于 Deepin 15.4 桌面版  &amp; Moeditor V_0.1.1</strong></font></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1，做科研其实是很简单的事，掌握一定的手段，发现未知问题，用现有手段去解决问题。在此过程中掌握新手段，再发现新问题，解决新问题。&lt;/p&gt;
&lt;p&gt;2，人生的高度取决于你的能力。人可能有很多想法，但是能实现的有多少。做论文是一样的，想法很多，手段单一是弄不成的。&lt;/p&gt;
&lt;p&gt;
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="开会记录" scheme="http://ynuwm.github.io/tags/%E5%BC%80%E4%BC%9A%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>墙外的世界</title>
    <link href="http://ynuwm.github.io/2017/07/14/%E5%A2%99%E5%A4%96%E7%9A%84%E4%B8%96%E7%95%8C/"/>
    <id>http://ynuwm.github.io/2017/07/14/墙外的世界/</id>
    <published>2017-07-14T03:37:16.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="racaljk-hosts"><a href="#racaljk-hosts" class="headerlink" title="racaljk/hosts"></a><strong>racaljk/hosts</strong></h2><p>目前更新频率较高，关注人也多。正是因为用的人多吧，所以IP坏的也快。<br><strong>开源地址</strong>：<a href="https://github.com/racaljk/hosts" target="_blank" rel="external">https://github.com/racaljk/hosts</a><br><strong>Hosts地址</strong>：<a href="https://raw.githubusercontent.com/racaljk/hosts/master/hosts" target="_blank" rel="external">https://raw.githubusercontent.com/racaljk/hosts/master/hosts</a></p>
<h2 id="wangchunming-2017hosts"><a href="#wangchunming-2017hosts" class="headerlink" title="wangchunming/2017hosts"></a><strong>wangchunming/2017hosts</strong></h2><p>可看YouTube，PC和手机不同的Hosts<br><strong>开源地址</strong>：<a href="https://github.com/wangchunming/2017hosts" target="_blank" rel="external">https://github.com/wangchunming/2017hosts</a><br><strong>Hosts地址</strong>：<br><a href="https://raw.githubusercontent.com/wangchunming/2017hosts/master/hosts-pc" target="_blank" rel="external">https://raw.githubusercontent.com/wangchunming/2017hosts/master/hosts-pc</a><br><a href="https://raw.githubusercontent.com/wangchunming/2017hosts/master/hosts-mobile" target="_blank" rel="external">https://raw.githubusercontent.com/wangchunming/2017hosts/master/hosts-mobile</a></p>
<h2 id="lennylxx-ipv6-hosts"><a href="#lennylxx-ipv6-hosts" class="headerlink" title="lennylxx/ipv6-hosts"></a><strong>lennylxx/ipv6-hosts</strong></h2><p>特点很明显IPV6嘛<br><strong>开源地址</strong>：<a href="https://github.com/lennylxx/ipv6-hosts" target="_blank" rel="external">https://github.com/lennylxx/ipv6-hosts</a><br><strong>Hosts地址</strong>：<a href="https://raw.githubusercontent.com/lennylxx/ipv6-hosts/master/hosts" target="_blank" rel="external">https://raw.githubusercontent.com/lennylxx/ipv6-hosts/master/hosts</a></p>
<p><font color="#E10602/font"><strong><br>——— 来自于 Deepin 15.4 桌面版  &amp; Moeditor V_0.1.1</strong></font></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;racaljk-hosts&quot;&gt;&lt;a href=&quot;#racaljk-hosts&quot; class=&quot;headerlink&quot; title=&quot;racaljk/hosts&quot;&gt;&lt;/a&gt;&lt;strong&gt;racaljk/hosts&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;目前更新频率较高，
    
    </summary>
    
      <category term="翻墙" scheme="http://ynuwm.github.io/categories/%E7%BF%BB%E5%A2%99/"/>
    
    
      <category term="VPN" scheme="http://ynuwm.github.io/tags/VPN/"/>
    
      <category term="hosts" scheme="http://ynuwm.github.io/tags/hosts/"/>
    
  </entry>
  
  <entry>
    <title>Nokia Lumia 1020 拍摄技巧</title>
    <link href="http://ynuwm.github.io/2017/07/04/Nokia-Lumia-1020-%E6%8B%8D%E6%91%84%E6%8A%80%E5%B7%A7/"/>
    <id>http://ynuwm.github.io/2017/07/04/Nokia-Lumia-1020-拍摄技巧/</id>
    <published>2017-07-04T03:22:19.000Z</published>
    <updated>2017-12-01T09:03:47.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摄影专业术语理解"><a href="#摄影专业术语理解" class="headerlink" title="摄影专业术语理解"></a>摄影专业术语理解</h2><h3 id="闪光灯"><a href="#闪光灯" class="headerlink" title="闪光灯"></a>闪光灯</h3><p>在光线不足的情况，或要在明亮背景中照亮前景对象，则非常适合闪光灯。</p>
<h3 id="亮度（曝光值）"><a href="#亮度（曝光值）" class="headerlink" title="亮度（曝光值）"></a>亮度（曝光值）</h3><p>在背景光过强或整个 图像非常明亮的情况下拍照，使用亮度尤其有效。比如在大片雪景或者海滩上拍照。它使照片更亮或者更暗。</p>
<h3 id="手动对焦"><a href="#手动对焦" class="headerlink" title="手动对焦"></a>手动对焦</h3><p>手动对焦可以用来决定在照片上突出哪个对象。通过显示被拍对象与背景之间的距离感，可创建具有景深感的照片。若要拍摄远距离对象或风景，与自动对焦相比，将焦点设为无穷大能更快地排除清晰锐利的画面。</p>
<h3 id="白平衡"><a href="#白平衡" class="headerlink" title="白平衡"></a>白平衡</h3><p>白平衡可控制相机如何在不同光照条件下再现白色。该设置可确保看起来呈白色的物体在照片中也会再现白色。</p>
<h3 id="光圈值"><a href="#光圈值" class="headerlink" title="光圈值"></a>光圈值</h3><p>光圈大小最直观的影响是精深，通俗的说就是背景虚化程度。简单点说就是光圈大（光圈值小），背景虚化能力强；光圈小（光圈值大），背景虚化能力小。</p>
<h3 id="快门速度"><a href="#快门速度" class="headerlink" title="快门速度"></a>快门速度</h3><p>快门速度或曝光时间用于定义相机捕捉光线的时长。1,当需要更多光线，又不希望使用闪光灯时，可以使用较慢的快门速度。２，所有对象都需要保持更长的静止时间，但在这种情况下，诺基亚的ＯＩＳ光学防抖可以帮助拍摄出更清晰的照片。３，较短快门速度非常适合拍摄移动对象，但这种情况下需要有大量的光线。</p>
<h3 id="灵敏度（ISO、感光度）"><a href="#灵敏度（ISO、感光度）" class="headerlink" title="灵敏度（ISO、感光度）"></a>灵敏度（ISO、感光度）</h3><p>灵敏度可控制相机对光线的敏感度。高敏感度值可提高快门速度（缩短曝光时间），但会更加照片中的噪点。低灵敏度值将减少噪点。感光度小（比如ISO100）画质细腻噪点少；感光度大（比如ISO6400、ISO12800）画质非常差，噪点很多。</p>
<h3 id="曝光的核心"><a href="#曝光的核心" class="headerlink" title="曝光的核心"></a>曝光的核心</h3><p>“摄影是用光的艺术”<br>曝光的三个参数：光圈值、快门速度、灵敏度。一张图形象说明三个参数：<br><img src="/2017/07/04/Nokia-Lumia-1020-拍摄技巧/0000.jpg" alt="img"></p>
<h2 id="Nokia-Lumia-1020相机参数"><a href="#Nokia-Lumia-1020相机参数" class="headerlink" title="Nokia Lumia 1020相机参数"></a>Nokia Lumia 1020相机参数</h2><h3 id="相机参数"><a href="#相机参数" class="headerlink" title="相机参数"></a>相机参数</h3><p>后置摄像头：4100 万像素 PureView摄像头<br>后置摄像头光圈：f/2.2<br>相机焦距: 26 毫米<br>传感器类型：CMOS<br>摄像头类型：卡尔·蔡司认证<br>光学防抖：第二代O.I.S.光学防抖<br>闪光灯：LED补光灯+氙气闪光灯</p>
<h3 id="摄影应用程序"><a href="#摄影应用程序" class="headerlink" title="摄影应用程序"></a>摄影应用程序</h3><p>诺基亚专业拍摄: 充分利用诺基亚 Lumia 非凡系列相机：在自动模式下可以拍出效果出众的照片，您也可以像使用 DSLR 相机一样手动控制曝光、快门速度、白平衡和焦距。可以用不同的取景网格帮助构图，并且如果照片没有十分笔直，不要担心 - 可以稍后将它拉直并进行裁减，即便一直重复操作也不会有任何问题。此外，我们还首次在诺基亚 Lumia 非凡系列中采用了 HD 视频模式，可以录制高品质立体声。<br>诺基亚全景: 借助诺基亚易于使用的 Panorama 应用程序拍摄更大的照片。只需拍摄照片，应用程序就会自动将照片整合到一个正好合适的视图中。准备好之后，就可直接发布到社交网站上与朋友共享。<br>创意工作室（需从应用商店下载）: 利用这种快速简便的照片编辑器，制作出效果更加精良的照片。您可使用 Creative Studio 的编辑工具快速调整色彩平衡、去除红眼并使用滤镜。然后直接在社交网络上共享您的照片。<br>Nokia 智能拍摄: 拍摄一系列照片，更轻松地捕捉精彩瞬间。选择最佳照片，或将这些照片合成一张图片，利用闪光灯效果来凸显动感，删除不需要的对象，或选择拍的最好的面孔，来合成最精彩的集体照。<br>诺基亚魅力魔镜: 魅力魔镜是适用于 Windows Phone 的主要美颜应用程序。有了魅力魔镜，您可以通过调色、调整眼睛大小、美白牙齿以及各种美化效果，让自己的照片更加美丽动人。<br>Nokia 动态图片: 照片如电影动画般的神奇组合，创造了看起来几乎栩栩如生的图片。实用的屏幕帮助让您可以选择您图片的动画区域，并轻松地创建和编辑动态图片。您可以通过社交媒体、电子邮件和信息传送功能与朋友分享您的动态照片。</p>
<h2 id="摄影技巧"><a href="#摄影技巧" class="headerlink" title="摄影技巧"></a>摄影技巧</h2><p>１，曝光补偿：光补偿可整体调整画面的亮度。<br>增加曝光补偿可将画面整体变亮，但是画面中原本亮度足够的区域也可能因此而过度曝光，丢失色彩和细节。减少曝光补偿可将画面整体变暗，使得强光下过于明亮的部分清晰的展现出来，但是画面中原本亮度不够的区域可能因此而曝光不足，令细节丢失。曝光补偿不是万能的，过暗的情况下仍然需要调整快门速度、闪光灯或ISO来提高画面亮度。<br>调节曝光补偿的一般原则是：光线比较强烈的时候，可降低 0.3-0.7档的曝光。<br>光线较为不足，同时又无法使用慢速快门的情况下，可调高 0.3-0.7档的曝光。</p>
<p>２，白平衡：在特定光源下拍摄物体有时会出现偏色现象，可以通过加强对应的补色来进行<br>补偿。调节白平衡可让被拍摄的物体在不同的光线环境中呈现出 “本来”的颜<br>色。<br>调节白平衡的一般原则是：<br>一般选择自动白平衡，除非画面颜色严重与物体的真实颜色不符，或有意对画面进行色彩创作。<br>在进行色彩创作时，如果要强调蓝天、大海的颜色时，可设置为荧光灯白平<br>衡，减少画面中的暖色调，令画面偏冷。<br>在进行色彩创作时，如果要强调温馨的感觉以及食物的色泽时，可设置为阴天<br>白平衡，增加画面中的暖色调，令画面偏暖。 </p>
<p>３，夜景<br>使用诺基亚Lumia1020 拍摄夜景的方式大致为两种，一种是手持直接拍<br>摄，借助光学防抖的强大功能得到出色的照片。另一种，是借助诺基亚Lumia1<br>020 的专业拍摄设置拍出媲美单反相机的夜景照片<br>1.将手机固定，推荐使用带有充电功能的诺基亚PD-95G 拍照手柄连接三脚架<br>2.感光度设置为 ISO100<br>3.白平衡设置为荧光灯模式，以便强调天空的蓝色<br>4.手动对焦设置为无限远<br>5.在 “诺基亚专业拍摄”界面中点击 “。。。”，在菜单中选择 “快门延<br>迟”，点击快门拍摄<br>如果拍摄时无法将手机固定，只能手持拍摄，则可按照如下方法拍摄：<br>1.将感光度设置为 ISO800<br>2.手动设置 1/15-1/20之间的快门速度<br>3.端稳相机，点击快门拍摄 </p>
<p>更多：<a href="http://bbs.dospy.com/thread-17393238-1-855-1.html" target="_blank" rel="external">http://bbs.dospy.com/thread-17393238-1-855-1.html</a></p>
<p><font color="#E10602/font"><strong><br>——— 来自于 Deepin 15.4 桌面版  &amp; Moeditor V_0.1.1</strong></font></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;摄影专业术语理解&quot;&gt;&lt;a href=&quot;#摄影专业术语理解&quot; class=&quot;headerlink&quot; title=&quot;摄影专业术语理解&quot;&gt;&lt;/a&gt;摄影专业术语理解&lt;/h2&gt;&lt;h3 id=&quot;闪光灯&quot;&gt;&lt;a href=&quot;#闪光灯&quot; class=&quot;headerlink&quot; ti
    
    </summary>
    
      <category term="摄影" scheme="http://ynuwm.github.io/categories/%E6%91%84%E5%BD%B1/"/>
    
    
      <category term="Lumia" scheme="http://ynuwm.github.io/tags/Lumia/"/>
    
      <category term="照片" scheme="http://ynuwm.github.io/tags/%E7%85%A7%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>One Question</title>
    <link href="http://ynuwm.github.io/2017/06/24/%E4%BB%8A%E5%A4%A9%E5%BC%80%E6%AF%95%E4%B8%9A%E5%85%B8%E7%A4%BC/"/>
    <id>http://ynuwm.github.io/2017/06/24/今天开毕业典礼/</id>
    <published>2017-06-24T03:18:14.000Z</published>
    <updated>2017-12-01T09:03:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>突然想到一个问题：<br>AlphaGo 和 Master 下棋谁更厉害？</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;突然想到一个问题：&lt;br&gt;AlphaGo 和 Master 下棋谁更厉害？&lt;/p&gt;

    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="AlphaGo" scheme="http://ynuwm.github.io/tags/AlphaGo/"/>
    
  </entry>
  
  <entry>
    <title>挥挥手</title>
    <link href="http://ynuwm.github.io/2017/06/13/%E6%8C%A5%E6%8C%A5%E6%89%8B/"/>
    <id>http://ynuwm.github.io/2017/06/13/挥挥手/</id>
    <published>2017-06-13T01:02:38.000Z</published>
    <updated>2017-12-01T09:03:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>天空下着冰冷的雨<br>落在原来的地方<br>化成烟<br>模糊了视线<br>昨天已越来越遥远<br>我没做过任何坏事<br>却为何要经受这番坎坷<br>罢了<br>挥挥手<br>告别昨日的自己</p>
<p>挥挥手<br>等待你出现</p>
<p><img src="/2017/06/13/挥挥手/0000.jpg" alt="img"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;天空下着冰冷的雨&lt;br&gt;落在原来的地方&lt;br&gt;化成烟&lt;br&gt;模糊了视线&lt;br&gt;昨天已越来越遥远&lt;br&gt;我没做过任何坏事&lt;br&gt;却为何要经受这番坎坷&lt;br&gt;罢了&lt;br&gt;挥挥手&lt;br&gt;告别昨日的自己&lt;/p&gt;
&lt;p&gt;挥挥手&lt;br&gt;等待你出现&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2
    
    </summary>
    
      <category term="日记" scheme="http://ynuwm.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
      <category term="心情" scheme="http://ynuwm.github.io/tags/%E5%BF%83%E6%83%85/"/>
    
  </entry>
  
  <entry>
    <title>gensim学习笔记</title>
    <link href="http://ynuwm.github.io/2017/06/04/gensim%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://ynuwm.github.io/2017/06/04/gensim学习笔记/</id>
    <published>2017-06-04T01:18:39.000Z</published>
    <updated>2017-12-01T09:03:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LDA主题模型"><a href="#LDA主题模型" class="headerlink" title="LDA主题模型"></a>LDA主题模型</h1><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>1、<a href="https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2" rel="external" color="blue" target="_blank">中文维基百科数据</a><br>2、gensim中Corpus类处理数据(*xml.bz2)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; wiki = WikiCorpus(&apos;enwiki-20100622-pages-articles.xml.bz2&apos;) </div><div class="line">&gt;&gt;&gt; MmCorpus.serialize(&apos;wiki_en_vocab200k.mm&apos;, wiki)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line">import logging</div><div class="line">import sys</div><div class="line">from gensim.corpora import WikiCorpus</div><div class="line">logging.basicConfig(format=&apos;%(asctime)s: %(levelname)s: %(message)s&apos;, level=logging.INFO)</div><div class="line"></div><div class="line">def help():</div><div class="line">    print &quot;Usage: python process_wiki.py zhwiki-latest-pages-articles.xml.bz2 wiki.zh.txt&quot;</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    if len(sys.argv) &lt; 3:</div><div class="line">        help()</div><div class="line">        sys.exit(1)</div><div class="line">    logging.info(&quot;running %s&quot; % &apos; &apos;.join(sys.argv))</div><div class="line">    inp, outp = sys.argv[1:3]</div><div class="line">    i = 0</div><div class="line"></div><div class="line">    output = open(outp, &apos;w&apos;)</div><div class="line">    wiki = WikiCorpus(inp, lemmatize=False, dictionary=&#123;&#125;)</div><div class="line">    for text in wiki.get_texts():</div><div class="line">        output.write(&quot; &quot;.join(text) + &quot;\n&quot;)</div><div class="line">        i = i + 1</div><div class="line">        if (i % 10000 == 0):</div><div class="line">            logging.info(&quot;Save &quot;+str(i) + &quot; articles&quot;)</div><div class="line">    output.close()</div><div class="line">    logging.info(&quot;Finished saved &quot;+str(i) + &quot;articles&quot;)</div><div class="line">    </div><div class="line">process_wiki_1.py</div></pre></td></tr></table></figure>
<p>3、数据预处理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line"># Traditional Chinese to Simplified Chinese</div><div class="line">echo &quot;opencc: Traditional Chinese to Simplified Chinese...&quot;</div><div class="line">#time opencc -i wiki.zh.txt -o wiki.zh.chs.txt -c zht2zhs.ini</div><div class="line">time opencc -i wiki.zh.txt -o wiki.zh.chs.txt -c t2s.json</div><div class="line"></div><div class="line"># Cut words</div><div class="line">echo &quot;jieba: Cut words...&quot;</div><div class="line">time python -m jieba -d &apos; &apos; wiki.zh.chs.txt &gt; wiki.zh.seg.txt</div><div class="line"></div><div class="line"># Change encode</div><div class="line">echo &quot;iconv: ascii to utf-8...&quot;</div><div class="line">time iconv -c -t UTF-8 &lt; wiki.zh.seg.txt &gt; wiki.zh.seg.utf.txt</div><div class="line"></div><div class="line">process_wiki_2.sh</div></pre></td></tr></table></figure></p>
<p>处理完之后的数据，已经分好词：<br><a href="http://pan.baidu.com/s/1gfMhkcV" target="_blank" rel="external">http://pan.baidu.com/s/1gfMhkcV</a> 密码：gdua</p>
<h2 id="LDA实验"><a href="#LDA实验" class="headerlink" title="LDA实验"></a>LDA实验</h2><p>1、去掉停用词后即可训练lda模型<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">import codecs</div><div class="line">from gensim.models import LdaModel</div><div class="line">from gensim.corpora import Dictionary</div><div class="line"></div><div class="line">train = []</div><div class="line">stopwords = codecs.open(&apos;stopwords.txt&apos;,&apos;r&apos;,encoding=&apos;utf8&apos;).readlines()</div><div class="line">stopwords = [ w.strip() for w in stopwords ]</div><div class="line">fp = codecs.open(&apos;wiki.zh.word.txt&apos;,&apos;r&apos;,encoding=&apos;utf8&apos;)</div><div class="line">for line in fp:</div><div class="line">    line = line.split()</div><div class="line">    train.append([ w for w in line if w not in stopwords ])</div><div class="line"></div><div class="line">dictionary = corpora.Dictionary(train)</div><div class="line">corpus = [ dictionary.doc2bow(text) for text in train ]</div><div class="line">lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=100)</div></pre></td></tr></table></figure></p>
<p>停用词下载：<a href="http://pan.baidu.com/s/1qYnsSLe" target="_blank" rel="external">http://pan.baidu.com/s/1qYnsSLe</a> 密码：s0hc</p>
<p><strong>此外</strong>，gensim也提供了对wiki压缩包直接进行抽取并保存为稀疏矩阵的脚本 make_wiki，可在bash运行下面命令查看用法。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">python -m gensim.scripts.make_wiki</div><div class="line"><span class="comment">#USAGE: make_wiki.py WIKI_XML_DUMP OUTPUT_PREFIX [VOCABULARY_SIZE]</span></div><div class="line">USAGE: make_wiki.py WIKI_XML_DUMP OUTPUT_PREFIX [VOCABULARY_SIZE]</div><div class="line"></div><div class="line">Convert articles from a Wikipedia dump to (sparse) vectors. The input is a</div><div class="line">bz2-compressed dump of Wikipedia articles, <span class="keyword">in</span> XML format.</div><div class="line"></div><div class="line">This actually creates three files:</div><div class="line"></div><div class="line">* `OUTPUT_PREFIX_wordids.txt`: mapping between words and their <span class="built_in">integer</span> ids</div><div class="line">* `OUTPUT_PREFIX_bow.mm`: bag-of-words (word counts) representation, <span class="keyword">in</span></div><div class="line">  Matrix Matrix format</div><div class="line">* `OUTPUT_PREFIX_tfidf.mm`: TF-IDF representation</div><div class="line">* `OUTPUT_PREFIX.tfidf_model`: TF-IDF model dump</div><div class="line"></div><div class="line">python -m gensim.scripts.make_wiki zhwiki-latest-pages-articles.xml.bz2 zhwiki</div></pre></td></tr></table></figure></p>
<p>将文章变成清晰的文本，并以稀疏TF-IDF向量存储。在具体情况可以看<a href="http://radimrehurek.com/gensim/wiki.html#latent-semantic-analysis" rel="external" color="blue" target="_blank">gensim官网</a>，mm后缀表示Matrix Market格式保存的稀疏矩阵.</p>
<p>2、实验部分<br>利用 tfidf.mm 及wordids.txt 训练LDA模型<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line">from gensim import corpora, models</div><div class="line"></div><div class="line"># 语料导入</div><div class="line">id2word = corpora.Dictionary.load_from_text(&apos;zhwiki_wordids.txt&apos;)</div><div class="line">mm = corpora.MmCorpus(&apos;zhwiki_tfidf.mm&apos;)</div><div class="line"></div><div class="line"># 模型训练</div><div class="line">lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100)</div></pre></td></tr></table></figure></p>
<p>3、模型结果<br>训练过程指定参数 num_topics=100, 即训练100个主题，通过print_topics() 和print_topic() 可查看各个主题下的词分布，也可通过save/load 进行模型保存加载。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 打印前20个topic的词分布</div><div class="line">lda.print_topics(20)</div><div class="line"># 打印id为20的topic的词分布</div><div class="line">lda.print_topic(20)</div><div class="line"></div><div class="line">#模型的保存/ 加载</div><div class="line">lda.save(&apos;zhwiki_lda.model&apos;)</div><div class="line">lda = models.ldamodel.LdaModel.load(&apos;zhwiki_lda.model&apos;)</div></pre></td></tr></table></figure></p>
<p>4、主题预测<br>对新文档，转换成bag-of-word后，可进行主题预测。模型差别主要在于主题数的设置，以及语料本身，wiki语料是全领域语料，主题分布并不明显，而且这里使用的语料没有去停止词，得到的结果差强人意。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">test_doc = list(jieba.cut(test_doc))　　  #新文档进行分词</div><div class="line">doc_bow = id2word.doc2bow(test_doc)      #文档转换成bow</div><div class="line">doc_lda = lda[doc_bow]                   #得到新文档的主题分布</div><div class="line">#输出新文档的主题分布</div><div class="line">print doc_lda</div><div class="line">for topic in doc_lda:</div><div class="line">    print &quot;%s\t%f\n&quot;%(lda.print_topic(topic[0]), topic[1])</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;LDA主题模型&quot;&gt;&lt;a href=&quot;#LDA主题模型&quot; class=&quot;headerlink&quot; title=&quot;LDA主题模型&quot;&gt;&lt;/a&gt;LDA主题模型&lt;/h1&gt;&lt;h2 id=&quot;准备数据&quot;&gt;&lt;a href=&quot;#准备数据&quot; class=&quot;headerlink&quot; titl
    
    </summary>
    
      <category term="Python" scheme="http://ynuwm.github.io/categories/Python/"/>
    
    
      <category term="gensim" scheme="http://ynuwm.github.io/tags/gensim/"/>
    
  </entry>
  
</feed>
