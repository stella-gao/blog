<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[考研英语相关]]></title>
    <url>%2F2017%2F12%2F05%2F%E8%80%83%E7%A0%94%E8%8B%B1%E8%AF%AD%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[阅读核心100词]]></content>
      <categories>
        <category>英语</category>
      </categories>
      <tags>
        <tag>考研</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六级英语相关]]></title>
    <url>%2F2017%2F12%2F03%2F%E5%85%AD%E7%BA%A7%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[写作主题词1.顺境与逆境( Favorable Circumstances and adverse Circumstances)2.勤奋(hard working, diligence, painstaking efforts）3.谨慎(prudence and determination）4.坚持/毅力(perseverance，persistence, determination）5.热情和乐观(enthusiasm and optimism）6.博学和求知(learnedness and seeking knowledge/pursuit of knowledge）7.活力(vitality）8.身强体壮，充满活力(bursting with vitality and good health）9.独立 (independence)10.感恩(gratitude ,gratification)11.创新( creation, innovation, critical mind, critical thinking, unconventional thinking )12.鼓励 (encouragement)13.真诚(sincerity）14.宽容(humanity, love, understanding and tolerance）15.自满和谦逊(Being self-satisfied and being modest）16.勇敢(courage and bravery）17.敬业精神(professional dedication and professional ethics )18.业务水平(competence）19.苦难(suffering and hardship）20.简朴(simplicity）21.谦逊的耐心(moderation and patience）22.适应性(adaptability）23.果敢性(decisiveness）24.羡慕(admiration; )嫉妒(jealousy；envy）25.榜样（example, model）26.时间管理（time management）守时（punctuality） 六级核心救命词救命核心形容词abundance /ə’bʌndəns/ n.丰富，充 absorption /əb’sɔ:pʃn/ n. 吸收，专注 abbreviation /əˌbri:vi’eiʃn/ n.节略，缩写，缩短 hospitality /ˌhɒspi’tæləti/ n.友好，好客 pastime /‘pɑ:staim/ n. 消遣，娱乐 revenue /‘revənju:/ n. 税收，岁入 routine /ru:’ti:n/ n.常规，惯例，例行公事 scorn /skɔ:n/ n. 轻蔑，鄙视 shortage /‘ʃɔ:tidʒ/ n. 短缺，不足 smash /smæʃ/ n. 打碎，粉碎 stability /stə’biləti/ n.稳定(性)，稳固 approach /ə’prəʊtʃ/ n. 方法，途径，接近 vt. 接近，着手处理 vi. 靠近 standard /‘stændəd/ n. 标准 survival /sə’vaivl/ n. 幸存，幸存者，残存物 temperament /‘temprəmənt/ n. 气质，性格 threshold /‘θreʃhəʊld/ n. 开端，入口 tolerance /‘tɒlərəns/ n. 容忍，忍耐力 transaction /træn’zækʃn/ n.处理，办理，交易 trend /trend/ n. 倾向，趋势 transition /træn’ziʃn/ n. 过渡，转变 variation /ˌveəri’eiʃn/ n. 变化，变动 warehouse /‘weəhaʊs/ n. 货仓 community /kə’mju:nəti/ n. 社区，（生态） 群落，共同体，团体 access /‘ækses/ n. 入口，通路，接触 accommodation /əˌkɒmə’deiʃn/ n. 住宿，调节，和解 acknowledgment /ək’nɒlidʒmənt/ n. 承认，感谢，致谢 pattern /‘pætn/ n. 模式 penalty /‘penəlti/ n. 制裁，惩罚 pension /‘penʃn/ n. 养老金 personality /ˌpɜ:sə’næləti/ n. 人格，人性 pledge /pledʒ/ n. 保证，誓言 position /pə’ziʃn/ n. 位置，职位，职务 predecessor /‘pri:disesə(r)/ n. 前任，原有的事物 premise /‘premis/ n. 前提，假设 prescription /pri’skripʃn/ n. 处方 preservation /ˌprezə’veiʃn/ n. 保护，防护 prestige /pre’sti:ʒ/ n. 威信，威望 priority /prai’ɒrəti/ n. 优先(权) prospect /‘prɒspekt/ n. 前景，可能性 rate /reit/ n. 速度 v. 评价，评估 ration /‘ræʃn/ n. 比率，定量，配给量 reflection /ri’flekʃn/ n. 反映，表现 recession /ri’seʃn/ n. (经济)衰退，不景气，后退 reputation /ˌrepju’teiʃn/ n. 名声，声望 reservation /ˌrezə’veiʃn/ n. 贮存，贮藏，预订 illusion /i’lu:ʒn/ n. 错觉，假象 ingredient /in’gri:diənt/ n. 成分 insight /‘insait/ n. 理解，洞察力 inspection /in’spekʃn/ n. 检查，视察 instinct /‘instiŋkt/ n. 本能，直觉 integrity /in’tegrəti/ n. 正直，诚实 intuition /ˌintju’iʃn/ n. 直觉 lease /li:s/ n. 租约，契约 legislation /ˌledʒis’leiʃn/ n. 立法，法律 limitation /ˌlimi’teiʃn/ n. 局限性，缺点 loyalty /‘lɔiəlti/ n. 忠诚，忠心 luxury /‘lʌkʃəri/ n. 奢侈，豪华 manifestation /ˌmænife’steiʃn/ n. 表现(形式) mechanism /‘mekənizəm/ n. 机械装置 minority /mai’nɒrəti/ n. 少数 misfortune /ˌmis’fɔ:tʃu:n/ n. 不幸，灾难 morality /mə’ræləti/ n. 道德，美德 notion /‘nəʊʃn/ n. 概念，观念，理解 obligation /ˌɒbli’geiʃn/ n.(法律上或道义上)责任 occasion /ə’keiʒn/ n. 场合 opponent /ə’pəʊnənt/ n. 敌人，对手 ornament /‘ɔ:nəmənt/ n. 装饰，装饰品 admiration /ˌædmə’reiʃn/ n. 欣赏 advocate /‘ædvəkeit/ n. 提倡者，拥护者 allowance /ə’laʊəns/ n. 津贴 ambition /æm’biʃn/ n. 野心，雄心 analogy /ə’nælədʒi/ n. 相似，模拟，类比 anticipation /ænˌtisi’peiʃn/ n. 预期，期望 appreciation /əˌpri:ʃi’eiʃn/ n. 感谢，感激 array /ə’rei/ n. 陈列，一系列 assurance /ə’ʃʊərəns/ n. 保证 multivitamin /ˌmʌlti’vitəmin/ adj. 多种维他命的 n. 多种维他命剂 blunder /‘blʌndə(r)/ n. 错误，大错 budget /‘bʌdʒit/ n. 预算 capability /ˌkeipə’biləti/ n. 能力，才能 cash /kæʃ/ n. 现金 circulation /ˌsɜ:kjə’leiʃn/ n. 流通，传播，发行量 commitment /kə’mitmənt/ n. 承诺，许诺 compensation /ˌkɒmpen’seiʃn/ n. 补偿，赔偿 consideration /kənˌsidə’reiʃn/ n. 考虑 distinction /di’stiŋkʃn/ n. 区分，辨别 emergency /i’mɜ:dʒənsi/ n. 紧急情况 encouragement /in’kʌridʒmənt/ n. 鼓励 essence /‘esns/ n. 本质 estimate /‘estimət/ n. 估计 expenditure / ik’spenditʃə(r) / n. 开支 extinction /ik’stiŋkʃn/ n. 消失，消灭，灭绝 quality/‘kwɒləti / n. 质量，（统计） 品质，特性，才能 flaw /flɔ:/ n. 裂纹，瑕疵 fortune /‘fɔ:tʃu:n/ n. 财产，大笔的钱 fraction /‘frækʃn/ n. 小部分，一点 symptom/‘simptəm/ n.（临床） 症状，征兆 guarantee /ˌgærən’ti:/ n. 保修单 guilt /gilt/ n. 犯罪 harmony /‘hɑ:məni/ n.协调， 一致，和谐 救命核心名词accessory /ək’sesəri/ n. 配件，附件 adj. 附属的 absurd /əb’sɜ:d/ adj.不合理的，荒唐的 abstract /‘æbstrækt/ n. 摘要 vt. 摘要，提取 adj. 抽象的 absent /‘æbsənt/ adj.不在意的 abnormal /æb’nɔ:ml/ adj.不正常的 absurd /əb’sɜ:d/ adj.荒缪的 abundant /ə’bʌndənt/ adj.丰富的 acute /ə’kju:t/ adj.敏锐的，锋利的 aggressive /ə’gresiv/ adj.侵略的，好斗的，有进取心的 ambiguous /æm’bigjuəs/ adj.模棱两可的，模糊的 ambitious /æm’biʃəs/ adj.有雄心的，有抱负的 appropriate /ə’prəʊpriət/ adj.合适的，恰当的 vt. 占用，拨出 authentic /ɔ:’θentik/ adj.可靠的，可信的 average /‘ævəridʒ/ adj. 一般的，普通的 barren /‘bærən/ adj.贫瘠的，不毛的 bound /baʊnd/ adj.一定的 chronic /‘krɒnik/ adj.慢性的 commentary /‘kɒməntri/ adj.实况报道 compact /kəm’pækt/ adj. 紧凑的，小巧的 competitive /kəm’petətiv/ adj.竞争性的，具有竞争力的 compulsory /kəm’pʌlsəri/ adj.强迫的，强制的，义务的 confidential /ˌkɒnfi’denʃl/ adj. 机紧的，秘密的 conservative /kən’sɜ:vətiv/ adj. 保守的，传统的 consistent /kən’sistənt/ adj. 和……一致 conspicuous /kən’spikjuəs/ adj. 显而易见的，引人注目的 crucial /‘kru:ʃl/ adj. 关键的 current /‘kʌrənt/ adj. 当前的 decent /‘di:snt/ adj. 体面像样的，还不错的 delicate /‘delikət/ adj. 精细的，微妙的，纤弱的 destructive /di’strʌktiv/ adj. 毁灭的 economic /ˌi:kə’nɒmik/ adj. 经济的 elegant /‘eligənt/ adj. 优雅的， 优美的， 精致的 embarrassing /im’bærəsiŋ/ adj. 令人尴尬的 energetic /ˌenə’dʒetik/ adj. 精力充沛的 equivalent /i’kwivələnt/ adj. adj. 等价的，相等的，同意义的 eternal /i’tɜ:nl/ adj. 永恒的，无休止的 exclusive /ik’sklu:siv/ adj. 独有的，排他的 extinct /ik’stiŋkt/ adj. 灭绝的 fake /feik/ adj. 假的，冒充的 fatal /‘feitl/ adj. 致命的，毁灭性的 feasible /‘fi:zəbl/ adj. 可行的 feeble /‘fi:bl/ adj. 脆弱的，虚弱的 gloomy /‘glu:mi/ adj. 暗淡的 individual/ˌindi’vidʒuəl/ adj. 个人的，个别的，独特的 n. 个人，个体 identical /ai’dentikl/ adj. 相同的，一样的 imaginative /i’mædʒinətiv/ adj. 富有想象力的， 爱想象的 inaccessible /ˌinæk’sesəbl/ adj.达不到的，难以接近 inadequate /in’ædikwət/ adj.不充分的，不适当的 incredible /in’kredəbl/ adj. 难以置信的 indifference/in’difrəns/ adj. 不关心的，冷漠的 indignant /in’dignənt/ adj. 生气的，愤怒的 infectious /in’fekʃəs/ adj. 传染的，传染性的 inferior /in’fiəriə(r)/ adj.劣质的，地位较低的，较差的 inherent /in’hiərənt/ adj. 固有的，生来的 inspirational /ˌinspə’reiʃənl/ adj. 灵感的 intent /in’tent/ adj. 专心的，专注的 intricate /‘intrikət/ adj. 复杂精细的 intrinsic /in’trinsik/ adj. 固有的， 本质的， 内在的 irreplaceable /ˌiri’pleisəbl/ adj. 不能替换的， 不能代替的 literal /‘litərəl/ adj.文字的， 字面的 massive /‘mæsiv/ adj. 大规模的，大量的 merciful /‘mɜ:sifl/ adj. 仁慈的，宽大的 mobile /‘məʊbail/ adj. 活动的，流动的 naive /nai’i:v/ adj.天真的， 质朴的 negligible /‘neglidʒəbl/ adj.微不足道的 notorious /nəʊ’tɔ:riəs/ adj.臭名昭著的， 声名狼藉的 obedient /ə’bi:diənt/ adj.服从的， 顺从的 obscure /əb’skjʊə(r)/ adj. 模糊不清的 optimistic /ˌɒpti’mistik/ adj. 乐观的 original /ə’ridʒənl/ adj. 原先的，最早的 pathetic /pə’θetik/ adj. 悲哀的，悲惨的 persistent /pə’sistənt/ adj. 坚持不懈的 potential /pə’tenʃl/ adj. 可能的，潜在的 prevalent /‘prevələnt/ adj. 普遍的，流行的 primitive /‘primətiv/ adj. 原始的，早期的 proficient /prə’fiʃnt/ adj. 熟练的，精通的 profound /prə’faʊnd/ adj. 深刻的，深远的 prominent /‘prɒminənt/ adj. 突出的，杰出的 prompt /prɒmpt/ adj. 即刻的，迅速的 raw /rɔ:/ adj. 自然状态的，未加工的 relevant /‘reləvənt/ adj. 与…有关的 respectable /ri’spektəbl/ adj. 可尊敬的 rewarding /ri’wɔ:diŋ/ adj. 值得的 rough /rʌf/ adj. 粗略的，不精确的 rude /ru:d/ adj. 粗鲁的，不礼貌的 sensitive /‘sensətiv/ adj. 敏感的 sheer /ʃiə(r)/ adj. 完全的，十足的 shrewd /ʃru:d/ adj. 精明的 stationary /‘steiʃənri/ adj. 固定的 subordinate /sə’bɔ:dinət/ adj. 次要的，从属的 subtle /‘sʌtl/ adj. 微妙的， 精巧的，细微的 superficial /ˌsu:pə’fiʃl/ adj. 肤浅的 suspicious /sə’spiʃəs/ adj. 对…怀疑 tedious /‘ti:diəs/ adj. 冗长的，乏味的 trivial /‘triviəl/ adj. 琐碎的， 不重要的 turbulent /‘tɜ:bjələnt/ adj. 动荡的，混乱的 underlying /ˌʌndə’laiiŋ/ adj. 潜在的 versatile /‘vɜ:sətail/ adj. 多才多艺的 vivid /‘vivid/ adj. 生动的，栩栩如生的 void /vɔid/ adj. 无效的 vulnerable /‘vʌlnərəbl/ adj. 易受伤的 worth /wɜ:θ/ adj. 值得 救命核心动词accord /ə’kɔ:d/ v.使符合，使适合 abandon /ə’bændən/ v. 抛弃，放弃，放纵，使沉溺于 abolish /ə’bɒliʃ/ v.废除，取消 acknowledge /ək’nɒlidʒ/ v.承认，答谢，报偿 acquaint /ə’kweint/ v. 熟悉，认识 acquire /ə’kwaiə(r)/ v. (靠己能力努力行为)获得 afford /ə’fɔ:d/ v. 付得起 allege /ə’ledʒ/ v. 断言，宣称 alternate / ɔ:l’tɜ:nət / v. 交替，轮流 / ɔːl’tɜːnit / adj. 交替的 anticipate /æn’tisipeit/ v. 预期 applaud /ə’plɔ:d/ v. 赞扬，称赞 ascend /ə’send/ v. 上升，攀登 ascribe /ə’skraib/ v. 归因于，归功于 assemble /ə’sembl/ v. 集合，聚集 assign /ə’sain/ v.分派，指派(职务，任务) attribute /ə’tribju:t/ v. 归因于 base /beis/ v. 建立在……的基础上 bewilder /bi’wildə(r)/ v. 迷惑，弄糊涂 breed /bri:d/ v. 培育，养育 cling /kliŋ/ v. 坚守，抱紧 coincide /ˌkəʊin’said/ v. 相同，相一致 collaborate /kə’læbəreit/ v. 合著，合作 collide /kə’laid/ v. 互撞，碰撞 commence /kə’mens/ v. 开始 compensate /‘kɒmpenseit/ v. 补偿，赔偿 complement /‘kɒmpliment/ v.与…结合，补充 comply /kəm’plai/ v. 遵守 conceive /kən’si:v/ v. 想出，设想 concern /kən’sɜ:n/ v. 涉及，担忧 condense /kən’dens/ v. 压缩，浓缩 conflict /‘kɒnflikt/ v. 冲突，战争 conform /kən’fɔ:m/ v. 符合，遵守，适应 confront /kən’frʌnt/ v. 面对，面临 conserve /kən’sɜ:v/ v. 保护，保存 consolidate /kən’sɒlideit/ v. 巩固 convey /kən’vei/ v. 表达，传达 crash /kræʃ/ v. (飞机)坠毁 cruise /kru:z/ v. 航行，漫游 dazzle /‘dæzl/ v. 使眩目，耀眼 deceive /di’si:v/ v. 欺骗，哄骗 decline /di’klain/ v. 下降，减少 dedicate /‘dedikeit/ v. 奉献，献身，致力于 defend /di’fend/ v. 为…辩护 defy /di’fai/ v. 违抗，藐视 deny /di’nai/ v. 否认 deprive /di’praiv/ v. 剥夺 derive /di’raiv/ v. 得来，得到 descend /di’send/ v. 下落 deserve /di’zɜ:v/ v. 值得 deviate /‘di:vieit/ v. (使)背离，(使)偏离 disguise /dis’gaiz/ v. 假扮，伪装 dominate /‘dɒmineit/ v. 统治，占据 drain /drein/ v. 渐渐耗尽 duplicate /‘dju:plikeit/ v. 复制，重复 eliminate /i’limineit/ v. 消除 endure /in’djʊə(r)/ v. 忍受，忍耐 enhance /in’hɑ:ns/ v. 提高，增加 enroll /in’rəʊl/ v.使成为……的成员，注册 evoke /i’vəʊk/ v. 引起，唤起 immerse /i’mɜ:s/ v. 使浸没 impose /im’pəʊz/ v. 征税，把…强加于 induce /in’dju:s/ v. 劝诱，诱导 indulge /in’dʌldʒ/ v. 纵容，放任 intend /in’tend/ v. 意欲 interpret /in’tɜ:prit/ v. 解释，说明 jeopardize /‘dʒepədaiz/ v. 危及，损坏 linger /‘liŋgə(r) / v.逗留，徘徊，拖延 locate / ləʊ’keit / v. 位于 magnify /‘mægnifai/ v. 放大 mean /mi:n/ v. 打算，意欲 mingle /‘miŋgl/ v. 混合起来，相混合 minimize /‘minimaiz/ v. 使减到最少，最小化 monitor /‘mɒnitə(r)/ v. 检测，监测 neglect /ni’glekt/ v. 忽视 occupy /‘ɒkjupai/ v. 占领，使忙碌 oppress /ə’pres/ v. 压迫 originate /ə’ridʒineit/ v. 首创，起源 overlap /ˌəʊvə’læp/ v. 部分重叠 overwhelm /ˌəʊvə’welm/ v.压倒，浸没，使不安 permeate /‘pɜ:mieit/ v. 渗入，渗透 prescribe /pri’skraib/ v.指示，规定，处方开药 preside /pri’zaid/ v. 主持 prolong /prə’lɒŋ/ v. 延长，拖延 promise /‘prɒmis/ v. 许诺 propel /prə’pel/ v. 推进，推动 protest /‘prəʊtest/ v. 抗议，反对 provoke /prə’vəʊk/ v. 引起，激起 radiate /‘reidieit/ v. 辐射状发出，从中心向各方伸展出 reconcile /‘rekənsail/ v. 使和好，调解 refresh /ri’freʃ/ v.提神，使清新，使精力恢复 refute /ri’fju:t/ v. 反驳，驳斥，驳倒 remain /ri’mein/ v. 停留，依旧是 repel /ri’pel/ v. 抗御，抵拒 rescue /‘reskju:/ v. 营救，救援 resign /ri’zain/ v. 辞职 resort /ri’zɔ:t/ v. 求助，凭借，诉诸 resume /ri’zju:m/ v. 重新开始，继续 revenge /ri’vendʒ/ v. 报仇，报复 scan /skæn/ v. 细察，审视 scrape /skreip/ v. 剥下，刮下 scratch /skrætʃ/ v. 抓，搔 shrink /ʃriŋk/ v. 收缩，减少 standardize /‘stændədaiz / v. 使标准化 steer /stiə(r)/ v. 驾驶，引导 strengthen /‘streŋθn/ v. 加强，使更强壮 stretch /stretʃ/ v. 伸展 subscribe /səb’skraib/ v. 预订，订阅 suck /sʌk/ v. (用嘴)吸，吞噬，卷入 suppress /sə’pres/ v. 镇压 sustain /sə’stein/ v. 承受，维持 tackle /‘tækl/ v. 解决，处理 tempt /tempt/ v. 引诱，劝诱 terminate /‘tɜ:mineit/ v. 终止，结束 transmit /træns’mit/ v. 传播，传递 verify /‘verifai/ v. 证实，证明 view /vju:/ v. 视为，看做 wreck /rek/ v. (船只)失事 救命核心副词deliberately /di’libərətli/ adv. 故意地，深思熟虑地，审慎地 exclusively /ik’sklu:sivli/ adv. 唯一地，专有地，排外地 explicitly /ik’splisitli/ adv. 明确地 forcibly /‘fɔ:səbli/ adv. 强行地，有力地 formerly /‘fɔ:məli/ adv. 原先地，以前，从前 increasingly /in’kri:siŋli/ adv.越来越多地 inevitably /in’evitəbli/ adv.必然地， 不可避免地 intentionally /in’tenʃənəli/ adv.有意地，故意地 optimistically /ˌɒpti’mistikli/ adv. 乐观地 outwardly /‘aʊtwədli/ adv.表面上，外表上地 presumably /pri’zju:məbli/ adv.大概可能，据推测 simultaneously /ˌsiməl’teiniəsli/ adv.同时发生地 somewhat /‘sʌmwɒt/ adv. 颇为，稍稍，有几分 spontaneously /spɒn’teiniəsli/ adv.自发地， 自然产生 startlingly /‘stɑ:rtliŋli/ adv. 惊人地 triumphantly /trai’ʌmfəntli/ adv.(欣喜)胜利，成功地 unexpectedly /ˌʌnik’spektidli/ adv. 意外地 virtually /‘vɜ:tʃuəli/ adv. 事实上，实际地 救命核心短语adhere to 忠于 after all 毕竟，归根结底 at random 随机地，任意地 break out 突然发生，爆发 break up 打碎 but for 要不是 by far 到目前为止；远，非常 by no means 决不，一点也不 catch on 理解，明白 catch up with 赶上 collide with 碰撞，冲突 come up with 想出，提出，追及，赶上 comment on 评论 contrary to 与……相反 contribute to 有助于，促成 cope with 应付，妥善处理 cut short 打断，制止 do away with 消灭，废除，去掉 do credit to 为……带来光荣 due to 因为 go in for 从事，致力于 go off 爆炸 hang by a thread 千钧一发，岌岌可危 heap praise upon 对……大加称赞 in accordance with 与……一致，按照，根据 in between 在两者之间 in case of 防备，以防 in honor of 为纪念 in response to 响应，反应 in terms of 根据，从……方面来说 in that 因为 in the vicinity of 在附近 keep off 远离，抑制 lay off (暂时)解雇 let alone 更不必说 look into 调查 look on 看待 lose no time 立即 make sense of sth. 讲得通言之有理 of no avail 无用，无效 on file 存档 on no account 决不，绝对不 on the decline 衰落中，衰退中 out of stock 无现货的，脱销的 provided that 假如，若是 pull up 使停下 put away 放好，放起来 regardless of 不管，不顾 result in 导致，结果是，发生 see to 照料，注意 show to 引导，引领 stand for 容忍，接受 take on 承担，接受 take over 接管，接收 take to 对…产生好感，开始喜欢 talk into 说服 that is 即，也就是 turn in 上交 turn out 生产出 turn to 求助于 ward off 防止，避开 with reference to 关于，有关 work out 想出，制订出 worth one’s while 值得花时间做待续…]]></content>
      <categories>
        <category>英语</category>
      </categories>
      <tags>
        <tag>六级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[把信送给加西亚]]></title>
    <url>%2F2017%2F11%2F24%2F%E6%8A%8A%E4%BF%A1%E9%80%81%E7%BB%99%E5%8A%A0%E8%A5%BF%E4%BA%9A%2F</url>
    <content type="text"><![CDATA[在所有与古巴有关的事情中，有一个人常常令我无法忘怀。美西战争爆发以后，美国必须马上与西班牙反抗军首领加西亚将军取得联系。加西亚将军隐藏在古巴辽阔的崇山峻岭中——没有人知道确切的地点，因而无法送信给他。但是，美国总统必须尽快地与他建立合作关系。怎么办呢？有人对总统推荐说：“有一个名叫罗文的人，如果有人能找到加西亚将军，那个人一定就是他。”于是，他们将罗文找来，交给他一封信——写给加西亚的信。关于那个名叫罗文的人，如何拿了信，将它装进一个油纸袋里，打封，吊在胸口藏好，如何在3个星期之后，徒步穿越一个危机四伏的国家，将信交到加西亚手上——这些细节都不是我想说明的，我要强调的重点是：美国总统将一封写给加西亚的信交给了罗文，罗文接过信后，并没有问：“他在哪里？” 喜欢原文中这句话：年轻人所需要的不只是学习书本上的知识，也不只是聆听他人种种的指导，而是更需要一种敬业精神，对上级的托付，立即采取行动，全心全意去完成任务——把信送给加西亚。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>职场</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[南航求职经历]]></title>
    <url>%2F2017%2F11%2F24%2F%E5%8D%97%E8%88%AA%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86%2F</url>
    <content type="text"><![CDATA[我是3,4月份投的简历，那个时候来软院宣讲，研究生很少，去的基本是软院大三的应届生，估计的话收的简历有三四十份。那次说的是要招暑假实习生，但是投完简历就没消息了。直到10月份，突然接到一个广州的电话，但是我没接到，打过去一直在忙，就没怎么管。十月份自己脚崴了，一直在休息，拄着拐杖。10月22号那周的周三也就是10月18号突然收到短信，说我的简历通过了，叫我参加笔试，叫我准备一下，考试时间就是10月22（周日），准备个屁啊，时间太紧我直接裸考的，上学期投完简历我是有买托业资料，做了一段时间没收到消息也就没做了。 说说笔试吧。21号的晚上就去了市区，拄着拐杖太不方便了，地上是湿的刚刚下完雨。在小菜园立交桥那个地方就迷路了，道路太复杂，以前又没怎么去过，边打听边走。走了一个多小时才找到住的地方，累的全身都出汗了，想着就去打打酱油吧，本来就没准备。第二天考试就下大雨了，比较幸运是等我到考点后下起来的。考试的话，30个人只去了13个。行测比较坑，上面显示的是38道题，等提交的时候发现后面还有题，大家好像都没做。英语很简单，偏应用，听力也简单，提前十多分钟做完了。考完直接回来了。然后就等通知。 10月30号收到面试通知说11月1号要过去面试，再次打电话确认我参加不，这次电话又没有接到，不过我打过去了说参加的。然后就是面试，我是6号，一共有9个通过了笔试，其中一个没来，也就是只剩8个，有两个女生报的行销，跟我们不是一个岗位，剩下6个全是男生都是信息开发岗。面试分两轮：第一轮小组讨论，全部6个人坐在一起，讨论15分钟，最后派一个代表总结。抽到的题目是做一个网页系统，同时要面对海量用户，要综合考虑数据的准确性和及时性，问要用到哪些技术。其实面试官这里是不太管你们讨论出什么结果的，他们看的是你的思路，你在小组里扮演的角色。我自己的话，就没说很多话，但是每次我说的都是他们讨论之外的东西，他们讨论热烈的时候，我就说海量用户，那用户数据安全性怎么设计，并发执行的话我们有哪些要注意的等等。 然后小组派一个人总结了。第二轮，多对一面试，有三个面试官，一个是技术，一个是行销的，另一个是其他的。在我前面面完的人，有一个出来我们就问他都问了哪些问题，他说数据结构什么的，最后他说还讲了毕业设计。我当时想这哥们应该没戏了，都研究生了还在讲毕业设计，不显得很low吗，而且面试你给人家讲毕设不觉得这个很普遍吗，研究生阶段很多东西可以讲啊，局限在毕设在我看来已经失败了。刚进去做自我介绍，问我的第一个问题是我的两篇论文，我讲了很多估计他们也不太懂，神经网络LSTM什么的。接着问我说进去之后要用Java我说学过，他说你现在Python用的多，这两个语言的区别，我又balala说了一堆。第三个问题我是想继续读还是出来工作，我想都没想说不想读书了我要工作。好像就没问别的问题了，另外两个人也没有问，当时真的很慌，面试我的时间我觉得是最短的，而且另外两个都没问问题这就完了？然后就出来了。对了，其他7个人都穿正装了，就我没穿，短信上没有说，招聘官网说要穿，由于没有懒得借就没穿。对了笔试的时候碰到了同班同学，他们是最近一个月投的简历。 11月9号收到MAP职业性格测试。认识一起面试的两个，应该是所有面试的都收到了这个测试。 11月22收到Offer。那个说毕设的哥们跟我预测的一样，没收到短信。]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SemEval2017-Task4前几名思路与技巧]]></title>
    <url>%2F2017%2F11%2F18%2FSemEval2017%E5%89%8D%E5%87%A0%E5%90%8D%E6%80%9D%E8%B7%AF%E4%B8%8E%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[No.1 BB_twtr第一，预处理url to ‘url’emotions to ‘smile’,’sadness’…‘sooooo’ to ‘soo’lowercased 第二，100million unlabeled tweets 预训练词向量(Twitter API) 第三，模型(参考论文Ye Zhang and Byron Wallace,2015)卷积核大小２,3,4，每种有２个，对同一个句子卷积，得到６个univariate vectors,然后concat在一起，再经全连接层和softmax层分类LSTM 类似的处理第四，数据Task-A(49693 labeled)Task-BD(30849)Task-CE(18948) 第五，ensemble10 CNNs and 10 LSTMs together through soft voting No.2 DataStories第一，自己写的文本分词器第二，TaskA两层双向LSTM+Att(基于message)第三，TaskBCDE句子和话题分别通过BiLSTM然后concat+att-context(基于话题) No.3 LIAensemble CNN 和 LSTM第一，Word EmbeddingLexical embeddingSentiment embeddings(Multitask-learning)Sentiment embeddings(distant-supervision)Sentiment embeddings(negative-sampling) 第二，句子层特征提取Lexicons:MPQA+NRCEmoticons:number of emoticons grouped in pos,neg,neuAll-caps:number of words in all-capsElongated units:words in which characters are repeated more than wtice(eg,looooool)Punctuation:number of contiguous sequences of severl periods.exclaimation marks and question marks No.4 Senti17第一，HappyTokenizer 处理文本第二，十个卷积网络投票，每个网络训练数据一样，词向量一样，不同的是初始权重]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>semeval</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[这几天的云大]]></title>
    <url>%2F2017%2F11%2F15%2F%E8%BF%99%E5%87%A0%E5%A4%A9%E7%9A%84%E4%BA%91%E5%A4%A7%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>照片</tag>
        <tag>云大</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[综述自然语言处理NLP]]></title>
    <url>%2F2017%2F11%2F15%2F%E7%BB%BC%E8%BF%B0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86NLP%2F</url>
    <content type="text"><![CDATA[前言 自然语言处理是文本挖掘的研究领域之一，是人工智能和语言学领域的分支学科。在此领域中探讨如何处理及运用自然语言。 对于自然语言处理的发展历程，可以从哲学中的经验主义和理性主义说起。基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处理是哲学中的理性主义。在哲学领域中经验主义与理性主义的斗争一直是此消彼长，这种矛盾与斗争也反映在具体科学上，如自然语言处理。 早期的自然语言处理具有鲜明的经验主义色彩。如 1913 年马尔科夫提出马尔科夫随机过程与马尔科夫模型的基础就是“手工查频”，具体说就是统计了《欧根·奥涅金》长诗中元音与辅音出现的频度；1948 年香农把离散马尔科夫的概率模型应用于语言的自动机，同时采用手工方法统计英语字母的频率。 然而这种经验主义到了乔姆斯基时出现了转变。 1956 年乔姆斯基借鉴香农的工作，把有限状态机用作刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型。这些工作非常伟大，为自然语言和形式语言找到了一种统一的数学描述理论，一个叫做“形式语言理论”的新领域诞生了。这个时代，“经验主义”被全盘否定，“理性主义”算是完胜。 然而在 20 世纪 50 年代末到 60 年代中期，经验主义东山再起了。多数学者普遍认为只有详尽的历史语料才能带来靠谱的结论。于是一些比较著名的理论与算法就诞生了，如贝叶斯方法（Bayesian Method）、隐马尔可夫、最大熵、Viterbi 算法、支持向量机之类。世界上第一个联机语料库也是在那个时候的 Brown University 诞生的。 但是总的来说，这个时代依然是基于规则的理性主义的天下，经验主义虽然取得了不俗的成就，却依然没有受到太大的重视。但是金子总会发光的。 90 年代以来，基于统计的自然语言处理就开始大放异彩了。首先是在机器翻译领域取得了突破，因为引入了许多基于语料库的方法（哈钦斯，英国著名学者）。1990 年在芬兰赫尔辛基举办的第 13 届国际计算语言学会议确定的主题是“处理大规模真实文本的理论、方法与工具”，大家的重心开始转向大规模真实文本了，传统的仅仅基于规则的自然语言处理显然力不从心了。学者们认为，大规模语料至少是对基于规则方法有效的补充。 到了 1994~1999 年，经验主义就开始空前繁荣了。如句法剖析、词类标注、参照消解、话语处理的算法几乎把“概率”与“数据”作为标准方法，成为了自然语言处理的主流。 总之，理性主义在自然语言处理的发展史上是有重要地位的，也辉煌了几十年，历史事物常常是此消彼长的，至于谁好谁坏，不是固定的，取决于不同时代的不同历史任务。总的来说，基于规则的理性主义在这个时代被提及得比较少，用的也比较少，主要是由于以下几个缺陷： • 鲁棒性差，过于严格的规则导致对非本质错误的零容忍（这一点在最近的一些新的剖析技术上有所改善）； • 研究强度大，泛化能力差。一个研究要语言学家、语音学家和各种领域的专家配合，在当前大规模文本处理的时间、资源要求下太不划算。且机器学习的方法很难应用，难以普及； • 实践性差。基于统计的经验主义方法可以根据数据集不断对参数进行优化，而基于规则的方法就不可以，这在当前数据量巨大的情况下，影响是致命的，因为前者常常可以通过增大训练集来获得更好的效果，后者则死板许多，结果往往不尽人意。 但理性主义还是有很多优点的，同样经验主义也有很多缺陷，算是各有所长、各有所短。不同学科有不同学科的研究角度，只能说某些角度在某个特定的历史时期对提高生产力“更有用”，所以重视的人更多。但“有用”不代表胜利，暂时的“无用”更不能说是科学层面上的“失败”。尤其是在当前中文自然语言处理发展还不甚成熟的时期，私以为基于统计的方法在很多方面并不完美，“理性主义”的作用空间还很大，需要更多的人去关注、助力。——《统计自然语言处理》宗成庆 自然语言处理涉及的范畴如下（维基百科）： • 中文自动分词（Chinese word segmentation）• 词性标注（Part-of-speech tagging）• 句法分析（Parsing）• 自然语言生成（Natural language generation）• 文本分类（Text categorization）• 信息检索（Information retrieval）• 信息抽取（Information extraction）• 文字校对（Text-proofing）• 问答系统（Question answering）• 机器翻译（Machine translation）• 自动摘要（Automatic summarization） 本文针对其中几个主要领域的研究现状和进展，通过论文、博客等资料，结合自身的学习和实践经历进行浅显地介绍。由于个人实践经验不足，除中文分词、自动文摘、文本分类、情感分析和话题模型方面进行过实际业务的实践，其他方面经验欠缺，若有不当之处，欢迎童鞋们批评指正！ 目录 一. 中文分词 中文分词主要包括词的歧义切分和未登录词识别，主要可以分为基于词典和基于统计的方法，最新的方法是多种方法的混合。从目前汉语分词研究的总体水平看，F1 值已经达到 95% 左右，主要分词错误是由新词造成的，尤其对领域的适应性较差。下面主要介绍一下中文分词存在的主要问题和分词方法。 问题 1.1 歧义切分 切分歧义处理包括两部分内容： • 切分歧义的检测； • 切分歧义的消解。 这两部分在逻辑关系上可分成两个相对独立的步骤。 • 切分歧义的检测。“最大匹配法”（精确的说法应该叫“最长词优先匹配法”） 是最早出现、同时也是最基本的汉语自动分词方法。依扫描句子的方向，又分正向最大匹配 MM（从左向右）和逆向最大匹配 RMM（从右向左）两种。 最大匹配法实际上将切分歧义检测与消解这两个过程合二为一，对输入句子给出唯一的切分可能性，并以之为解。从最大匹配法出发导出了“双向最大匹配法”，即 MM＋ RMM。双向最大匹配法存在着切分歧义检测盲区。 针对切分歧义检测，另外两个有价值的工作是“最少分词法”，这种方法歧义检测能力较双向最大匹配法要强些，产生的可能切分个数仅略有增加；和“全切分法”，这种方法穷举所有可能的切分，实现了无盲区的切分歧义检测，但代价是导致大量的切分“垃圾”。 • 切分歧义的消解。典型的方法包括句法统计和基于记忆的模型。句法统计将自动分词和基于 Markov 链的词性自动标注技术结合起来，利用从人工标注语料库中提取出的词性二元统计规律来消解切分歧义，基于记忆的模型对伪歧义型高频交集型歧义切分，可以把它们的正确（唯一）切分形式预先记录在一张表中，其歧义消解通过直接查表即可实现。 1.2 未登录词识别 未登录词大致包含两大类： • 新涌现的通用词或专业术语等； • 专有名词。如中国人名、外国译名、地名、机构名（泛指机关、团体和其它企事业单位）等。 前一种未登录词理论上是可预期的，能够人工预先添加到词表中（但这也只是理想状态，在真实环境下并不易做到）；后一种未登录词则完全不可预期，无论词表多么庞大，也无法囊括。 真实文本中（即便是大众通用领域），未登录词对分词精度的影响超过了歧义切分。未登录词处理在实用型分词系统中占的份量举足轻重。 • 新涌现的通用词或专业术语。对这类未登录词的处理，一般是在大规模语料库的支持下，先由机器根据某种算法自动生成一张候选词表（无监督的机器学习策略），再人工筛选出其中的新词并补充到词表中。 鉴于经过精加工的千万字、甚至亿字级的汉语分词语料库目前还是水月镜花，所以这个方向上现有的研究无一不以从极大规模生语料库中提炼出的 n 元汉字串之分布（n≥2）为基础。其中汉字之间的结合力通过全局统计量包括互信息、t- 测试差、卡方统计量、字串频等来表示。 • 专有名词。对专有名词的未登录词的处理，首先依据从各类专有名词库中总结出的统计知识 （如姓氏用字及其频度）和人工归纳出的专有名词的某些结构规则，在输入句子中猜测可能成为专有名词的汉字串并给出其置信度，之后利用对该类专有名词有标识意义的紧邻上下文信息（如称谓），以及全局统计量和局部统计量（局部统计量是相对全局统计量而言的，是指从当前文章得到且其有效范围一般仅限于该文章的统计量，通常为字串频），进行进一步的鉴定。 已有的工作涉及了四种常见的专有名词：中国人名的识别、外国译名的识别、中国地名的识别及机构名的识别。 从各家报告的实验结果来看，外国译名的识别效果最好，中国人名次之，中国地名再次之，机构名最差。而任务本身的难度实质上也是遵循这个顺序由小增大。 沈达阳、孙茂松等（1997b）特别强调了局部统计量在未登录词处理中的价值。 方法 2.1 基于词典的方法 在基于词典的方法中，对于给定的词，只有词典中存在的词语能够被识别，其中最受欢迎的方法是最大匹配法（MM），这种方法的效果取决于词典的覆盖度，因此随着新词不断出现，这种方法存在明显的缺点。 2.2 基于统计的方法 基于统计的方法由于使用了概率或评分机制而非词典对文本进行分词而被广泛应用。这种方法主要有三个缺点： 一是这种方法只能识别 OOV（out-of-vocabulary）词而不能识别词的类型，比如只能识别为一串字符串而不能识别出是人名；二是统计方法很难将语言知识融入分词系统，因此对于不符合语言规范的结果需要额外的人工解析；三是在许多现在分词系统中，OOV 词识别通常独立于分词过程。 二. 词性标注 词性标注是指为给定句子中的每个词赋予正确的词法标记，给定一个切好词的句子，词性标注的目的是为每一个词赋予一个类别，这个类别称为词性标记（part-of-speech tag），比如，名词（noun）、动词（verb）、形容词（adjective）等。 它是自然语言处理中重要的和基础的研究课题之一，也是其他许多智能信息处理技术的基础，已被广泛的应用于机器翻译、文字识别、语音识别和信息检索等领域。 词性标注对于后续的自然语言处理工作是一个非常有用的预处理过程，它的准确程度将直接影响到后续的一系列分析处理任务的效果。 长期以来，兼类词的词性歧义消解和未知词的词性识别一直是词性标注领域需要解决的热点问题。当兼类词的词性歧义消解变得困难时，词性的标注就出现了不确定性的问题。而对那些超出了词典收录范围的词语或者新涌现的词语的词性推测，也是一个完整的标注系统所应具备的能力。 词性标注方法 词性标注是一个非常典型的序列标注问题。最初采用的方法是隐马尔科夫生成式模型， 然后是判别式的最大熵模型、支持向量机模型，目前学术界通常采用结构感知器模型和条件随机场模型。 近年来，随着深度学习技术的发展，研究者们也提出了很多有效的基于深层神经网络的词性标注方法。 迄今为止，词性标注主要分为基于规则的和基于统计的方法。 • 规则方法能准确地描述词性搭配之间的确定现象，但是规则的语言覆盖面有限，庞大的规则库的编写和维护工作则显得过于繁重，并且规则之间的优先级和冲突问题也不容易得到满意的解决。 • 统计方法从宏观上考虑了词性之间的依存关系，可以覆盖大部分的语言现象，整体上具有较高的正确率和稳定性，不过其对词性搭配确定现象的描述精度却不如规则方法。 针对这样的情况，如何更好地结合利用统计方法和规则处理手段，使词性标注任务既能够有效地利用语言学家总结的语言规则，又可以充分地发挥统计处理的优势成为了词性标注研究的焦点。 词性标注研究进展 • 词性标注和句法分析联合建模：研究者们发现，由于词性标注和句法分析紧密相关，词性标注和句法分析联合建模可以同时显著提高两个任务准确率。 • 异构数据融合：汉语数据目前存在多个人工标注数据，然而不同数据遵守不同的标注规范，因此称为多源异构数据。近年来，学者们就如何利用多源异构数据提高模型准确率，提出了很多有效的方法，如基于指导特征的方法、基于双序列标注的方法、以及基于神经网络共享表示的方法。 • 基于深度学习的方法：传统词性标注方法的特征抽取过程主要是将固定上下文窗口的词进行人工组合，而深度学习方法能够自动利用非线性激活函数完成这一目标。进一步，如果结合循环神经网络如双向 LSTM，则抽取到的信息不再受到固定窗口的约束，而是考虑整个句子。 除此之外，深度学习的另一个优势是初始词向量输入本身已经刻画了词语之间的相似度信息，这对词性标注非常重要。 三. 句法分析 语言语法的研究有非常悠久的历史，可以追溯到公元前语言学家的研究。不同类型的句法分析体现在句法结构的表示形式不同，实现过程的复杂程度也有所不同。因此，科研人员采用不同的方法构建符合各个语法特点的句法分析系统。其主要分类如下图所示： 下文主要对句法分析技术方法和研究现状进行总结分析： 依存句法分析 依存语法存在一个共同的基本假设：句法结构本质上包含词和词之间的依存（修饰）关系。一个依存关系连接两个词，分别是核心词（head）和依存词（dependent）。依存关系可以细分为不同的类型，表示两个词之间的具体句法关系。 目前研究主要集中在数据驱动的依存句法分析方法，即在训练实例集合上学习得到依存句法分析器，而不涉及依存语法理论的研究。数据驱动的方法的主要优势在于给定较大规模的训练数据，不需要过多的人工干预，就可以得到比较好的模型。因此，这类方法很容易应用到新领域和新语言环境。 数据驱动的依存句法分析方法主要有两种主流方法：基于图（ graph-based）的分析方法和基于转移（ transition-based）的分析方法。 2.1 基于图的依存句法分析方法 基于图的方法将依存句法分析问题看成从完全有向图中寻找最大生成树的问题。一棵依存树的分值由构成依存树的几种子树的分值累加得到。 根据依存树分值中包含的子树的复杂度，基于图的依存分析模型可以简单区分为一阶和高阶模型。高阶模型可以使用更加复杂的子树特征，因此分析准确率更高，但是解码算法的效率也会下降。 基于图的方法通常采用基于动态规划的解码算法，也有一些学者采用柱搜索（beam search）来提高效率。学习特征权重时，通常采用在线训练算法，如平均感知器（averaged perceptron）。 2.2 基于转移的依存句法分析方法 基于转移的方法将依存树的构成过程建模为一个动作序列，将依存分析问题转化为寻找最优动作序列的问题。早期，研究者们使用局部分类器（如支持向量机等）决定下一个动作。近年来，研究者们采用全局线性模型来决定下一个动作，一个依存树的分值由其对应的动作序列中每一个动作的分值累加得到。 特征表示方面，基于转移的方法可以充分利用已形成的子树信息，从而形成丰富的特征，以指导模型决策下一个动作。模型通过贪心搜索或者柱搜索等解码算法找到近似最优的依存树。和基于图的方法类似，基于转移的方法通常也采用在线训练算法学习特征权重。 2.3 多模型融合的依存句法分析方法 基于图和基于转移的方法从不同的角度解决问题，各有优势。基于图的模型进行全局搜索但只能利用有限的子树特征，而基于转移的模型搜索空间有限但可以充分利用已构成的子树信息构成丰富的特征。详细比较发现，这两种方法存在不同的错误分布。 因此，研究者们使用不同的方法融合两种模型的优势，常见的方法有：stacked learning；对多个模型的结果加权后重新解码（re-parsing）；从训练语料中多次抽样训练多个模型（bagging）。 短语结构句法分析 分词，词性标注技术一般只需对句子的局部范围进行分析处理，目前已经基本成熟，其标志就是它们已经被成功地用于文本检索、文本分类、信息抽取等应用之中，而句法分析、语义分析技术需要对句子进行全局分析，目前，深层的语言分析技术还没有达到完全实用的程度。 短语结构句法分析的研究基于上下文无关文法（Context Free Grammar，CFG）。上下文无关文法可以定义为四元组，其中 T 表示终结符的集合（即词的集合），N 表示非终结符的集合（即文法标注和词性标记的集合），S 表示充当句法树根节点的特殊非终结符，而 R 表示文法规则的集合，其中每条文法规则可以表示为 Ni®g ，这里的 g 表示由非终结符与终结符组成的一个序列（允许为空）。 根据文法规则的来源不同，句法分析器的构建方法总体来说可以分为两大类： • 人工书写规则 • 从数据中自动学习规则 人工书写规则受限于规则集合的规模：随着书写的规则数量的增多，规则与规则之间的冲突加剧，从而导致继续添加规则变得困难。 与人工书写规模相比，自动学习规则的方法由于开发周期短和系统健壮性强等特点，加上大规模人工标注数据，比如宾州大学的多语种树库的推动作用，已经成为句法分析中的主流方法。 而数据驱动的方法又推动了统计方法在句法分析领域中的大量应用。为了在句法分析中引入统计信息，需要将上下文无关文法扩展成为概率上下文无关文法（Probabilistic Context Free Grammar，PCFG），即为每条文法规则指定概率值。 概率上下文无关文法与非概率化的上下文无关文法相同，仍然表示为四元组，区别在于概率上下文无关文法中的文法规则必须带有概率值。 获得概率上下文无关文法的最简单的方法是直接从树库中读取规则，利用最大似然估计（Maximum Likelihood Estimation，MLE）计算得到每条规则的概率值。使用该方法得到的文法可以称为简单概率上下文无关文法。在解码阶段，CKY 10 等解码算法就可以利用学习得到的概率上下文无关文法搜索最优句法树。 虽然基于简单概率上下文无关文法的句法分析器的实现比较简单，但是这类分析器的性能并不能让人满意。 性能不佳的主要原因在于上下文无关文法采取的独立性假设过强：一条文法规则的选择只与该规则左侧的非终结符有关，而与任何其它上下文信息无关。文法中缺乏其它信息用于规则选择的消歧。因此后继研究工作的出发点大都基于如何弱化上下文无关文法中的隐含独立性假设。 总结 分词，词性标注技术一般只需对句子的局部范围进行分析处理，目前已经基本成熟，其标志就是它们已经被成功地用于文本检索、文本分类、信息抽取等应用之中，而句法分析、语义分析技术需要对句子进行全局分析，目前，深层的语言分析技术还没有达到完全实用的程度。 四. 文本分类 文本分类是文本挖掘的核心任务，一直以来倍受学术界和工业界的关注。文本分类（Text Classification）的任务是根据给定文档的内容或主题，自动分配预先定义的类别标签。 对文档进行分类，一般需要经过两个步骤： • 文本表示 • 学习分类 文本表示是指将无结构化的文本内容转化成结构化的特征向量形式，作为分类模型的输入。在得到文本对应的特征向量后，就可以采用各种分类或聚类模型，根据特征向量训练分类器或进行聚类。因此，文本分类或聚类的主要研究任务和相应关键科学问题如下： 任务 1.1 构建文本特征向量 构建文本特征向量的目的是将计算机无法处理的无结构文本内容转换为计算机能够处理的特征向量形式。文本内容特征向量构建是决定文本分类和聚类性能的重要环节。 为了根据文本内容生成特征向量，需要首先建立特征空间。其中典型代表是文本词袋（Bag of Words）模型，每个文档被表示为一个特征向量，其特征向量每一维代表一个词项。所有词项构成的向量长度一般可以达到几万甚至几百万的量级。 这样高维的特征向量表示如果包含大量冗余噪音，会影响后续分类聚类模型的计算效率和效果。 因此，我们往往需要进行特征选择（Feature Selection）与特征提取（Feature Extraction），选取最具有区分性和表达能力的特征建立特征空间，实现特征空间降维；或者，进行特征转换（Feature Transformation），将高维特征向量映射到低维向量空间。特征选择、提取或转换是构建有效文本特征向量的关键问题。 1.2 建立分类或聚类模型 在得到文本特征向量后，我们需要构建分类或聚类模型，根据文本特征向量进行分类或聚类。 其中，分类模型旨在学习特征向量与分类标签之间的关联关系，获得最佳的分类效果； 而聚类模型旨在根据特征向量计算文本之间语义相似度，将文本集合划分为若干子集。 分类和聚类是机器学习领域的经典研究问题。 我们一般可以直接使用经典的模型或算法解决文本分类或聚类问题。例如，对于文本分类，我们可以选用朴素贝叶斯、决策树、k-NN、逻辑回归（Logistic Regression）、支持向量机（Support Vector Machine, SVM）等分类模型。 对于文本聚类，我们可以选用 k-means、层次聚类或谱聚类（spectral clustering）等聚类算法。 这些模型算法适用于不同类型的数据而不仅限于文本数据。 但是，文本分类或聚类会面临许多独特的问题，例如，如何充分利用大量无标注的文本数据，如何实现面向文本的在线分类或聚类模型，如何应对短文本带来的表示稀疏问题，如何实现大规模带层次分类体系的分类功能，如何充分利用文本的序列信息和句法语义信息，如何充分利用外部语言知识库信息，等等。这些问题都是构建文本分类和聚类模型所面临的关键问题。 模型 2.1 文本分类模型 近年来，文本分类模型研究层出不穷，特别是随着深度学习的发展，深度神经网络模型 也在文本分类任务上取得了巨大进展。我们将文本分类模型划分为以下三类： • 基于规则的分类模型 基于规则的分类模型旨在建立一个规则集合来对数据类别进行判断。这些规则可以从训练样本里自动产生，也可以人工定义。给定一个测试样例，我们可以通过判断它是否满足某 些规则的条件，来决定其是否属于该条规则对应的类别。 典型的基于规则的分类模型包括决策树（Decision Tree）、随机森林（Random Forest）、 RIPPER 算法等。 • 基于机器学习的分类模型 典型的机器学习分类模型包括贝叶斯分类器（Naïve Bayes）、线性分类器（逻辑回归）、 支持向量机（Support Vector Machine, SVM）、最大熵分类器等。 SVM 是这些分类模型中比较有效、使用较为广泛的分类模型。它能够有效克服样本分布不均匀、特征冗余以及过拟合等问题，被广泛应用于不同的分类任务与场景。通过引入核函数，SVM 还能够解决原始特征空间线性不可分的问题。 除了上述单分类模型，以 Boosting 为代表的分类模型组合方法能够有效地综合多个弱分类模型的分类能力。在给定训练数据集合上同时训练这些弱分类模型，然后通过投票等机制综合多个分类器的预测结果，能够为测试样例预测更准确的类别标签。 • 基于神经网络的方法 以人工神经网络为代表的深度学习技术已经在计算机视觉、语音识别等领域取得了巨大成功，在自然语言处理领域，利用神经网络对自然语言文本信息进行特征学习和文本分类，也成为文本分类的前沿技术。 前向神经网络：多层感知机（Multilayer Perceptron, MLP）是一种典型的前向神经网络。它能够自动学习多层神经网络，将输入特征向量映射到对应的类别标签上。 通过引入非线性激活层，该模型能够实现非线性的分类判别式。包括多层感知机在内的文本分类模型均使用了词袋模型假设，忽略了文本中词序和结构化信息。对于多层感知机模型来说，高质量的初始特征表示是实现有效分类模型的必要条件。 为了更加充分地考虑文本词序信息，利用神经网络自动特征学习的特点，研究者后续提出了卷积神经网络（Convolutional Neural Network, CNN）和循环神经网络（Recurrent Neural Network, RNN）进行文本分类。 基于 CNN 和 RNN 的文本分类模型输入均为原始的词序列，输出为该文本在所有类别上的概率分布。这里，词序列中的每个词项均以词向量的形式作为输入。 卷积神经网络（CNN）：卷积神经网络文本分类模型的主要思想是，对词向量形式的文本输入进行卷积操作。CNN 最初被用于处理图像数据。与图像处理中选取二维域进行卷积操作不同，面向文本的卷积操作是针对固定滑动窗口内的词项进行的。 经过卷积层、 池化层和非线性转换层后，CNN 可以得到文本特征向量用于分类学习。CNN 的优势在于在计算文本特征向量过程中有效保留有用的词序信息。 针对 CNN 文本分类模型还有许多改进工作， 如基于字符级 CNN 的文本分类模型、将词位置信息加入到词向量。 循环神经网络（RNN）：循环神经网络将文本作为字符或词语序列{x0 , … , xN}，对于第 t 时刻输入的字符或词语 xt，都会对应产生新的低维特征向量 st。如图 3 所示，st 的取值会受到 xt 和上个时刻特征向量 st-1 的共同影响，st 包含了文本序列从 x0 到 xt 的语义信息。因此，我们可以利用 sN 作为该文本序列的特征向量，进行文本分类学习。 与 CNN 相比，RNN 能够更自然地考虑文本的词序信息，是近年来进行文本表示最流行的方案之一。 为了提升 RNN 对文本序列的语义表示能力，研究者提出很多扩展模型。 例如，长短时记忆网络（LSTM）提出记忆单元结构，能够更好地处理文本序列中的长程依赖，克服循环神经网络梯度消失问题。如图 4 是 LSTM 单元示意图，其中引入了三个门（input gate, output gate, forget gate）来控制是否输入输出以及记忆单元更新。 提升 RNN 对文本序列的语义表示能力的另外一种重要方案是引入选择注意力机制 (Selective Attention)，可以让模型根据具体任务需求对文本序列中的词语给予不同的关注度。 应用 文本分类技术在智能信息处理服务中有着广泛的应用。例如，大部分在线新闻门户网站（如新浪、搜狐、腾讯等）每天都会产生大量新闻文章，如果对这些新闻进行人工整理非常耗时耗力，而自动对这些新闻进行分类，将为新闻归类以及后续的个性化推荐等都提供巨大帮助。 互联网还有大量网页、论文、专利和电子图书等文本数据，对其中文本内容进行分类，是实现对这些内容快速浏览与检索的重要基础。此外，许多自然语言分析任务如观点挖掘、垃圾邮件检测等，也都可以看作文本分类或聚类技术的具体应用。 对文档进行分类，一般需要经过两个步骤：（1）文本表示，以及（2）学习。文本表示是指将无结构化的文本内容转化成结构化的特征向量形式，作为分类模型的输入。在得到文本对应的特征向量后，就可以采用各种分类或聚类模型，根据特征向量训练分类器 五. 信息检索 信息检索（Information Retrieval, IR）是指将信息按一定的方式加以组织，并通过信息查找满足用户的信息需求的过程和技术。 1951 年，Calvin Mooers 首次提出了“信息检索”的概念，并给出了信息检索的主要任务：协助信息的潜在用户将信息需求转换为一张文献来源列表，而这些文献包含有对其有用的信息。 信息检索学科真正取得长足发展是在计算机诞生并得到广泛应用之后，文献数字化使得信息的大规模共享及保存成为现实，而检索就成为了信息管理与应用中必不可少的环节。 互联网的出现和计算机硬件水平的提高使得人们存储和处理信息的能力得到巨大的提高，从而加速了信息检索研究的进步，并使其研究对象从图书资料和商用数据扩展到人们生活的方方面面。 伴随着互联网及网络信息环境的迅速发展，以网络信息资源为主要组织对象的信息检索系统：搜索引擎应运而生，成为了信息化社会重要的基础设施。 2016 年初，中文搜索引擎用户数达到 5.66 亿人，这充分说明搜索引擎在应用层次取得的巨大成功，也使得信息检索，尤其是网络搜索技术的研究具有了重要的政治、经济和社会价值。 内容结构 检索用户、信息资源和检索系统三个主要环节组成了信息检索应用环境下知识获取与信息传递的完整结构，而当前影响信息获取效率的因素也主要体现在这几个环节，即： • 检索用户的意图表达 • 信息资源（尤其是网络信息资源）的质量度量 • 需求与资源的合理匹配 具体而言，用户有限的认知能力导致其知识结构相对大数据时代的信息环境而言往往存在缺陷，进而影响信息需求的合理组织和清晰表述；数据资源的规模繁杂而缺乏管理，在互联网“注意力经济”盛行的环境下，不可避免地存在欺诈作弊行为，导致检索系统难以准确感知其质量；用户与资源提供者的知识结构与背景不同，对于相同或者相似事物的描述往往存在较大差异，使得检索系统传统的内容匹配技术难以很好应对，无法准确度量资源与需求的匹配程度。 上述技术挑战互相交织，本质上反映了用户个体有限的认知能力与包含近乎无限信息的数据资源空间之间的不匹配问题。 概括地讲，当前信息检索的研究包括如下四个方面的研究内容及相应的关键科学问题： 1.1 信息需求理解 面对复杂的泛在网络空间，用户有可能无法准确表达搜索意图；即使能够准确表达，搜索引擎也可能难以正确理解；即使能够正确理解，也难以与恰当的网络资源进行匹配。这使得信息需求理解成为了影响检索性能提高的制约因素，也构成了检索技术发展面临的第一个关键问题。 1.2 资源质量度量 资源质量管理与度量在传统信息检索研究中并非处于首要的位置，但随着互联网信息资源逐渐成为检索系统的主要查找对象，网络资源特有的缺乏编审过程、内容重复度高、质量参差不齐等问题成为了影响检索质量的重要因素。 目前，搜索引擎仍旧面临着如何进行有效的资源质量度量的挑战，这构成了当前信息检索技术发展面临的第二个关键问题。 1.3 结果匹配排序 近年来，随着网络技术的进步，信息检索系统（尤其是搜索引擎）涉及的数据对象相应 的变得多样化、异质化，这也造成了传统的以文本内容匹配为主要手段的结果排序方法面临着巨大的挑战。 高度动态繁杂的泛在网络内容使得文本相似度计算方法无法适用；整合复杂异构网络资源作为结果使得基于同质性假设构建的用户行为模型难以应对；多模态的交互方式则使得传统的基于单一维度的结果分布规律的用户行为假设大量失效。 因此，在大数据时代信息进一步多样化、异质化的背景下，迫切需要构建适应现代信息资源环境的检索结果匹配排序方法，这是当前信息检索技术发展面临的第三个关键问题。 1.4 信息检索评价 信息检索评价是信息检索和信息获取领域研究的核心问题之一。信息检索和信息获取系统核心的目标是帮助用户获取到满足他们需求的信息，而评价系统的作用是帮助和监督研究开发人员向这一核心目标前进，以逐步开发出更好的系统，进而缩小系统反馈和用户需求之间的差距，提高用户满意度。 因此，如何设计合理的评价框架、评价手段、评价指标，是当前信息检索技术发展面临的第四个关键问题。 个性化搜索 现有的主要个性化搜索算法可分为基于内容分析的算法、基于链接分析的方法和基于协作过滤的算法。 • 基于内容的个性化搜索算法通过比较用户兴趣爱好和结果文档的内容相似性来对文档的用户相关性进行判断进而对搜索结果进行重排。 用户模型一般表述为关键词或主题向量或层次的形式。个性化算法通过比较用户模型和文档的相似性，判断真实的搜索意图，并估计文档对用户需求的匹配程度。 • 基于链接分析的方法主要是利用互联网上网页之间的链接关系，并假设用户点击和访问过的网页为用户感兴趣的网页，通过链接分析算法进行迭代最终计算出用户对每个网页的喜好度。 • 基于协作过滤的个性化搜索算法主要借鉴了基于协作过滤的推荐系统的思想，这种方法考虑到能够收集到的用户的个人信息有限，因此它不仅仅利用用户个人的信息，还利用与用户相似的其它用户或群组的信息，并基于用户群组和相似用户的兴趣偏好来个性化当前用户的搜索结果。用户之间的相似性可以通过用户的兴趣爱好、历史查询、点击过的网页等内容计算得出。 语义搜索技术 随着互联网信息的爆炸式增长，传统的以关键字匹配为基础的搜索引擎，已越来越难以满足用户快速查找信息的需求。同时由于没有知识引导及对网页内容的深入整理，传统网页搜索返回的网页结果也不能精准给出所需信息。 针对这些问题，以知识图谱为代表的语义搜索（Semantic Search）将语义 Web 技术和传统的搜索引擎技术结合，是一个很有研究价值 但还处于初期阶段的课题。 在未来的一段时间，结合互联网应用需求的实际和技术、产品运营能力的实际发展水平，语义搜索技术的发展重点将有可能集中在以各种情境的垂直搜索资源为基础，知识化推理为检索运行方式，自然语言多媒体交互为手段的智能化搜索与推荐技术。 首先将包括各类垂直搜索资源在内的深度万维网数据源整合成为提供搜索服务的资源池；随后利用广泛分布在公众终端计算设备上的浏览器作为客户端载体，通过构建的复杂情境知识库来开发多层次查询技术，并以此管理、调度、整合搜索云端的搜索服务资源，满足用户的多样化、多模态查询需求；最后基于面向情境体验的用户行为模型构建，以多模态信息推荐的形式实现对用户信息需求的主动满足。 六. 信息抽取 信息抽取（Information Extraction）是指从非结构化/半结构化文本（如网页、新闻、 论文文献、微博等）中提取指定类型的信息（如实体、属性、关系、事件、商品记录等）， 并通过信息归并、冗余消除和冲突消解等手段将非结构化文本转换为结构化信息的一项综合技术。例如: • 从相关新闻报道中抽取出恐怖事件信息：时间、地点、袭击者、受害人、袭击 目标、后果等； • 从体育新闻中抽取体育赛事信息：主队、客队、赛场、比分等； • 从论文和医疗文献中抽取疾病信息：病因、病原、症状、药物等 被抽取出来的信息通常以结构化的形式描述，可以为计算机直接处理，从而实现对海量非结构化数据的分析、组织、管理、计算、 查询和推理，并进一步为更高层面的应用和任务（如自然语言理解、知识库构建、智能问答系统、舆情分析系统）提供支撑。 目前信息抽取已被广泛应用于舆情监控、网络搜索、智能问答等多个重要领域。与此同时，信息抽取技术是中文信息处理和人工智能的核心技术，具有重要的科学意义。 一直以来，人工智能的关键核心部件之一是构建可支撑类人推理和自然语言理解的大规模常识知识库。然而，由于人类知识的复杂性、开放性、多样性和巨大的规模，目前仍然无法构建满足上述需求的大规模知识库。 信息抽取技术通过结构化自然语言表述的语义知识，并整合来自海量文本中的不同语义知识，是构建大规模知识库最有效的技术之一。 每一段文本内所包含的寓意可以描述为其中的一组实体以及这些实体相互之间的关联和交互，因此抽取文本中的实体和它们之间的语义关系也就成为了理解文本意义的基础。 信息抽取可以通过抽取实体和实体之间的语义关系，表示这些语义关系承载的信息，并基于这些信息进行计算和推理来有效的理解一段文本所承载的语义。 命名实体识别 命名实体识别的目的是识别文本中指定类别的实体，主要包括人名、地名、机构名、专有名词等的任务。 命名实体识别系统通常包含两个部分：实体边界识别和实体分类。 其中实体边界识别判断一个字符串是否是一个实体，而实体分类将识别出的实体划分到预先给定的不同类别中去。 命名实体识别是一项极具实用价值的技术，目前中英文上通用命名实体识别（人名、地名、机构名）的 F1 值都能达到 90% 以上。命名实体识别的主要难点在于表达不规律、且缺乏训练语料的开放域命名实体类别（如电影、歌曲名）等。 关系抽取 关系抽取指的是检测和识别文本中实体之间的语义关系，并将表示同一语义关系的提及（mention）链接起来的任务。关系抽取的输出通常是一个三元组（实体 1，关系类别，实体 2），表示实体 1 和实体 2 之间存在特定类别的语义关系。 例如，句子“北京是中国的首都、政治中心和文化中心”中表述的关系可以表示为（中国，首都，北京），（中国，政治中心，北京）和（中国，文化中心，北京）。语义关系类别可以预先给定（如 ACE 评测中的七大类关系），也可以按需自动发现（开放域信息抽取）。 关系抽取通常包含两个核心模块：关系检测和关系分类。 其中关系检测判断两个实体之间是否存在语义关系，而关系分类将存在语义关系的实体对划分到预先指定的类别中。 在某些场景和任务下，关系抽取系统也可能包含关系发现模块，其主要目的是发现实体和实体之间存在的语义关系类别。例如，发现人物和公司之间存在雇员、CEO、CTO、创始人、董事长等关系类别。 事件抽取 事件抽取指的是从非结构化文本中抽取事件信息，并将其以结构化形式呈现出来的任务。 例如，从“毛泽东 1893 年出生于湖南湘潭”这句话中抽取事件{类型：出生， 人物：毛泽东，时间：1893 年，出生地：湖南湘潭}。 事件抽取任务通常包含事件类型识别和事件元素填充两个子任务。 事件类型识别判断一句话是否表达了特定类型的事件。事件类型决定了事件表示的模板，不同类型的事件具有不同的模板。 例如出生事件的模板是{人物， 时间，出生地}，而恐怖袭击事件的模板是{地点，时间，袭击者，受害者，受伤人数,…}。 事件元素指组成事件的关键元素，事件元素识别指的是根据所属的事件模板，抽取相应的元素，并为其标上正确元素标签的任务。 信息集成 实体、关系和事件分别表示了单篇文本中不同粒度的信息。在很多应用中，需要将来自不同数据源、不同文本的信息综合起来进行决策，这就需要研究信息集成技术。 目前，信息抽取研究中的信息集成技术主要包括共指消解技术和实体链接技术。 共指消解指的是检测同一实体/关系/事件的不同提及，并将其链接在一起的任务，例如，识别“乔布斯是苹果的创始人之一，他经历了苹果公司几十年的起落与兴衰”这句话中的“乔布斯”和“他”指的是同一实体。 实体链接的目的是确定实体名所指向的真实世界实体。例如识别上一句话中的“苹果”和“乔布斯”分别指向真实世界中的苹果公司和其 CEO 史蒂夫·乔布斯。 七. 问答系统 自动问答（Question Answering, QA）是指利用计算机自动回答用户所提出的问题以满足用户知识需求的任务。不同于现有搜索引擎，问答系统是信息服务的一种高级形式，系统返回用户的不再是基于关键词匹配排序的文档列表，而是精准的自然语言答案。 近年来，随着人工智能的飞速发展，自动问答已经成为倍受关注且发展前景广泛的研究方向。自动问答的研究历史可以溯源到人工智能的原点。 1950 年，人工智能之父阿兰图灵（Alan M. Turing）在《Mind》上发表文章《Computing Machinery and Intelligence》，文章开篇提出通过让机器参与一个模仿游戏（Imitation Game）来验证“机器”能否“思考”，进而提出了经典的图灵测试（Turing Test），用以检验机器是否具备智能。 同样，在自然语言处理研究领域，问答系统被认为是验证机器是否具备自然语言理解能力的四个任务之一（其它三个是机器翻译、复述和文本摘要）。 自动问答研究既有利于推动人工智能相关学科的发展，也具有非常重要的学术意义。从应用上讲，现有基于关键词匹配和浅层语义分析的信息服务技术已经难以满足用户日益增长的精准化和智能化信息需求，已有的信息服务范式急需一场变革。 2011 年，华盛顿大学图灵中心主任 Etzioni 在 Nature 上发表的《Search Needs a Shake-Up》中明确指出：在万维网诞生 20 周年之际，互联网搜索正处于从简单关键词搜索走向深度问答的深刻变革的风口浪尖上。以直接而准确的方式回答用户自然语言提问的自动问答系统将构成下一代搜索引擎的基本形态。 同一年，以深度问答技术为核心的 IBM Watson 自动问答机器人在美国智力竞赛节目 Jeopardy 中战胜人类选手，引起了业内的巨大轰动。Watson 自动问答系统让人们看到已有信息服务模式被颠覆的可能性，成为了问答系统发展的一个里程碑。 此外，随着移动互联网崛起与发展，以苹果公司 Siri、Google Now、微软 Cortana 等为代表的移动生活助手爆发式涌现，上述系统都把以自然语言为基本输入方式的问答系统看作是下一代信息服务的新形态和突破口，并均加大人员、资金的投入，试图在这一次人工智能浪潮中取得领先。 关键问题 自动问答系统在回答用户问题时，需要正确理解用户所提的自然语言问题，抽取其中的关键语义信息，然后在已有语料库、知识库或问答库中通过检索、匹配、推理的手段获取答案并返回给用户。 上述过程涉及词法分析、句法分析、语义分析、信息检索、逻辑推理、知识工程、语言生成等多项关键技术。传统自动问答多集中在限定领域，针对限定类型的问题进行回答。伴随着互联网和大数据的飞速发展，现有研究趋向于开放域、面向开放类型问题的自动问答。概括地讲，自动问答的主要研究任务和相应关键科学问题如下。 1.1 问句理解 给定用户问题，自动问答首先需要理解用户所提问题。用户问句的语义理解包含词法分析、句法分析、语义分析等多项关键技术，需要从文本的多个维度理解其中包含的语义内容。 在词语层面，需要在开放域环境下，研究命名实体识别（Named Entity Recognition）、术语识别（Term Extraction）、词汇化答案类型词识别（Lexical Answer Type Recognition）、 实体消歧（Entity Disambiguation）、关键词权重计算（Keyword Weight Estimation）、答案集中词识别（Focused Word Detection）等关键问题。 在句法层面，需要解析句子中词与词之间、短语与短语之间的句法关系，分析句子句法结构。在语义层面，需要根据词语层面、句法层面的分析结果，将自然语言问句解析成可计算、结构化的逻辑表达形式（如一阶谓词逻辑表达式）。 1.2 文本信息抽取 给定问句语义分析结果，自动问答系统需要在已有语料库、知识库或问答库中匹配相关的信息，并抽取出相应的答案。 传统答案抽取构建在浅层语义分析基础之上，采用关键词匹配策略，往往只能处理限定类型的答案，系统的准确率和效率都难以满足实际应用需求。为保证信息匹配以及答案抽取的准确度，需要分析语义单元之间的语义关系，抽取文本中的结构化知识。 早期基于规则模板的知识抽取方法难以突破领域和问题类型的限制，远远不能满足开放领域自动问答的知识需求。为了适应互联网实际应用的需求，越来越多的研究者和开发者开始关注开放域知识抽取技术，其特点在于： • 文本领域开放：处理的文本是不限定领域的网络文本 • 内容单元类型开放：不限定所抽取的内容单元类型，而是自动地从网络中挖掘内容单元的类型，例如实体类型、事件类型和关系类型等。 1.3 知识推理 自动问答中，由于语料库、知识库和问答库本身的覆盖度有限，并不是所有问题都能直 接找到答案。这就需要在已有的知识体系中，通过知识推理的手段获取这些隐含的答案。 例如，知识库中可能包括了一个人的“出生地”信息，但是没包括这个人的“国籍”信息，因此无法直接回答诸如“某某人是哪国人?”这样的问题。但是一般情况下，一个人的“出生地”所属的国家就是他（她）的“国籍”。 在自动问答中，就需要通过推理的方式学习到这样的模式。传统推理方法采用基于符号的知识表示形式，通过人工构建的推理规则得到答案。 但是面对大规模、开放域的问答场景，如何自动进行规则学习，如何解决规则冲突仍然是亟待解决的难点问题。目前，基于分布式表示的知识表示学习方法能够将实体、概念以及它们之间的语义关系表示为低维空间中的对象（向量、矩阵等），并通过低维空间中的数值计算完成知识推理任务。 虽然这类推理的效果离实用还有距离，但是我们认为这是值得探寻的方法，特别是如何将已有的基于符号表示的逻辑推理与基于分布式表示的数值推理相结合，研究融合符号逻辑和表示学习的知识推理技术，是知识推理任务中的关键科学问题。 技术方法 根据目标数据源的不同，已有自动问答技术大致可以分为三类： • 检索式问答；• 社区问答;• 知识库问答。 以下分别就这几个方面对研究现状进行简要阐述。 2.1 检索式问答 检索式问答研究伴随搜索引擎的发展不断推进。1999 年，随着 TREC QA 任务的发起， 检索式问答系统迎来了真正的研究进展。TREC QA 的任务是给定特定 WEB 数据集，从中找到能够回答问题的答案。这类方法是以检索和答案抽取为基本过程的问答系统，具体过程包括问题分析、篇章检索和答案抽取。 根据抽取方法的不同，已有检索式问答可以分为基于模式匹配的问答方法和基于统计文本信息抽取的问答方法。 • 基于模式匹配的方法往往先离线地获得各类提问答案的模式。在运行阶段，系统首先判断当前提问属于哪一类，然后使用这类提问的模式来对抽取的候选答案进行验证。同时为了提高问答系统的性能，人们也引入自然语言处理技术。由于自然语言处理的技术还未成熟，现有大多数系统都基于浅层句子分析。 • 基于统计文本信息抽取的问答系统的典型代表是美国 Language Computer Corporation 公司的 LCC 系统。该系统使用词汇链和逻辑形式转换技术，把提问句和答案句转化成统一的逻辑形式（Logic Form），通过词汇链，实现答案的推理验证。 LCC 系统在 TREC QA Track 2001 ~ 2004 连续三年的评测中以较大领先优势获得第一名的成绩。 2011 年，IBM 研发的问答机器人 Watson 在美国智力竞赛节目《危险边缘 Jeopardy!》中战胜人类选手，成为问答系统发展的一个里程碑。 Watson 的技术优势大致可以分为以下三个方面： • 强大的硬件平台：包括 90 台 IBM 服务器，分布式计算环境； • 强大的知识资源：存储了大约 2 亿页的图书、新闻、电影剧本、辞海、文选和《世界图书百科全书》等资料； • 深层问答技术（DeepQA）：涉及统计机器学习、句法分析、主题分析、信息抽取、 知识库集成和知识推理等深层技术。 然而，Watson 并没有突破传统问答式检索系统的局限性，使用的技术主要还是检索和匹配，回答的问题类型大多是简单的实体或词语类问题，而推理能力不强。 2.2 社区问答 随着 Web2.0 的兴起，基于用户生成内容（User-Generated Content, UGC）的互联网服务越来越流行，社区问答系统应运而生，例如 Yahoo! Answers、百度知道等。 问答社区的出现为问答技术的发展带来了新的机遇。据统计 2010 年 Yahoo! Answers 上已解决的问题量达到 10 亿，2011 年“百度知道”已解决的问题量达到 3 亿，这些社区问答数据覆盖了方方面面的用户知识和信息需求。 此外，社区问答与传统自动问答的另一个显著区别是：社区问答系统有大量的用户参与，存在丰富的用户行为信息，例如用户投票信息、用户评价信息、回答者的问题采纳率、用户推荐次数、页面点击次数以及用户、问题、答案之间的相互关联信息等等，这些用户行为信息对于社区中问题和答案的文本内容分析具有重要的价值。 一般来讲，社区问答的核心问题是从大规模历史问答对数据中找出与用户提问问题语义相似的历史问题并将其答案返回提问用户。 假设用户查询问题为 q0,用于检索的问答对数据为 SQ,A = {(q1 , a1 ), (q2 , a2 )}, … , (qn, an)}}，相似问答对检索的目标是从 SQ,A 中检索出能够解答问题 q0 的问答对 (qi , ai)。 针对这一问题，传统的信息检索模型，如向量空间模型、语言模型等，都可以得到应用。 但是，相对于传统的文档检索，社区问答的特点在于：用户问题和已有问句相对来说都非常短，用户问题和已有问句之间存在“词汇鸿沟”问题，基于关键词匹配的检索模型很难达到较好的问答准确度。 目前，很多研究工作在已有检索框架中针对这一问题引入单语言翻译概率模型，通过 IBM 翻译模型，从海量单语问答语料中获得同种语言中两个不同词语之间的语义转换概率，从而在一定程度上解决词汇语义鸿沟问题。 例如和“减肥”对应的概率高的相关词有“瘦身”、“跑步”、“饮食”、“健康”、“远动”等等。 除此之外，也有许多关于问句检索中词重要性的研究和基于句法结构的问题匹配研究。 2.3 知识库问答 检索式问答和社区问答尽管在某些特定领域或者商业领域有所应用，但是其核心还是关键词匹配和浅层语义分析技术，难以实现知识的深层逻辑推理，无法达到人工智能的高级目标。 因此，近些年来，无论是学术界或工业界，研究者们逐步把注意力投向知识图谱或知识库（Knowledge Graph）。其目标是把互联网文本内容组织成为以实体为基本语义单元（节点）的图结构，其中图上的边表示实体之间语义关系。 目前互联网中已有的大规模知识库包括 DBpedia、Freebase、YAGO 等。这些知识库多是以“实体-关系-实体”三元组为基本单元所组成的图结构。 基于这样的结构化知识，问答系统的任务就是要根据用户问题的语义直接在知识库上查找、推理出相匹配的答案，这一任务称为面向知识库的问答系统或知识库问答。要完成在结构化数据上的查询、匹配、推理等操作，最有效的方式是利用结构化的查询语句，例如：SQL、SPARQL 等。 然而，这些语句通常是由专家编写，普通用户很难掌握并正确运用。对普通用户来说，自然语言仍然是最自然的交互方式。因此，如何把用户的自然语言问句转化为结构化的查询语句是知识库问答的核心所在，其关键是对于自然语言问句进行语义理解。 目前，主流方法是通过语义分析，将用户的自然语言问句转化成结构化的语义表示，如范式和 DCS-Tree。相对应的语义解析语法或方法包括组合范畴语法（ Category Compositional Grammar, CCG ）以 及 依 存 组 合 语 法（ Dependency-based Compositional Semantics, DCS）等。 八. 机器翻译 理论应用 机器翻译（machine translation，MT）是指利用计算机实现从一种自然语言到另外一种自然语言的自动翻译。被翻译的语言称为源语言（source language），翻译到的语言称作目标语言（target language）。 简单地讲，机器翻译研究的目标就是建立有效的自动翻译方法、模型和系统，打破语言壁垒，最终实现任意时间、任意地点和任意语言的自动翻译，完成人们无障碍自由交流的梦想。 人们通常习惯于感知（听、看和读）自己母语的声音和文字，很多人甚至只能感知自己的母语，因此，机器翻译在现实生活和工作中具有重要的社会需求。 从理论上讲，机器翻译涉及语言学、计算语言学、人工智能、机器学习，甚至认知语言学等多个学科，是一个典型的多学科交叉研究课题，因此开展这项研究具有非常重要的理论意义，既有利于推动相关学科的发展，揭示人脑实现跨语言理解的奥秘，又有助于促进其他自然语言处理任务，包括中文信息处理技术的快速发展。 从应用上讲，无论是社会大众、政府企业还是国家机构，都迫切需要机器翻译技术。特别是在“互联网+”时代，以多语言多领域呈现的大数据已成为我们面临的常态问题，机器翻译成为众多应用领域革新的关键技术之一。 例如，在商贸、体育、文化、旅游和教育等各个领域，人们接触到越来越多的外文资料，越来越频繁地与持各种语言的人通信和交流，从而对机器翻译的需求越来越强烈；在国家信息安全和军事情报领域，机器翻译技术也扮演着非常重要的角色。 可以说离开机器翻译，基于大数据的多语言信息获取、挖掘、分析和决策等其他应用都将成为空中楼阁。 尤其值得提出的是，在未来很长一段时间里，建立于丝绸之路这一历史资源之上的“一带一路”将是我国与周边国家发展政治、经济，进行文化交流的主要战略。据统计，“一带一路”涉及 60 多个国家、44 亿人口、53 种语言，可见机器翻译是“一带一路”战略实施中不可或缺的重要技术。 技术现状 基于规则的机器翻译方法需要人工设计和编纂翻译规则，统计机器翻译方法能够自动获取翻译规则，但需要人工定义规则的形式，而端到端的神经网络机器翻译方法可以直接通过编码网络和解码网络自动学习语言之间的转换算法。 从某种角度讲，其自动化程度和智能化程度在不断提升，机器翻译质量也得到了显著改善。机器翻译技术的研究现状可从欧盟组织的国际机器翻译评测（WMT）的结果中窥得一斑。 该评测主要针对欧洲语言之间的互译，2006 年至 2016 年每年举办一次。对比法语到英语历年的机器翻译评测结果可以发现，译文质量已经在自动评价指标 BLEU 值上从最初小于 0.3 到目前接近 0.4（大量的人工评测对比说明，BLEU 值接近 0.4 的译文能够达到人类基本可以理解的程度）。 另外，中国中文信息学会组织的全国机器翻译评测（CWMT）每两年组织一次， 除了英汉、日汉翻译评测以外，CWMT 还关注我国少数民族语言（藏、蒙、维）和汉语之间的翻译。 相对而言，由于数据规模和语言复杂性的问题，少数民族与汉语之间的翻译性能要低于汉英、汉日之间的翻译性能。虽然机器翻译系统评测的分值呈逐年增长的趋势，译文质量越来越好，但与专业译员的翻译结果相比，机器翻译还有很长的路要走，可以说，在奔向“信、达、雅”翻译目标的征程上，目前的机器翻译基本挣扎在“信”的阶段，很多理论和技术问题仍有待于更深入的研究和探索。 九. 自动摘要 自动文摘（又称自动文档摘要）是指通过自动分析给定的一篇文档或多篇文档，提炼、总结其中的要点信息，最终输出一篇长度较短、可读性良好的摘要（通常包含几句话或数百字），该摘要中的句子可直接出自原文，也可重新撰写所得。 简言之，文摘的目的是通过对原文本进行压缩、提炼，为用户提供简明扼要的文字描述。用户可以通过阅读简短的摘要而知晓原文中所表达的主要内容，从而大幅节省阅读时间。 自动文摘研究的目标是建立有效的自动文摘方法与模型，实现高性能的自动文摘系统。近二十年来，业界提出了各类自动文摘方法与模型，用于解决各类自动摘要问题，在部分自动摘要问题的研究上取得了明显的进展，并成功将自动文摘技术应用于搜索引擎、新闻阅读 等产品与服务中。 例如谷歌、百度等搜索引擎均会为每项检索结果提供一个短摘要，方便用 户判断检索结果相关性。在新闻阅读软件中，为新闻事件提供摘要也能够方便用户快速了解 该事件。2013 年雅虎耗资 3000 万美元收购了一项自动新闻摘要应用 Summly，则标志着自动文摘技术的应用走向成熟。 自动文摘的研究在图书馆领域和自然语言处理领域一直都很活跃，最早的应用需求来自于图书馆。图书馆需要为大量文献书籍生成摘要，而人工摘要的效率很低，因此亟需自动摘要方法取代人工高效地完成文献摘要任务。 随着信息检索技术的发展，自动文摘在信息检索系统中的重要性越来越大，逐渐成为研究热点之一。经过数十年的发展，同时在 DUC 与 TAC 等自动文摘国际评测的推动下，文本摘要技术已经取得长足的进步。国际上自动文摘方面比较著名的几个系统包括 ISI 的 NeATS 系统，哥伦比亚大学的 NewsBlaster 系统，密歇根大学的 NewsInEssence 系统等。 方法 自动文摘所采用的方法从实现上考虑可以分为抽取式摘要（extractive summarization） 和生成式摘要（abstractive summarization）。 抽取式方法相对比较简单，通常利用不同方法对文档结构单元（句子、段落等）进行评价，对每个结构单元赋予一定权重，然后选择最重要的结构单元组成摘要。而生成式方法通常需要利用自然语言理解技术对文本进行语法、 语义分析，对信息进行融合，利用自然语言生成技术生成新的摘要句子。 目前的自动文摘方法主要基于句子抽取，也就是以原文中的句子作为单位进行评估与选取。抽取式方法的好处是易于实现，能保证摘要中的每个句子具有良好的可读性。 为解决如前所述的要点筛选和文摘合成这两个关键科学问题，目前主流自动文摘研究工作大致遵循如下技术框架： 内容表示 → 权重计算 → 内容选择 → 内容组织。 首先将原始文本表示为便于后续处理的表达方式，然后由模型对不同的句法或语义单元 进行重要性计算，再根据重要性权重选取一部分单元，经过内容上的组织形成最后的摘要。 1.1 内容表示与权重计算 原文档中的每个句子由多个词汇或单元构成，后续处理过程中也以词汇等元素为基本单位，对所在句子给出综合评价分数。 以基于句子选取的抽取式方法为例，句子的重要性得分由其组成部分的重要性衡量。由于词汇在文档中的出现频次可以在一定程度上反映其重要性， 我们可以使用每个句子中出现某词的概率作为该词的得分，通过将所有包含词的概率求和得到句子得分。 也有一些工作考虑更多细节，利用扩展性较强的贝叶斯话题模型，对词汇本身的话题相关性概率进行建模。一些方法将每个句子表示为向量，维数为总词表大小。通常使用加权频数作为句子向量相应维上的取值。加权频数的定义可以有多种，如信息检索中常用的词频-逆文档频率 （TF-IDF）权重。 也有研究工作考虑利用隐语义分析或其他矩阵分解技术，得到低维隐含语义表示并加以利用。得到向量表示后计算两两之间的某种相似度（例如余弦相似度）。随后根据计算出的相似度构建带权图，图中每个节点对应每个句子。 在多文档摘要任务中，重要的句子可能和更多其他句子较为相似，所以可以用相似度作为节点之间的边权，通过迭代求解基于图的排序算法来得到句子的重要性得分。 也有很多工作尝试捕捉每个句子中所描述的概念，例如句子中所包含的命名实体或动词。 出于简化考虑，现有工作中更多将二元词（bigram）作为概念。近期则有工作提出利用频繁图挖掘算法从文档集中挖掘得到深层依存子结构作为语义表示单元。 另一方面，很多摘要任务已经具备一定数量的公开数据集，可用于训练有监督打分模型。 例如对于抽取式摘要，我们可以将人工撰写的摘要贪心匹配原文档中的句子或概念，从而得到不同单元是否应当被选作摘要句的数据。然后对各单元人工抽取若干特征，利用回归模型或排序学习模型进行有监督学习，得到句子或概念对应的得分。 文档内容描述具有结构性，因此也有利用隐马尔科夫模型（HMM）、条件随机场（CRF）、结构化支持向量机（Structural SVM）等常见序列标注或一般结构预测模型进行抽取式摘要有监督训练的工作。 所提取的特征包括所在位置、包含词汇、与邻句的相似度等等。对特定摘要任务一般也会引入与具体设定相关的特征，例如查询相关摘要任务中需要考虑与查询的匹配或相似程度。 1.2 内容选择 无论从效果评价还是从实用性的角度考虑，最终生成的摘要一般在长度上会有限制。在获取到句子或其他单元的重要性得分以后，需要考虑如何在尽可能短的长度里容纳尽可能多的重要信息，在此基础上对原文内容进行选取。内容选择方法包括贪心选择和全局优化。 技术现状 相比机器翻译、自动问答、知识图谱、情感分析等热门领域，自动文摘在国内并没有受到足够的重视。 国内早期的基础资源与评测举办过中文单文档摘要的评测任务，但测试集规模比较小，而且没有提供自动化评价工具。2015 年 CCF 中文信息技术专委会组织了 NLPCC 评测，其中包括了面向中文微博的新闻摘要任务，提供了规模相对较大的样例数据和测试数据，并采用自动评价方法，吸引了多支队伍参加评测，目前这些数据可以公开获得。 但上述中文摘要评测任务均针对单文档摘要任务，目前还没有业界认可的中文多文档摘要数据，这在事实上阻碍了中文自动摘要技术的发展。 近些年，市面上出现了一些文本挖掘产品，能够提供中文文档摘要功能（尤其是单文档 摘要），例如方正智思、拓尔思（TRS），海量科技等公司的产品。百度等搜索引擎也能为检索到的文档提供简单的单文档摘要。这些文档摘要功能均被看作是系统的附属功能，其实现方法均比较简单。 十. 学习资料 书籍 1.1 李航《统计学习方法》这本经典书值得反复读，从公式推导到定理证明逻辑严谨，通俗易懂。推荐指数：★★★★★ 1.1 宗成庆《统计自然语言处理》推荐指数：★★★★☆ 博客 斯坦福 cs224d：http://cs224d.stanford.edu/syllabus.html 会议 ACL 2015:http://acl2015.org/accepted_papers.htmlACL 2016:http://acl2016.org/index.php?article_id=13#long_papersEMNLP 2015:http://www.emnlp2015.org/accepted-papers.html 实践案例 https://github.com/carpedm20/lstm-char-cnn-tensorflowhttps://github.com/zoneplus/DL4NLPhttps://github.com/HIT-SCIR/scir-training-day 十一. 进一步学习 论文下载地址： http://ccl.pku.edu.cn/alcourse/nlp/LectureNotes/An%20Overview%20on%20Chinese%20Word%20Segmentation%20(Sun%20Maosong).pdfhttps://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/cl-05.gao_.pdfhttp://www.voidcn.com/blog/forever1dreamsxx/article/p-1295137.htmlhttp://cleanbugs.com/item/the-syntactic-structure-of-nlp-three-chinese-syntactic-structure-cips2016-413620.htmlhttp://cips-upload.bj.bcebos.com/cips2016.pdf 本文经授权转载自公众号「数据派THU」]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faker采访的时候说]]></title>
    <url>%2F2017%2F10%2F30%2FFaker%E9%87%87%E8%AE%BF%E7%9A%84%E6%97%B6%E5%80%99%E8%AF%B4%2F</url>
    <content type="text"><![CDATA[10月28日，半决SKT VS RNG赛后Faker接受采访说：我们的目标一直都是冠军。如果在总决赛上我们输给了对面，那亚军对我来说毫无意义。]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>LOL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Learning PyTorch With Examples]]></title>
    <url>%2F2017%2F10%2F11%2FLearning-PyTorch-With-Examples%2F</url>
    <content type="text"><![CDATA[TensorWarm-up:numpy12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-import numpy as np# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random input and output datax = np.random.randn(N, D_in)y = np.random.randn(N, D_out)# Randomly initialize weightsw1 = np.random.randn(D_in, H)w2 = np.random.randn(H, D_out)learning_rate = 1e-6for t in range(500): # Forward pass: compute predicted y h = x.dot(w1) h_relu = np.maximum(h, 0) y_pred = h_relu.dot(w2) # Compute and print loss loss = np.square(y_pred - y).sum() print(t, loss) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.T.dot(grad_y_pred) grad_h_relu = grad_y_pred.dot(w2.T) grad_h = grad_h_relu.copy() grad_h[h &lt; 0] = 0 grad_w1 = x.T.dot(grad_h) # Update weights w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 PyTorch:Tensor123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding: utf-8 -*-import torchdtype = torch.FloatTensor# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random input and output datax = torch.randn(N, D_in).type(dtype)y = torch.randn(N, D_out).type(dtype)# Randomly initialize weightsw1 = torch.randn(D_in, H).type(dtype)w2 = torch.randn(H, D_out).type(dtype)learning_rate = 1e-6for t in range(500): # Forward pass: compute predicted y h = x.mm(w1) h_relu = h.clamp(min=0) y_pred = h_relu.mm(w2) # Compute and print loss loss = (y_pred - y).pow(2).sum() print(t, loss) # Backprop to compute gradients of w1 and w2 with respect to loss grad_y_pred = 2.0 * (y_pred - y) grad_w2 = h_relu.t().mm(grad_y_pred) grad_h_relu = grad_y_pred.mm(w2.t()) grad_h = grad_h_relu.clone() grad_h[h &lt; 0] = 0 grad_w1 = x.t().mm(grad_h) # Update weights using gradient descent w1 -= learning_rate * grad_w1 w2 -= learning_rate * grad_w2 AutogradPyTorch:Variables and autogradPyTorch中所有的神经网络都来自于autograd包在上面的例子中，我们必须手动实现神经网络的向前和向后遍。手动实施后向传递对于小型双层网络来说不是一件大事，但是对于大型复杂网络来说，可以很快地得到很多毛病。 幸运的是，我们可以使用自动区分 来自动计算神经网络中的向后遍。PyTorch中的 autograd包提供了这个功能。使用自动格式时，网络的正向传递将定义一个 计算图 ; 图中的节点将是Tensors，边缘将是从输入Tensors生成输出Tensors的函数。通过此图反向传播，您可以轻松地计算渐变。 这听起来很复杂，在实践中使用起来很简单。我们将PyTorch Tensors包装在可变对象中; 变量表示计算图中的节点。如果x是变量，则x.data是Tensor，并且x.grad是另一个变量，x其相对于某个标量值保存渐变 。 PyTorch变量与PyTorch Tensors具有相同的API（几乎）您可以在Tensor上执行的任何操作也适用于变量; 区别在于使用变量定义计算图，允许您自动计算渐变。 这里我们使用PyTorch变量和自动调整来实现我们的两层网络; 现在我们不再需要手动实现向后通过网络：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# -*- coding: utf-8 -*-import torchfrom torch.autograd import Variabledtype = torch.FloatTensor# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random Tensors to hold input and outputs, and wrap them in Variables.# Setting requires_grad=False indicates that we do not need to compute gradients# with respect to these Variables during the backward pass.x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)# Create random Tensors for weights, and wrap them in Variables.# Setting requires_grad=True indicates that we want to compute gradients with# respect to these Variables during the backward pass.w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)learning_rate = 1e-6for t in range(500): # Forward pass: compute predicted y using operations on Variables; these # are exactly the same operations we used to compute the forward pass using # Tensors, but we do not need to keep references to intermediate values since # we are not implementing the backward pass by hand. y_pred = x.mm(w1).clamp(min=0).mm(w2) # Compute and print loss using operations on Variables. # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape # (1,); loss.data[0] is a scalar value holding the loss. loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # Use autograd to compute the backward pass. This call will compute the # gradient of loss with respect to all Variables with requires_grad=True. # After this call w1.grad and w2.grad will be Variables holding the gradient # of the loss with respect to w1 and w2 respectively. loss.backward() # Update weights using gradient descent; w1.data and w2.data are Tensors, # w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are # Tensors. w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # Manually zero the gradients after updating weights w1.grad.data.zero_() w2.grad.data.zero_() PyTorch:Defining new autograd functions在PyTorch中，我们可以通过定义一个子类torch.autograd.Function并实现forward 和backward函数来轻松地定义自己的autograd运算符。然后，我们可以通过构造一个实例并将其称为函数，传递包含输入数据的变量来使用我们的新的自动格式运算符。 在这个例子中，我们定义了我们自己的自定义自整定函数来执行ReLU非线性，并用它来实现我们的两层网络：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# -*- coding: utf-8 -*-import torchfrom torch.autograd import Variableclass MyReLU(torch.autograd.Function): """ We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. """ def forward(self, input): """ In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. You can cache arbitrary Tensors for use in the backward pass using the save_for_backward method. """ self.save_for_backward(input) return input.clamp(min=0) def backward(self, grad_output): """ In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. """ input, = self.saved_tensors grad_input = grad_output.clone() grad_input[input &lt; 0] = 0 return grad_inputdtype = torch.FloatTensor# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random Tensors to hold input and outputs, and wrap them in Variables.x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)# Create random Tensors for weights, and wrap them in Variables.w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)learning_rate = 1e-6for t in range(500): # Construct an instance of our MyReLU class to use in our network relu = MyReLU() # Forward pass: compute predicted y using operations on Variables; we compute # ReLU using our custom autograd operation. y_pred = relu(x.mm(w1)).mm(w2) # Compute and print loss loss = (y_pred - y).pow(2).sum() print(t, loss.data[0]) # Use autograd to compute the backward pass. loss.backward() # Update weights using gradient descent w1.data -= learning_rate * w1.grad.data w2.data -= learning_rate * w2.grad.data # Manually zero the gradients after updating weights w1.grad.data.zero_() w2.grad.data.zero_() TensorFlow: Static GraphsPyTorch autograd看起来很像TensorFlow：在两个框架中我们定义一个计算图，并使用自动差分来计算梯度。两者之间的最大区别在于TensorFlow的计算图是静态的，PyTorch使用 动态计算图。 在TensorFlow中，我们定义了一次计算图，然后一遍又一遍地执行相同的图，可能会将不同的输入数据提供给图形。在PyTorch中，每个前进传递定义了一个新的计算图。 静态图是很好的，因为你可以优化前面的图形; 例如，框架可能决定融合一些图形操作以获得效率，或者提出一种将图形分布在多个GPU或许多机器上的策略。如果您一遍又一遍地重复使用相同的图表，那么这个潜在的昂贵的前期优化可以被分摊，因为同一个图表一遍又一遍地重新运行。 静态和动态图不同的一个方面是控制流程。对于某些型号，我们可能希望对每个数据点执行不同的计算; 例如，对于每个数据点，可以展开不同数量的时间步长的循环网络; 这个展开可以被实现为循环。使用静态图形，循环构造需要是图形的一部分; 由于这个原因，TensorFlow提供了诸如tf.scan将循环嵌入图中的操作符。使用动态图表，情况更简单：由于我们为每个示例动态构建图表，因此我们可以使用正常的命令式流程控制来执行每个输入不同的计算。 为了与上面的PyTorch autograd示例进行对比，我们使用TensorFlow来简化两层网络：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# -*- coding: utf-8 -*-import tensorflow as tfimport numpy as np# First we set up the computational graph:# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create placeholders for the input and target data; these will be filled# with real data when we execute the graph.x = tf.placeholder(tf.float32, shape=(None, D_in))y = tf.placeholder(tf.float32, shape=(None, D_out))/# Create Variables for the weights and initialize them with random data.# A TensorFlow Variable persists its value across executions of the graph.w1 = tf.Variable(tf.random_normal((D_in, H)))w2 = tf.Variable(tf.random_normal((H, D_out)))# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.# Note that this code does not actually perform any numeric operations; it# merely sets up the computational graph that we will later execute.h = tf.matmul(x, w1)h_relu = tf.maximum(h, tf.zeros(1))y_pred = tf.matmul(h_relu, w2)# Compute loss using operations on TensorFlow Tensorsloss = tf.reduce_sum((y - y_pred) ** 2.0)# Compute gradient of the loss with respect to w1 and w2.grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])# Update the weights using gradient descent. To actually update the weights# we need to evaluate new_w1 and new_w2 when executing the graph. Note that# in TensorFlow the the act of updating the value of the weights is part of# the computational graph; in PyTorch this happens outside the computational# graph.learning_rate = 1e-6new_w1 = w1.assign(w1 - learning_rate * grad_w1)new_w2 = w2.assign(w2 - learning_rate * grad_w2)# Now we have built our computational graph, so we enter a TensorFlow session to# actually execute the graph.with tf.Session() as sess: # Run the graph once to initialize the Variables w1 and w2. sess.run(tf.global_variables_initializer()) # Create numpy arrays holding the actual data for the inputs x and targets # y x_value = np.random.randn(N, D_in) y_value = np.random.randn(N, D_out) for _ in range(500): # Execute the graph many times. Each time it executes we want to bind # x_value to x and y_value to y, specified with the feed_dict argument. # Each time we execute the graph we want to compute the values for loss, # new_w1, and new_w2; the values of these Tensors are returned as numpy # arrays. loss_value, _, _ = sess.run([loss, new_w1, new_w2], feed_dict=&#123;x: x_value, y: y_value&#125;) print(loss_value) nn modulePyTorch: nn1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding: utf-8 -*-import torchfrom torch.autograd import Variable# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random Tensors to hold inputs and outputs, and wrap them in Variables.x = Variable(torch.randn(N, D_in))y = Variable(torch.randn(N, D_out), requires_grad=False)# Use the nn package to define our model as a sequence of layers. nn.Sequential# is a Module which contains other Modules, and applies them in sequence to# produce its output. Each Linear Module computes output from input using a# linear function, and holds internal Variables for its weight and bias.model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out),)# The nn package also contains definitions of popular loss functions; in this# case we will use Mean Squared Error (MSE) as our loss function.loss_fn = torch.nn.MSELoss(size_average=False)learning_rate = 1e-4for t in range(500): # Forward pass: compute predicted y by passing x to the model. Module objects # override the __call__ operator so you can call them like functions. When # doing so you pass a Variable of input data to the Module and it produces # a Variable of output data. y_pred = model(x) # Compute and print loss. We pass Variables containing the predicted and true # values of y, and the loss function returns a Variable containing the # loss. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # Zero the gradients before running the backward pass. model.zero_grad() # Backward pass: compute gradient of the loss with respect to all the learnable # parameters of the model. Internally, the parameters of each Module are stored # in Variables with requires_grad=True, so this call will compute gradients for # all learnable parameters in the model. loss.backward() # Update the weights using gradient descent. Each parameter is a Variable, so # we can access its data and gradients like we did before. for param in model.parameters(): param.data -= learning_rate * param.grad.data PyTorch: optim12345678910111213141516171819202122232425262728293031323334353637383940414243444546# -*- coding: utf-8 -*-import torchfrom torch.autograd import Variable# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random Tensors to hold inputs and outputs, and wrap them in Variables.x = Variable(torch.randn(N, D_in))y = Variable(torch.randn(N, D_out), requires_grad=False)# Use the nn package to define our model and loss function.model = torch.nn.Sequential( torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out),)loss_fn = torch.nn.MSELoss(size_average=False)# Use the optim package to define an Optimizer that will update the weights of# the model for us. Here we will use Adam; the optim package contains many other# optimization algoriths. The first argument to the Adam constructor tells the# optimizer which Variables it should update.learning_rate = 1e-4optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)for t in range(500): # Forward pass: compute predicted y by passing x to the model. y_pred = model(x) # Compute and print loss. loss = loss_fn(y_pred, y) print(t, loss.data[0]) # Before the backward pass, use the optimizer object to zero all of the # gradients for the variables it will update (which are the learnable weights # of the model) optimizer.zero_grad() # Backward pass: compute gradient of the loss with respect to model # parameters loss.backward() # Calling the step function on an Optimizer makes an update to its # parameters optimizer.step() PyTorch: Custom nn Modules123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# -*- coding: utf-8 -*-import torchfrom torch.autograd import Variableclass TwoLayerNet(torch.nn.Module): def __init__(self, D_in, H, D_out): """ In the constructor we instantiate two nn.Linear modules and assign them as member variables. """ super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(D_in, H) self.linear2 = torch.nn.Linear(H, D_out) def forward(self, x): """ In the forward function we accept a Variable of input data and we must return a Variable of output data. We can use Modules defined in the constructor as well as arbitrary operators on Variables. """ h_relu = self.linear1(x).clamp(min=0) y_pred = self.linear2(h_relu) return y_pred# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random Tensors to hold inputs and outputs, and wrap them in Variablesx = Variable(torch.randn(N, D_in))y = Variable(torch.randn(N, D_out), requires_grad=False)# Construct our model by instantiating the class defined abovemodel = TwoLayerNet(D_in, H, D_out)# Construct our loss function and an Optimizer. The call to model.parameters()# in the SGD constructor will contain the learnable parameters of the two# nn.Linear modules which are members of the model.criterion = torch.nn.MSELoss(size_average=False)optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)for t in range(500): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x) # Compute and print loss loss = criterion(y_pred, y) print(t, loss.data[0]) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step() PyTorch: Control Flow + Weight Sharing1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# -*- coding: utf-8 -*-import randomimport torchfrom torch.autograd import Variableclass DynamicNet(torch.nn.Module): def __init__(self, D_in, H, D_out): """ In the constructor we construct three nn.Linear instances that we will use in the forward pass. """ super(DynamicNet, self).__init__() self.input_linear = torch.nn.Linear(D_in, H) self.middle_linear = torch.nn.Linear(H, H) self.output_linear = torch.nn.Linear(H, D_out) def forward(self, x): """ For the forward pass of the model, we randomly choose either 0, 1, 2, or 3 and reuse the middle_linear Module that many times to compute hidden layer representations. Since each forward pass builds a dynamic computation graph, we can use normal Python control-flow operators like loops or conditional statements when defining the forward pass of the model. Here we also see that it is perfectly safe to reuse the same Module many times when defining a computational graph. This is a big improvement from Lua Torch, where each Module could be used only once. """ h_relu = self.input_linear(x).clamp(min=0) for _ in range(random.randint(0, 3)): h_relu = self.middle_linear(h_relu).clamp(min=0) y_pred = self.output_linear(h_relu) return y_pred# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random Tensors to hold inputs and outputs, and wrap them in Variablesx = Variable(torch.randn(N, D_in))y = Variable(torch.randn(N, D_out), requires_grad=False)# Construct our model by instantiating the class defined abovemodel = DynamicNet(D_in, H, D_out)# Construct our loss function and an Optimizer. Training this strange model with# vanilla stochastic gradient descent is tough, so we use momentumcriterion = torch.nn.MSELoss(size_average=False)optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)for t in range(500): # Forward pass: Compute predicted y by passing x to the model y_pred = model(x) # Compute and print loss loss = criterion(y_pred, y) print(t, loss.data[0]) # Zero gradients, perform a backward pass, and update the weights. optimizer.zero_grad() loss.backward() optimizer.step()]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BAT机器学习面试1000题系列]]></title>
    <url>%2F2017%2F10%2F10%2FBAT%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%951000%E9%A2%98%E7%B3%BB%E5%88%97%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知乎看山杯夺冠记(转)]]></title>
    <url>%2F2017%2F10%2F08%2F%E7%9F%A5%E4%B9%8E%E7%9C%8B%E5%B1%B1%E6%9D%AF%E5%A4%BA%E5%86%A0%E8%AE%B0-%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[https://zhuanlan.zhihu.com/p/28923961]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>文本分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[913周会小结]]></title>
    <url>%2F2017%2F09%2F13%2F913%E5%91%A8%E4%BC%9A%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1,好的人脉和环境真的使人看起来不一样。脚踏实地的钻研的同时也不要忘了交流的重要性。信息的获取不能局限于一个小的范围内。真的，跟一群整天安于现状不求上进的人在一起，自己也会逐渐磨灭前进的意志，在无声无息中。玩游戏,娱乐肯定是必须的，但如果不追求段位的提升，技术的精湛，那多少年过去了，你有何资格说曾经哥也玩过这个游戏。人始终是环境产物，你不明白为何进入传销的人再也走不出来，其实就像此时此刻的你坚信的一些观念，习惯等等其实也就是你所处大环境的东西。要做的不受外界环境和周围人的影响是难的，这大抵就是成大事者必备的不同常人的意志和信念。为什么我很不喜欢云南更多的是因为这里的环境不是我想要的，平台不够好，资源不够多。 ２，]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>开会记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习和神经网络－吴恩达2017]]></title>
    <url>%2F2017%2F09%2F09%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%8D%E5%90%B4%E6%81%A9%E8%BE%BE2017%2F</url>
    <content type="text"><![CDATA[第一周 深度学习概论欢迎来到深度学习工程微专业什么是神经网络用神经网络进行监督学习为什么深度学习会兴起关于这门课 第二周 神经网络基础二分分类logistic 回归logistic 回归损失函数梯度下降法导数更多导数的例子计算图计算图的导数计算logistic 回归中的梯度下降法m个样本的梯度下降向量化向量化的更多例子向量化logistic 回归向量化logistic 回归的梯度输出python中的广播关于Python/Numpy 向量的说明Jupyter/Ipython笔记本的快速指南(选修)logistic 损失函数的解释]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>神经网络，机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[今宵酒醒何处]]></title>
    <url>%2F2017%2F09%2F06%2F%E4%BB%8A%E5%AE%B5%E9%85%92%E9%86%92%E4%BD%95%E5%A4%84%2F</url>
    <content type="text"><![CDATA[大雨滂沱整整一个下午打落思念的果实手机突然响了飞机就要航向 远方那时候凝望青藏铁道窗外地中海的蔚蓝 如今你在何方异国早晨一个我慢慢走Ah bon vin bon vin bon vin boire bon vinAh pas trop pas trop pas trop pas boire trop遗憾是少年时爱与暧昧分不清楚火会熄灭因为风的缘故思念的旅人今宵酒醒何处潇洒是一首诗饮下寂寞在夜深处人会辛苦因为爱的缘故陌生的恋人今宵酒醒何处Ah bon vin bon vin bon vin boire bon vin都门帐饮无绪Ah pas trop pas trop pas trop pas boire trop良辰好景虚设那时候凝望青藏铁道窗外地中海的蔚蓝 如今你在何方异国早晨一个我慢慢走遗憾是少年时爱与暧昧分不清楚火会熄灭因为风的缘故思念的旅人今宵酒醒何处潇洒是一首诗饮下寂寞在夜深处人会辛苦因为爱的缘故陌生的恋人今宵酒醒何处遗憾是少年时爱与暧昧分不清楚火会熄灭因为风的缘故思念的旅人今宵酒醒何处潇洒是一首诗饮下寂寞在夜深处人会辛苦因为爱的缘故陌生的恋人今宵酒醒何处Ah bon vin bon vin bon vin boire bon vin都门帐饮无绪Ah pas trop pas trop pas trop pas boire trop良辰好景虚设Ah bon vin bon vin bon vin boire bon vin都门帐饮无绪Ah pas trop pas trop pas trop pas boire trop良辰好景虚设大雨滂沱整整一个下午打落思念的果实手机突然响了飞机就要航向 远方]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>周传雄</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Welcome back]]></title>
    <url>%2F2017%2F08%2F25%2FWelcome-back%2F</url>
    <content type="text"><![CDATA[Welcome back VG]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>LOL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用理工科思维理解世界]]></title>
    <url>%2F2017%2F07%2F31%2F%E7%94%A8%E7%90%86%E5%B7%A5%E7%A7%91%E6%80%9D%E7%BB%B4%E7%90%86%E8%A7%A3%E4%B8%96%E7%95%8C%2F</url>
    <content type="text"><![CDATA[Part One 反常识思维反常识思维有时候他们把自己的价值判断称为“常识”，因为这写判断本来就是从人的原始思维而来的，然而现代社会产生了另一种思维，却是“反常识”的。 Tradeoff：我们不得不在生活中做出各种取舍，而很多的烦恼恰恰来自不愿意或者不知道取舍。 人脑有两套思维系统,”系统１”,”系统２”，前者起自动作用，能对事物给出一个很难被改变的第一印象；而后者费力而缓慢，需要我们集中注意力进行复杂的计算。系统２根本不是计算机的对手，而系统１却比计算机强大的多（谷歌识别猫脸实验）。文人思维是系统１的集大成者，而理工科思维则是系统２的产物。 Tradeoff 要求量化输入和预计输出，这也是理工科思维的最根本的方法。现在到了用了理工科思维取代文人思维的时候了。传统的文人腔已经越来越少出现在主流媒体上，一篇正经讨论现实问题的文章总要做点计算才说的过去。 “舌战群儒”的技术分析诸葛亮前往东吴说服孙权抗曹。这一仗是打还是不打，正确的讨论方法是摆事实讲道理，推演各种选择的最可能结局。利弊分析，再做决策。但是舌战群儒这场辩论的主题却不是打不打的问题，而是一种从气势上压过对方一头。 舌战群儒的技术不是证明对方的结论不对，而是证明对方这个“人”，或者对方代表的势力，不行。表面上是说具体问题，而实际上都是说人。舌战群儒技术一共三招：（１）列举事实证据，暗示对方能力不行。（２）如果比不过事实，比境界（３）你别说我如何如何不堪，著名英雄XXX也曾经如此过]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[719周老师开会记录]]></title>
    <url>%2F2017%2F07%2F20%2F719%E5%91%A8%E8%80%81%E5%B8%88%E5%BC%80%E4%BC%9A%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[1，做科研其实是很简单的事，掌握一定的手段，发现未知问题，用现有手段去解决问题。在此过程中掌握新手段，再发现新问题，解决新问题。 2，人生的高度取决于你的能力。人可能有很多想法，但是能实现的有多少。做论文是一样的，想法很多，手段单一是弄不成的。 3，为什么总想着比下有余，却不想想比上不足，。为什么不向更优秀的人看齐。 4，有什么事情一定要立刻去做。用最少的时间做最多的事情。 5，别人能做到的你也要做到。有时候不是事情太难，而是我们根本没花多少时间去钻研。 6，都要努力咯。没论文以后申请项目就难。硬件就不要想了。 7，计量党建学，重点实验室。 ——— 来自于 Deepin 15.4 桌面版 &amp; Moeditor V_0.1.1]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>开会记录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[墙外的世界]]></title>
    <url>%2F2017%2F07%2F14%2F%E5%A2%99%E5%A4%96%E7%9A%84%E4%B8%96%E7%95%8C%2F</url>
    <content type="text"><![CDATA[racaljk/hosts目前更新频率较高，关注人也多。正是因为用的人多吧，所以IP坏的也快。开源地址：https://github.com/racaljk/hostsHosts地址：https://raw.githubusercontent.com/racaljk/hosts/master/hosts wangchunming/2017hosts可看YouTube，PC和手机不同的Hosts开源地址：https://github.com/wangchunming/2017hostsHosts地址：https://raw.githubusercontent.com/wangchunming/2017hosts/master/hosts-pchttps://raw.githubusercontent.com/wangchunming/2017hosts/master/hosts-mobile lennylxx/ipv6-hosts特点很明显IPV6嘛开源地址：https://github.com/lennylxx/ipv6-hostsHosts地址：https://raw.githubusercontent.com/lennylxx/ipv6-hosts/master/hosts ——— 来自于 Deepin 15.4 桌面版 &amp; Moeditor V_0.1.1]]></content>
      <categories>
        <category>翻墙</category>
      </categories>
      <tags>
        <tag>VPN</tag>
        <tag>hosts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nokia Lumia 1020 拍摄技巧]]></title>
    <url>%2F2017%2F07%2F04%2FNokia-Lumia-1020-%E6%8B%8D%E6%91%84%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[摄影专业术语理解闪光灯在光线不足的情况，或要在明亮背景中照亮前景对象，则非常适合闪光灯。 亮度（曝光值）在背景光过强或整个 图像非常明亮的情况下拍照，使用亮度尤其有效。比如在大片雪景或者海滩上拍照。它使照片更亮或者更暗。 手动对焦手动对焦可以用来决定在照片上突出哪个对象。通过显示被拍对象与背景之间的距离感，可创建具有景深感的照片。若要拍摄远距离对象或风景，与自动对焦相比，将焦点设为无穷大能更快地排除清晰锐利的画面。 白平衡白平衡可控制相机如何在不同光照条件下再现白色。该设置可确保看起来呈白色的物体在照片中也会再现白色。 光圈值光圈大小最直观的影响是精深，通俗的说就是背景虚化程度。简单点说就是光圈大（光圈值小），背景虚化能力强；光圈小（光圈值大），背景虚化能力小。 快门速度快门速度或曝光时间用于定义相机捕捉光线的时长。1,当需要更多光线，又不希望使用闪光灯时，可以使用较慢的快门速度。２，所有对象都需要保持更长的静止时间，但在这种情况下，诺基亚的ＯＩＳ光学防抖可以帮助拍摄出更清晰的照片。３，较短快门速度非常适合拍摄移动对象，但这种情况下需要有大量的光线。 灵敏度（ISO、感光度）灵敏度可控制相机对光线的敏感度。高敏感度值可提高快门速度（缩短曝光时间），但会更加照片中的噪点。低灵敏度值将减少噪点。感光度小（比如ISO100）画质细腻噪点少；感光度大（比如ISO6400、ISO12800）画质非常差，噪点很多。 曝光的核心“摄影是用光的艺术”曝光的三个参数：光圈值、快门速度、灵敏度。一张图形象说明三个参数： Nokia Lumia 1020相机参数相机参数后置摄像头：4100 万像素 PureView摄像头后置摄像头光圈：f/2.2相机焦距: 26 毫米传感器类型：CMOS摄像头类型：卡尔·蔡司认证光学防抖：第二代O.I.S.光学防抖闪光灯：LED补光灯+氙气闪光灯 摄影应用程序诺基亚专业拍摄: 充分利用诺基亚 Lumia 非凡系列相机：在自动模式下可以拍出效果出众的照片，您也可以像使用 DSLR 相机一样手动控制曝光、快门速度、白平衡和焦距。可以用不同的取景网格帮助构图，并且如果照片没有十分笔直，不要担心 - 可以稍后将它拉直并进行裁减，即便一直重复操作也不会有任何问题。此外，我们还首次在诺基亚 Lumia 非凡系列中采用了 HD 视频模式，可以录制高品质立体声。诺基亚全景: 借助诺基亚易于使用的 Panorama 应用程序拍摄更大的照片。只需拍摄照片，应用程序就会自动将照片整合到一个正好合适的视图中。准备好之后，就可直接发布到社交网站上与朋友共享。创意工作室（需从应用商店下载）: 利用这种快速简便的照片编辑器，制作出效果更加精良的照片。您可使用 Creative Studio 的编辑工具快速调整色彩平衡、去除红眼并使用滤镜。然后直接在社交网络上共享您的照片。Nokia 智能拍摄: 拍摄一系列照片，更轻松地捕捉精彩瞬间。选择最佳照片，或将这些照片合成一张图片，利用闪光灯效果来凸显动感，删除不需要的对象，或选择拍的最好的面孔，来合成最精彩的集体照。诺基亚魅力魔镜: 魅力魔镜是适用于 Windows Phone 的主要美颜应用程序。有了魅力魔镜，您可以通过调色、调整眼睛大小、美白牙齿以及各种美化效果，让自己的照片更加美丽动人。Nokia 动态图片: 照片如电影动画般的神奇组合，创造了看起来几乎栩栩如生的图片。实用的屏幕帮助让您可以选择您图片的动画区域，并轻松地创建和编辑动态图片。您可以通过社交媒体、电子邮件和信息传送功能与朋友分享您的动态照片。 摄影技巧１，曝光补偿：光补偿可整体调整画面的亮度。增加曝光补偿可将画面整体变亮，但是画面中原本亮度足够的区域也可能因此而过度曝光，丢失色彩和细节。减少曝光补偿可将画面整体变暗，使得强光下过于明亮的部分清晰的展现出来，但是画面中原本亮度不够的区域可能因此而曝光不足，令细节丢失。曝光补偿不是万能的，过暗的情况下仍然需要调整快门速度、闪光灯或ISO来提高画面亮度。调节曝光补偿的一般原则是：光线比较强烈的时候，可降低 0.3-0.7档的曝光。光线较为不足，同时又无法使用慢速快门的情况下，可调高 0.3-0.7档的曝光。 ２，白平衡：在特定光源下拍摄物体有时会出现偏色现象，可以通过加强对应的补色来进行补偿。调节白平衡可让被拍摄的物体在不同的光线环境中呈现出 “本来”的颜色。调节白平衡的一般原则是：一般选择自动白平衡，除非画面颜色严重与物体的真实颜色不符，或有意对画面进行色彩创作。在进行色彩创作时，如果要强调蓝天、大海的颜色时，可设置为荧光灯白平衡，减少画面中的暖色调，令画面偏冷。在进行色彩创作时，如果要强调温馨的感觉以及食物的色泽时，可设置为阴天白平衡，增加画面中的暖色调，令画面偏暖。 ３，夜景使用诺基亚Lumia1020 拍摄夜景的方式大致为两种，一种是手持直接拍摄，借助光学防抖的强大功能得到出色的照片。另一种，是借助诺基亚Lumia1020 的专业拍摄设置拍出媲美单反相机的夜景照片1.将手机固定，推荐使用带有充电功能的诺基亚PD-95G 拍照手柄连接三脚架2.感光度设置为 ISO1003.白平衡设置为荧光灯模式，以便强调天空的蓝色4.手动对焦设置为无限远5.在 “诺基亚专业拍摄”界面中点击 “。。。”，在菜单中选择 “快门延迟”，点击快门拍摄如果拍摄时无法将手机固定，只能手持拍摄，则可按照如下方法拍摄：1.将感光度设置为 ISO8002.手动设置 1/15-1/20之间的快门速度3.端稳相机，点击快门拍摄 更多：http://bbs.dospy.com/thread-17393238-1-855-1.html ——— 来自于 Deepin 15.4 桌面版 &amp; Moeditor V_0.1.1]]></content>
      <categories>
        <category>摄影</category>
      </categories>
      <tags>
        <tag>Lumia</tag>
        <tag>照片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[One Question]]></title>
    <url>%2F2017%2F06%2F24%2F%E4%BB%8A%E5%A4%A9%E5%BC%80%E6%AF%95%E4%B8%9A%E5%85%B8%E7%A4%BC%2F</url>
    <content type="text"><![CDATA[突然想到一个问题：AlphaGo 和 Master 下棋谁更厉害？]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>AlphaGo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挥挥手]]></title>
    <url>%2F2017%2F06%2F13%2F%E6%8C%A5%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[天空下着冰冷的雨落在原来的地方化成烟模糊了视线昨天已越来越遥远我没做过任何坏事却为何要经受这番坎坷罢了挥挥手告别昨日的自己 挥挥手等待你出现]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gensim学习笔记]]></title>
    <url>%2F2017%2F06%2F04%2Fgensim%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[LDA主题模型准备数据1、中文维基百科数据2、gensim中Corpus类处理数据(*xml.bz2)12&gt;&gt;&gt; wiki = WikiCorpus(&apos;enwiki-20100622-pages-articles.xml.bz2&apos;) &gt;&gt;&gt; MmCorpus.serialize(&apos;wiki_en_vocab200k.mm&apos;, wiki) 12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-import loggingimport sysfrom gensim.corpora import WikiCorpuslogging.basicConfig(format=&apos;%(asctime)s: %(levelname)s: %(message)s&apos;, level=logging.INFO)def help(): print &quot;Usage: python process_wiki.py zhwiki-latest-pages-articles.xml.bz2 wiki.zh.txt&quot;if __name__ == &apos;__main__&apos;: if len(sys.argv) &lt; 3: help() sys.exit(1) logging.info(&quot;running %s&quot; % &apos; &apos;.join(sys.argv)) inp, outp = sys.argv[1:3] i = 0 output = open(outp, &apos;w&apos;) wiki = WikiCorpus(inp, lemmatize=False, dictionary=&#123;&#125;) for text in wiki.get_texts(): output.write(&quot; &quot;.join(text) + &quot;\n&quot;) i = i + 1 if (i % 10000 == 0): logging.info(&quot;Save &quot;+str(i) + &quot; articles&quot;) output.close() logging.info(&quot;Finished saved &quot;+str(i) + &quot;articles&quot;) process_wiki_1.py 3、数据预处理123456789101112131415#!/bin/bash# Traditional Chinese to Simplified Chineseecho &quot;opencc: Traditional Chinese to Simplified Chinese...&quot;#time opencc -i wiki.zh.txt -o wiki.zh.chs.txt -c zht2zhs.initime opencc -i wiki.zh.txt -o wiki.zh.chs.txt -c t2s.json# Cut wordsecho &quot;jieba: Cut words...&quot;time python -m jieba -d &apos; &apos; wiki.zh.chs.txt &gt; wiki.zh.seg.txt# Change encodeecho &quot;iconv: ascii to utf-8...&quot;time iconv -c -t UTF-8 &lt; wiki.zh.seg.txt &gt; wiki.zh.seg.utf.txtprocess_wiki_2.sh 处理完之后的数据，已经分好词：http://pan.baidu.com/s/1gfMhkcV 密码：gdua LDA实验1、去掉停用词后即可训练lda模型123456789101112131415import codecsfrom gensim.models import LdaModelfrom gensim.corpora import Dictionarytrain = []stopwords = codecs.open(&apos;stopwords.txt&apos;,&apos;r&apos;,encoding=&apos;utf8&apos;).readlines()stopwords = [ w.strip() for w in stopwords ]fp = codecs.open(&apos;wiki.zh.word.txt&apos;,&apos;r&apos;,encoding=&apos;utf8&apos;)for line in fp: line = line.split() train.append([ w for w in line if w not in stopwords ])dictionary = corpora.Dictionary(train)corpus = [ dictionary.doc2bow(text) for text in train ]lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=100) 停用词下载：http://pan.baidu.com/s/1qYnsSLe 密码：s0hc 此外，gensim也提供了对wiki压缩包直接进行抽取并保存为稀疏矩阵的脚本 make_wiki，可在bash运行下面命令查看用法。12345678910111213141516python -m gensim.scripts.make_wiki#USAGE: make_wiki.py WIKI_XML_DUMP OUTPUT_PREFIX [VOCABULARY_SIZE]USAGE: make_wiki.py WIKI_XML_DUMP OUTPUT_PREFIX [VOCABULARY_SIZE]Convert articles from a Wikipedia dump to (sparse) vectors. The input is abz2-compressed dump of Wikipedia articles, in XML format.This actually creates three files:* `OUTPUT_PREFIX_wordids.txt`: mapping between words and their integer ids* `OUTPUT_PREFIX_bow.mm`: bag-of-words (word counts) representation, in Matrix Matrix format* `OUTPUT_PREFIX_tfidf.mm`: TF-IDF representation* `OUTPUT_PREFIX.tfidf_model`: TF-IDF model dumppython -m gensim.scripts.make_wiki zhwiki-latest-pages-articles.xml.bz2 zhwiki 将文章变成清晰的文本，并以稀疏TF-IDF向量存储。在具体情况可以看gensim官网，mm后缀表示Matrix Market格式保存的稀疏矩阵. 2、实验部分利用 tfidf.mm 及wordids.txt 训练LDA模型123456789# -*- coding: utf-8 -*-from gensim import corpora, models# 语料导入id2word = corpora.Dictionary.load_from_text(&apos;zhwiki_wordids.txt&apos;)mm = corpora.MmCorpus(&apos;zhwiki_tfidf.mm&apos;)# 模型训练lda = models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100) 3、模型结果训练过程指定参数 num_topics=100, 即训练100个主题，通过print_topics() 和print_topic() 可查看各个主题下的词分布，也可通过save/load 进行模型保存加载。12345678# 打印前20个topic的词分布lda.print_topics(20)# 打印id为20的topic的词分布lda.print_topic(20)#模型的保存/ 加载lda.save(&apos;zhwiki_lda.model&apos;)lda = models.ldamodel.LdaModel.load(&apos;zhwiki_lda.model&apos;) 4、主题预测对新文档，转换成bag-of-word后，可进行主题预测。模型差别主要在于主题数的设置，以及语料本身，wiki语料是全领域语料，主题分布并不明显，而且这里使用的语料没有去停止词，得到的结果差强人意。1234567test_doc = list(jieba.cut(test_doc)) #新文档进行分词doc_bow = id2word.doc2bow(test_doc) #文档转换成bowdoc_lda = lda[doc_bow] #得到新文档的主题分布#输出新文档的主题分布print doc_ldafor topic in doc_lda: print &quot;%s\t%f\n&quot;%(lda.print_topic(topic[0]), topic[1])]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>gensim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算两个文本的相似度]]></title>
    <url>%2F2017%2F05%2F30%2F%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%9C%AC%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[相关背景资料主题模型采用LSI(Latent semantic indexing, 中文译为浅层语义索引），LSI区别LSA（Latent semantic analysis，中文译为浅层语义分析） 1） TF-IDF，余弦相似度，向量空间模型TF-IDF与余弦相似性的应用（一）：自动提取关键词、TF-IDF与余弦相似性的应用（二）：找出相似文章 2）SVD和LSI想了解LSI一定要知道SVD（Singular value decomposition, 中文译为奇异值分解），而SVD的作用不仅仅局限于LSI，在很多地方都能见到其身影，SVD自诞生之后，其应用领域不断被发掘，可以不夸张的说如果学了线性代数而不明白SVD，基本上等于没学。教程：机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)、机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 关于LSI，简单说两句，一种情况下考察两个词的关系常常考虑的是它们在一个窗口长度（譬如一句话，一段话或一个文章）里的共现情况，在语料库语言学里有个专业点叫法叫Collocation，中文译为搭配或词语搭配。而LSI所做的是挖掘如下这层词语关系：A和C共现，B和C共现，目标是找到A和B的隐含关系，学术一点的叫法是second-order co-ocurrence。以下引用百度空间上一篇介绍相关参考资料时的简要描述： LSI本质上识别了以文档为单位的second-order co-ocurrence的单词并归入同一个子空间。因此：1）落在同一子空间的单词不一定是同义词，甚至不一定是在同情景下出现的单词，对于长篇文档尤其如是。2）LSI根本无法处理一词多义的单词（多义词），多义词会导致LSI效果变差。 A persistent myth in search marketing circles is that LSI grants contextuality; i.e., terms occurring in the same context. This is not always the case. Consider two documents X and Y and three terms A, B and C and wherein: A and B do not co-occur.X mentions terms A and CY mentions terms B and C. :. A—C—B The common denominator is C, so we define this relation as an in-transit co-occurrence since both A and B occur while in transit with C. This is called second-order co-occurrence and is a special case of high-order co-occurrence. PDF Tutorial版本：Singular Value Decomposition (SVD)- A Fast Track Tutorial Latent Semantic Indexing (LSI) A Fast Track Tutorial 3) LDA机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA) gensim基本使用from gensim import corpora, models, similarities import logging logging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level=logging.INFO) 然后将上面那个文档中的例子作为文档输入，在Python中用document list表示： documents = [&quot;Shipment of gold damaged in a fire&quot;, &quot;Delivery of silver arrived in a silver truck&quot;, &quot;Shipment of gold arrived in a truck&quot;] 正常情况下，需要对英文文本做一些预处理工作，譬如去停用词，对文本进行tokenize，stemming以及过滤掉低频的词，但是为了说明问题，也是为了和这篇”LSI Fast Track Tutorial”保持一致，以下的预处理仅仅是将英文单词小写化： texts = [[word for word in document.lower().split()] for document in documents] print texts #[[&apos;shipment&apos;, &apos;of&apos;, &apos;gold&apos;, &apos;damaged&apos;, &apos;in&apos;, &apos;a&apos;, &apos;fire&apos;], [&apos;delivery&apos;, &apos;of&apos;, &apos;silver&apos;, &apos;arrived&apos;, &apos;in&apos;, &apos;a&apos;, &apos;silver&apos;, &apos;truck&apos;], [&apos;shipment&apos;, &apos;of&apos;, &apos;gold&apos;, &apos;arrived&apos;, &apos;in&apos;, &apos;a&apos;, &apos;truck&apos;]] 我们可以通过这些文档抽取一个“词袋（bag-of-words)”，将文档的token映射为id： dictionary = corpora.Dictionary(texts) print dictionary #Dictionary(11 unique tokens) print dictionary.token2id #{&apos;a&apos;: 0, &apos;damaged&apos;: 1, &apos;gold&apos;: 3, &apos;fire&apos;: 2, &apos;of&apos;: 5, &apos;delivery&apos;: 8, &apos;arrived&apos;: 7, &apos;shipment&apos;: 6, &apos;in&apos;: 4, &apos;truck&apos;: 10, &apos;silver&apos;: 9} 然后就可以将用字符串表示的文档转换为用id表示的文档向量： corpus = [dictionary.doc2bow(text) for text in texts] print corpus #[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (4, 1), (5, 1), (7, 1), (8, 1), (9, 2), (10, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (10, 1)]] 例如（9，2）这个元素代表第二篇文档中id为9的单词“silver”出现了2次。 有了这些信息，我们就可以基于这些“训练文档”计算一个TF-IDF“模型”： tfidf = models.TfidfModel(corpus) 基于这个TF-IDF模型，我们可以将上述用词频表示文档向量表示为一个用tf-idf值表示的文档向量： corpus_tfidf = tfidf[corpus] for doc in corpus_tfidf: ... print doc #[(1, 0.6633689723434505), (2, 0.6633689723434505), (3, 0.2448297500958463), (6, 0.2448297500958463)] [(7, 0.16073253746956623), (8, 0.4355066251613605), (9, 0.871013250322721), (10, 0.16073253746956623)] [(3, 0.5), (6, 0.5), (7, 0.5), (10, 0.5)] 发现一些token貌似丢失了，我们打印一下tfidf模型中的信息： print tfidf.dfs #{0: 3, 1: 1, 2: 1, 3: 2, 4: 3, 5: 3, 6: 2, 7: 2, 8: 1, 9: 1, 10: 2} print tfidf.idfs #{0: 0.0, 1: 1.5849625007211563, 2: 1.5849625007211563, 3: 0.5849625007211562, 4: 0.0, 5: 0.0, 6: 0.5849625007211562, 7: 0.5849625007211562, 8: 1.5849625007211563, 9: 1.5849625007211563, 10: 0.5849625007211562} 我们发现由于包含id为0， 4， 5这3个单词的文档数（df)为3，而文档总数也为3，所以idf被计算为0了，看来gensim没有对分子加1，做一个平滑。不过我们同时也发现这3个单词分别为a, in, of这样的介词，完全可以在预处理时作为停用词干掉，这也从另一个方面说明TF-IDF的有效性。 有了tf-idf值表示的文档向量，我们就可以训练一个LSI模型，和Latent Semantic Indexing (LSI) A Fast Track Tutorial中的例子相似，我们设置topic数为2： lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) lsi.print_topics(2) # topic #0(1.137): 0.438*&quot;gold&quot; + 0.438*&quot;shipment&quot; + 0.366*&quot;truck&quot; + 0.366*&quot;arrived&quot; + 0.345*&quot;damaged&quot; + 0.345*&quot;fire&quot; + 0.297*&quot;silver&quot; + 0.149*&quot;delivery&quot; + 0.000*&quot;in&quot; + 0.000*&quot;a&quot; topic #1(1.000): 0.728*&quot;silver&quot; + 0.364*&quot;delivery&quot; + -0.364*&quot;fire&quot; + -0.364*&quot;damaged&quot; + 0.134*&quot;truck&quot; + 0.134*&quot;arrived&quot; + -0.134*&quot;shipment&quot; + -0.134*&quot;gold&quot; + -0.000*&quot;a&quot; + -0.000*&quot;in&quot; lsi的物理意义不太好解释，不过最核心的意义是将训练文档向量组成的矩阵SVD分解，并做了一个秩为2的近似SVD分解，可以参考那篇英文tutorail。有了这个lsi模型，我们就可以将文档映射到一个二维的topic空间中： corpus_lsi = lsi[corpus_tfidf] for doc in corpus_lsi: ... print doc #[(0, 0.67211468809878649), (1, -0.54880682119355917)] [(0, 0.44124825208697727), (1, 0.83594920480339041)] [(0, 0.80401378963792647)] 可以看出，文档1，3和topic1更相关，文档2和topic2更相关； 我们也可以顺手跑一个LDA模型： lda = models.LdaModel(copurs_tfidf, id2word=dictionary, num_topics=2) lda.print_topics(2) #topic #0: 0.119*silver + 0.107*shipment + 0.104*truck + 0.103*gold + 0.102*fire + 0.101*arrived + 0.097*damaged + 0.085*delivery + 0.061*of + 0.061*in topic #1: 0.110*gold + 0.109*silver + 0.105*shipment + 0.105*damaged + 0.101*arrived + 0.101*fire + 0.098*truck + 0.090*delivery + 0.061*of + 0.061*in lda模型中的每个主题单词都有概率意义，其加和为1，值越大权重越大，物理意义比较明确，不过反过来再看这三篇文档训练的2个主题的LDA模型太平均了，没有说服力。 好了，我们回到LSI模型，有了LSI模型，我们如何来计算文档直接的相思度，或者换个角度，给定一个查询Query，如何找到最相关的文档？当然首先是建索引了： index = similarities.MatrixSimilarity(lsi[corpus]) 还是以这篇英文tutorial中的查询Query为例：gold silver truck。首先将其向量化： query = &quot;gold silver truck&quot; query_bow = dictionary.doc2bow(query.lower().split()) print query_bow [(3, 1), (9, 1), (10, 1)] 再用之前训练好的LSI模型将其映射到二维的topic空间： query_lsi = lsi[query_bow] print query_lsi [(0, 1.1012835748628467), (1, 0.72812283398049593)] 最后就是计算其和index中doc的余弦相似度了： sims = index[query_lsi] print list(enumerate(sims)) [(0, 0.40757114), (1, 0.93163693), (2, 0.83416492)] 当然，我们也可以按相似度进行排序： sort_sims = sorted(enumerate(sims), key=lambda item: -item[1]) print sort_sims [(1, 0.93163693), (2, 0.83416492), (0, 0.40757114)] 可以看出，这个查询的结果是doc2 &gt; doc3 &gt; doc1，和fast tutorial是一致的，虽然数值上有一些差别： 计算两个文档的相似度本节将主要说明如何基于gensim计算课程图谱上课程之间的主题相似度，同时考虑一些改进方法，包括借助英文的自然语言处理工具包NLTK以及用更大的维基百科的语料来看看效果。 1、数据准备这里准备了一份Coursera的课程数据，可以在这里下载：coursera_corpus，（百度网盘链接: http://t.cn/RhjgPkv, 密码: oppc）总共379个课程，每行包括3部分内容：课程名\t课程简介\t课程详情, 已经清除了其中的html tag, 下面所示的例子仅仅是其中的课程名： Writing II: Rhetorical Composing Genetics and Society: A Course for Educators General Game Playing Genes and the Human Condition (From Behavior to Biotechnology) A Brief History of Humankind New Models of Business in Society Analyse Numérique pour Ingénieurs Evolution: A Course for Educators Coding the Matrix: Linear Algebra through Computer Science Applications The Dynamic Earth: A Course for Educators ... 首先加载数据： from gensim import corpora, models, similarities import logging logging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level=logging.INFO) import csv file = open(&quot;coursera_corpus&quot;,&apos;r&apos;,encoding = &apos;UTF8&apos;) courses = [line.strip() for line in file] courses_name = [course.split(&apos;\t&apos;)[0] for course in courses] print(courses_name[0:10]) #[&apos;Writing II: Rhetorical Composing&apos;, &apos;Genetics and Society: A Course for Educators&apos;, &apos;General Game Playing&apos;, &apos;Genes and the Human Condition (From Behavior to Biotechnology)&apos;, &apos;A Brief History of Humankind&apos;, &apos;New Models of Business in Society&apos;, &apos;Analyse Num\xc3\xa9rique pour Ing\xc3\xa9nieurs&apos;, &apos;Evolution: A Course for Educators&apos;, &apos;Coding the Matrix: Linear Algebra through Computer Science Applications&apos;, &apos;The Dynamic Earth: A Course for Educators&apos;] 2、引入NLTKNTLK是著名的Python自然语言处理工具包，但是主要针对的是英文处理，不过课程图谱目前处理的课程数据主要是英文，因此也足够了。 import nltk nltk.download() 现在来处理刚才的课程数据，如果按此前的方法仅仅对文档的单词小写化的话，将得到如下的结果： exts_lower = [[word for word in document.lower().split()] for document in courses] print texts_lower[0] #[&apos;writing&apos;, &apos;ii:&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;engages&apos;, &apos;you&apos;, &apos;in&apos;, &apos;a&apos;, &apos;series&apos;, &apos;of&apos;, &apos;interactive&apos;, &apos;reading,&apos;, &apos;research,&apos;, &apos;and&apos;, &apos;composing&apos;, &apos;activities&apos;, &apos;along&apos;, &apos;with&apos;, &apos;assignments&apos;, &apos;designed&apos;, &apos;to&apos;, &apos;help&apos;, &apos;you&apos;, &apos;become&apos;, &apos;more&apos;, &apos;effective&apos;, &apos;consumers&apos;, &apos;and&apos;, &apos;producers&apos;, &apos;of&apos;, &apos;alphabetic,&apos;, &apos;visual&apos;, &apos;and&apos;, &apos;multimodal&apos;, &apos;texts.&apos;, &apos;join&apos;, &apos;us&apos;, &apos;to&apos;, &apos;become&apos;, &apos;more&apos;, &apos;effective&apos;, &apos;writers...&apos;, &apos;and&apos;, &apos;better&apos;, &apos;citizens.&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;is&apos;, &apos;a&apos;, &apos;course&apos;, &apos;where&apos;, &apos;writers&apos;, &apos;exchange&apos;, &apos;words,&apos;, &apos;ideas,&apos;, &apos;talents,&apos;, &apos;and&apos;, &apos;support.&apos;, &apos;you&apos;, &apos;will&apos;, &apos;be&apos;, &apos;introduced&apos;, &apos;to&apos;, &apos;a&apos;, ... 注意其中很多标点符号和单词是没有分离的，所以我们引入nltk的word_tokenize函数，并处理相应的数据： from nltk.tokenize import word_tokenize texts_tokenized = [[word.lower() for word in word_tokenize(document.decode(&apos;utf-8&apos;))] for document in courses] print texts_tokenized[0] #[&apos;writing&apos;, &apos;ii&apos;, &apos;:&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;engages&apos;, &apos;you&apos;, &apos;in&apos;, &apos;a&apos;, &apos;series&apos;, &apos;of&apos;, &apos;interactive&apos;, &apos;reading&apos;, &apos;,&apos;, &apos;research&apos;, &apos;,&apos;, &apos;and&apos;, &apos;composing&apos;, &apos;activities&apos;, &apos;along&apos;, &apos;with&apos;, &apos;assignments&apos;, &apos;designed&apos;, &apos;to&apos;, &apos;help&apos;, &apos;you&apos;, &apos;become&apos;, &apos;more&apos;, &apos;effective&apos;, &apos;consumers&apos;, &apos;and&apos;, &apos;producers&apos;, &apos;of&apos;, &apos;alphabetic&apos;, &apos;,&apos;, &apos;visual&apos;, &apos;and&apos;, &apos;multimodal&apos;, &apos;texts.&apos;, &apos;join&apos;, &apos;us&apos;, &apos;to&apos;, &apos;become&apos;, &apos;more&apos;, &apos;effective&apos;, &apos;writers&apos;, &apos;...&apos;, &apos;and&apos;, &apos;better&apos;, &apos;citizens.&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;is&apos;, &apos;a&apos;, &apos;course&apos;, &apos;where&apos;, &apos;writers&apos;, &apos;exchange&apos;, &apos;words&apos;, &apos;,&apos;, &apos;ideas&apos;, &apos;,&apos;, &apos;talents&apos;, &apos;,&apos;, &apos;and&apos;, &apos;support.&apos;, &apos;you&apos;, &apos;will&apos;, &apos;be&apos;, &apos;introduced&apos;, &apos;to&apos;, &apos;a&apos;, ... 对课程的英文数据进行tokenize之后，我们需要去停用词，幸好NLTK提供了一份英文停用词数据： from nltk.corpus import stopwords english_stopwords = stopwords.words(&apos;english&apos;) print english_stopwords #[&apos;i&apos;, &apos;me&apos;, &apos;my&apos;, &apos;myself&apos;, &apos;we&apos;, &apos;our&apos;, &apos;ours&apos;, &apos;ourselves&apos;, &apos;you&apos;, &apos;your&apos;, &apos;yours&apos;, &apos;yourself&apos;, &apos;yourselves&apos;, &apos;he&apos;, &apos;him&apos;, &apos;his&apos;, &apos;himself&apos;, &apos;she&apos;, &apos;her&apos;, &apos;hers&apos;, &apos;herself&apos;, &apos;it&apos;, &apos;its&apos;, &apos;itself&apos;, &apos;they&apos;, &apos;them&apos;, &apos;their&apos;, &apos;theirs&apos;, &apos;themselves&apos;, &apos;what&apos;, &apos;which&apos;, &apos;who&apos;, &apos;whom&apos;, &apos;this&apos;, &apos;that&apos;, &apos;these&apos;, &apos;those&apos;, &apos;am&apos;, &apos;is&apos;, &apos;are&apos;, &apos;was&apos;, &apos;were&apos;, &apos;be&apos;, &apos;been&apos;, &apos;being&apos;, &apos;have&apos;, &apos;has&apos;, &apos;had&apos;, &apos;having&apos;, &apos;do&apos;, &apos;does&apos;, &apos;did&apos;, &apos;doing&apos;, &apos;a&apos;, &apos;an&apos;, &apos;the&apos;, &apos;and&apos;, &apos;but&apos;, &apos;if&apos;, &apos;or&apos;, &apos;because&apos;, &apos;as&apos;, &apos;until&apos;, &apos;while&apos;, &apos;of&apos;, &apos;at&apos;, &apos;by&apos;, &apos;for&apos;, &apos;with&apos;, &apos;about&apos;, &apos;against&apos;, &apos;between&apos;, &apos;into&apos;, &apos;through&apos;, &apos;during&apos;, &apos;before&apos;, &apos;after&apos;, &apos;above&apos;, &apos;below&apos;, &apos;to&apos;, &apos;from&apos;, &apos;up&apos;, &apos;down&apos;, &apos;in&apos;, &apos;out&apos;, &apos;on&apos;, &apos;off&apos;, &apos;over&apos;, &apos;under&apos;, &apos;again&apos;, &apos;further&apos;, &apos;then&apos;, &apos;once&apos;, &apos;here&apos;, &apos;there&apos;, &apos;when&apos;, &apos;where&apos;, &apos;why&apos;, &apos;how&apos;, &apos;all&apos;, &apos;any&apos;, &apos;both&apos;, &apos;each&apos;, &apos;few&apos;, &apos;more&apos;, &apos;most&apos;, &apos;other&apos;, &apos;some&apos;, &apos;such&apos;, &apos;no&apos;, &apos;nor&apos;, &apos;not&apos;, &apos;only&apos;, &apos;own&apos;, &apos;same&apos;, &apos;so&apos;, &apos;than&apos;, &apos;too&apos;, &apos;very&apos;, &apos;s&apos;, &apos;t&apos;, &apos;can&apos;, &apos;will&apos;, &apos;just&apos;, &apos;don&apos;, &apos;should&apos;, &apos;now&apos;] len(english_stopwords) #127 总计127个停用词，我们首先过滤课程语料中的停用词： texts_filtered_stopwords = [[word for word in document if not word in english_stopwords] for document in texts_tokenized] print texts_filtered_stopwords[0] #[&apos;writing&apos;, &apos;ii&apos;, &apos;:&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;engages&apos;, &apos;series&apos;, &apos;interactive&apos;, &apos;reading&apos;, &apos;,&apos;, &apos;research&apos;, &apos;,&apos;, &apos;composing&apos;, &apos;activities&apos;, &apos;along&apos;, &apos;assignments&apos;, &apos;designed&apos;, &apos;help&apos;, &apos;become&apos;, &apos;effective&apos;, &apos;consumers&apos;, &apos;producers&apos;, &apos;alphabetic&apos;, &apos;,&apos;, &apos;visual&apos;, &apos;multimodal&apos;, &apos;texts.&apos;, &apos;join&apos;, &apos;us&apos;, &apos;become&apos;, &apos;effective&apos;, &apos;writers&apos;, &apos;...&apos;, &apos;better&apos;, &apos;citizens.&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;course&apos;, &apos;writers&apos;, &apos;exchange&apos;, &apos;words&apos;, &apos;,&apos;, &apos;ideas&apos;, &apos;,&apos;, &apos;talents&apos;, &apos;,&apos;, &apos;support.&apos;, &apos;introduced&apos;, &apos;variety&apos;, &apos;rhetorical&apos;, &apos;concepts\xe2\x80\x94that&apos;, &apos;,&apos;, &apos;ideas&apos;, &apos;techniques&apos;, &apos;inform&apos;, &apos;persuade&apos;, &apos;audiences\xe2\x80\x94that&apos;, &apos;help&apos;, &apos;become&apos;, &apos;effective&apos;, &apos;consumer&apos;, &apos;producer&apos;, &apos;written&apos;, &apos;,&apos;, &apos;visual&apos;, &apos;,&apos;, &apos;multimodal&apos;, &apos;texts.&apos;, &apos;class&apos;, &apos;includes&apos;, &apos;short&apos;, &apos;videos&apos;, &apos;,&apos;, &apos;demonstrations&apos;, &apos;,&apos;, &apos;activities.&apos;, &apos;envision&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;learning&apos;, &apos;community&apos;, &apos;includes&apos;, &apos;enrolled&apos;, &apos;course&apos;, &apos;instructors.&apos;, &apos;bring&apos;, &apos;expertise&apos;, &apos;writing&apos;, &apos;,&apos;, &apos;rhetoric&apos;, &apos;course&apos;, &apos;design&apos;, &apos;,&apos;, &apos;designed&apos;, &apos;assignments&apos;, &apos;course&apos;, &apos;infrastructure&apos;, &apos;help&apos;, &apos;share&apos;, &apos;experiences&apos;, &apos;writers&apos;, &apos;,&apos;, &apos;students&apos;, &apos;,&apos;, &apos;professionals&apos;, &apos;us.&apos;, &apos;collaborations&apos;, &apos;facilitated&apos;, &apos;wex&apos;, &apos;,&apos;, &apos;writers&apos;, &apos;exchange&apos;, &apos;,&apos;, &apos;place&apos;, &apos;exchange&apos;, &apos;work&apos;, &apos;feedback&apos;] 停用词被过滤了，不过发现标点符号还在，这个好办，我们首先定义一个标点符号list: english_punctuations = [&apos;,&apos;, &apos;.&apos;, &apos;:&apos;, &apos;;&apos;, &apos;?&apos;, &apos;(&apos;, &apos;)&apos;, &apos;[&apos;, &apos;]&apos;, &apos;&amp;&apos;, &apos;!&apos;, &apos;*&apos;, &apos;@&apos;, &apos;#&apos;, &apos;$&apos;, &apos;%&apos;] 然后过滤这些标点符号： texts_filtered = [[word for word in document if not word in english_punctuations] for document in texts_filtered_stopwords] print texts_filtered[0] #[&apos;writing&apos;, &apos;ii&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;engages&apos;, &apos;series&apos;, &apos;interactive&apos;, &apos;reading&apos;, &apos;research&apos;, &apos;composing&apos;, &apos;activities&apos;, &apos;along&apos;, &apos;assignments&apos;, &apos;designed&apos;, &apos;help&apos;, &apos;become&apos;, &apos;effective&apos;, &apos;consumers&apos;, &apos;producers&apos;, &apos;alphabetic&apos;, &apos;visual&apos;, &apos;multimodal&apos;, &apos;texts.&apos;, &apos;join&apos;, &apos;us&apos;, &apos;become&apos;, &apos;effective&apos;, &apos;writers&apos;, &apos;...&apos;, &apos;better&apos;, &apos;citizens.&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;course&apos;, &apos;writers&apos;, &apos;exchange&apos;, &apos;words&apos;, &apos;ideas&apos;, &apos;talents&apos;, &apos;support.&apos;, &apos;introduced&apos;, &apos;variety&apos;, &apos;rhetorical&apos;, &apos;concepts\xe2\x80\x94that&apos;, &apos;ideas&apos;, &apos;techniques&apos;, &apos;inform&apos;, &apos;persuade&apos;, &apos;audiences\xe2\x80\x94that&apos;, &apos;help&apos;, &apos;become&apos;, &apos;effective&apos;, &apos;consumer&apos;, &apos;producer&apos;, &apos;written&apos;, &apos;visual&apos;, &apos;multimodal&apos;, &apos;texts.&apos;, &apos;class&apos;, &apos;includes&apos;, &apos;short&apos;, &apos;videos&apos;, &apos;demonstrations&apos;, &apos;activities.&apos;, &apos;envision&apos;, &apos;rhetorical&apos;, &apos;composing&apos;, &apos;learning&apos;, &apos;community&apos;, &apos;includes&apos;, &apos;enrolled&apos;, &apos;course&apos;, &apos;instructors.&apos;, &apos;bring&apos;, &apos;expertise&apos;, &apos;writing&apos;, &apos;rhetoric&apos;, &apos;course&apos;, &apos;design&apos;, &apos;designed&apos;, &apos;assignments&apos;, &apos;course&apos;, &apos;infrastructure&apos;, &apos;help&apos;, &apos;share&apos;, &apos;experiences&apos;, &apos;writers&apos;, &apos;students&apos;, &apos;professionals&apos;, &apos;us.&apos;, &apos;collaborations&apos;, &apos;facilitated&apos;, &apos;wex&apos;, &apos;writers&apos;, &apos;exchange&apos;, &apos;place&apos;, &apos;exchange&apos;, &apos;work&apos;, &apos;feedback&apos;] 更进一步，我们对这些英文单词词干化（Stemming)，NLTK提供了好几个相关工具接口可供选择，具体参考这个页面: http://nltk.org/api/nltk.stem.html , 可选的工具包括Lancaster Stemmer, Porter Stemmer等知名的英文Stemmer。这里我们使用LancasterStemmer: from nltk.stem.lancaster import LancasterStemmer st = LancasterStemmer() st.stem(&apos;stemmed&apos;) #&apos;stem&apos; st.stem(&apos;stemming&apos;) #&apos;stem&apos; st.stem(&apos;stemmer&apos;) #&apos;stem&apos; st.stem(&apos;running&apos;) #&apos;run&apos; st.stem(&apos;maximum&apos;) #&apos;maxim&apos; st.stem(&apos;presumably&apos;) #&apos;presum&apos; 让我们调用这个接口来处理上面的课程数据: texts_stemmed = [[st.stem(word) for word in docment] for docment in texts_filtered] print texts_stemmed[0] #[&apos;writ&apos;, &apos;ii&apos;, &apos;rhet&apos;, &apos;compos&apos;, &apos;rhet&apos;, &apos;compos&apos;, &apos;eng&apos;, &apos;sery&apos;, &apos;interact&apos;, &apos;read&apos;, &apos;research&apos;, &apos;compos&apos;, &apos;act&apos;, &apos;along&apos;, &apos;assign&apos;, &apos;design&apos;, &apos;help&apos;, &apos;becom&apos;, &apos;effect&apos;, &apos;consum&apos;, &apos;produc&apos;, &apos;alphabet&apos;, &apos;vis&apos;, &apos;multimod&apos;, &apos;texts.&apos;, &apos;join&apos;, &apos;us&apos;, &apos;becom&apos;, &apos;effect&apos;, &apos;writ&apos;, &apos;...&apos;, &apos;bet&apos;, &apos;citizens.&apos;, &apos;rhet&apos;, &apos;compos&apos;, &apos;cours&apos;, &apos;writ&apos;, &apos;exchang&apos;, &apos;word&apos;, &apos;idea&apos;, &apos;tal&apos;, &apos;support.&apos;, &apos;introduc&apos;, &apos;vary&apos;, &apos;rhet&apos;, &apos;concepts\xe2\x80\x94that&apos;, &apos;idea&apos;, &apos;techn&apos;, &apos;inform&apos;, &apos;persuad&apos;, &apos;audiences\xe2\x80\x94that&apos;, &apos;help&apos;, &apos;becom&apos;, &apos;effect&apos;, &apos;consum&apos;, &apos;produc&apos;, &apos;writ&apos;, &apos;vis&apos;, &apos;multimod&apos;, &apos;texts.&apos;, &apos;class&apos;, &apos;includ&apos;, &apos;short&apos;, &apos;video&apos;, &apos;demonst&apos;, &apos;activities.&apos;, &apos;envid&apos;, &apos;rhet&apos;, &apos;compos&apos;, &apos;learn&apos;, &apos;commun&apos;, &apos;includ&apos;, &apos;enrol&apos;, &apos;cours&apos;, &apos;instructors.&apos;, &apos;bring&apos;, &apos;expert&apos;, &apos;writ&apos;, &apos;rhet&apos;, &apos;cours&apos;, &apos;design&apos;, &apos;design&apos;, &apos;assign&apos;, &apos;cours&apos;, &apos;infrastruct&apos;, &apos;help&apos;, &apos;shar&apos;, &apos;expery&apos;, &apos;writ&apos;, &apos;stud&apos;, &apos;profess&apos;, &apos;us.&apos;, &apos;collab&apos;, &apos;facilit&apos;, &apos;wex&apos;, &apos;writ&apos;, &apos;exchang&apos;, &apos;plac&apos;, &apos;exchang&apos;, &apos;work&apos;, &apos;feedback&apos;] 在我们引入gensim之前，还有一件事要做，去掉在整个语料库中出现次数为1的低频词，测试了一下，不去掉的话对效果有些影响： all_stems = sum(texts_stemmed, []) stems_once = set(stem for stem in set(all_stems) if all_stems.count(stem) == texts = [[stem for stem in text if stem not in stems_once] for text in texts_stemmed] 3、引入gensim有了上述的预处理，我们就可以引入gensim，并快速的做课程相似度的实验了。以下会快速的过一遍流程，具体的可以参考上一节的详细描述。 from gensim import corpora, models, similarities import logging logging.basicConfig(format=&apos;%(asctime)s : %(levelname)s : %(message)s&apos;, level=logging.INFO) dictionary = corpora.Dictionary(texts) corpus = [dictionary.doc2bow(text) for text in texts] tfidf = models.TfidfModel(corpus) corpus_tfidf = tfidf[corpus] #这里我训练topic数量为10的LSI模型： lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10) index = similarities.MatrixSimilarity(lsi[corpus]) 基于LSI模型的课程索引建立完毕，我们以Andrew Ng教授的机器学习公开课为例，这门课程在我们的coursera_corpus文件的第211行，也就是： print courses_name[210] #Machine Learning 现在我们就可以通过lsi模型将这门课程映射到10个topic主题模型空间上，然后和其他课程计算相似度： ml_course = texts[210] ml_bow = dicionary.doc2bow(ml_course) ml_lsi = lsi[ml_bow] print ml_lsi #[(0, 8.3270084238788673), (1, 0.91295652151975082), (2, -0.28296075112669405), (3, 0.0011599008827843801), (4, -4.1820134980024255), (5, -0.37889856481054851), (6, 2.0446999575052125), (7, 2.3297944485200031), (8, -0.32875594265388536), (9, -0.30389668455507612)] sims = index[ml_lsi] sort_sims = sorted(enumerate(sims), key=lambda item: -item[1]) 取按相似度排序的前10门课程： print sort_sims[0:10] #[(210, 1.0), (174, 0.97812241), (238, 0.96428639), (203, 0.96283489), (63, 0.9605484), (189, 0.95390636), (141, 0.94975704), (184, 0.94269753), (111, 0.93654782), (236, 0.93601125)] 第一门课程是它自己: print courses_name[210] #Machine Learning 第二门课是Coursera上另一位大牛Pedro Domingos机器学习公开课 print courses_name[174] #Machine Learning 第三门课是Coursera的另一位创始人，同样是大牛的Daphne Koller教授的概率图模型公开课： print courses_name[238] #Probabilistic Graphical Models 第四门课是另一位超级大牛Geoffrey Hinton的神经网络公开课，有同学评价是Deep Learning的必修课。 print courses_name[203] #Neural Networks for Machine Learning]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSA</tag>
        <tag>LAI</tag>
        <tag>文本相似度</tag>
        <tag>主题模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些机器学习算法]]></title>
    <url>%2F2017%2F05%2F29%2F%E4%B8%80%E4%BA%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯假设对于某个数据集，随机变量C表示样本为C类的概率，F1表示测试样本某特征出现的概率，套用基本贝叶斯公式，则如下所示： 上式表示对于某个样本，特征F1出现时，该样本被分为C类的条件概率。那么如何用上式来对测试样本分类呢？ 举例来说，有个测试样本，其特征F1出现了（F1=1），那么就计算P(C=0|F1=1)和P(C=1|F1=1)的概率值。前者大，则该样本被认为是0类；后者大，则分为1类。 对该公示，有几个概念需要熟知： 先验概率（Prior）:P(C)是C的先验概率，可以从已有的训练集中计算分为C类的样本占所有样本的比重得出。 证据（Evidence）:即上式P(F1)，表示对于某测试样本，特征F1出现的概率。同样可以从训练集中F1特征对应样本所占总样本的比例得出。 似然（likelihood）:即上式P(F1|C)，表示如果知道一个样本分为C类，那么他的特征为F1的概率是多少。 对于多个特征而言，贝叶斯公式可以扩展如下： 分子中存在一大串似然值。当特征很多的时候，这些似然值的计算是极其痛苦的。现在该怎么办？ 为了简化计算，朴素贝叶斯算法做了一假设：“朴素的认为各个特征相互独立”。这么一来，上式的分子就简化成了： P(C)P(F1|C)P(F2|C)…P(Fn|C)。 这样简化过后，计算起来就方便多了。 这个假设是认为各个特征之间是独立的，看上去确实是个很不科学的假设。因为很多情况下，各个特征之间是紧密联系的。然而在朴素贝叶斯的大量应用实践实际表明其工作的相当好。 其次，由于朴素贝叶斯的工作原理是计算P(C=0|F1…Fn)和P(C=1|F1…Fn)，并取最大值的那个作为其分类。而二者的分母是一模一样的。因此，我们又可以省略分母计算，从而进一步简化计算过程。 另外，贝叶斯公式推导能够成立有个重要前期，就是各个证据（evidence）不能为0。也即对于任意特征Fx，P(Fx)不能为0。而显示某些特征未出现在测试集中的情况是可以发生的。因此实现上通常要做一些小的处理，例如把所有计数进行+1（加法平滑(additive smoothing，又叫拉普拉斯平滑(Laplace smothing)）。而如果通过增加一个大于0的可调参数alpha进行平滑，就叫Lidstone平滑。 例如，在所有6个分为C=1的影评样本中，某个特征F1=1不存在，则P(F1=1|C=1) = 0/6，P(F1=0|C=1) = 6/6。 经过加法平滑后，P(F1=1|C=1) = (0+1)/(6+2)=1/8，P(F1=0|C=1) = (6+1)/(6+2)=7/8。 注意分母的+2，这种特殊处理使得2个互斥事件的概率和恒为1。 最后，我们知道，当特征很多的时候，大量小数值的小数乘法会有溢出风险。因此，通常的实现都是将其转换为log： log[P(C)P(F1|C)P(F2|C)…P(Fn|C)] = log[P(C)]+log[P(F1|C)] + … +log[P(Fn|C)] 将乘法转换为加法，就彻底避免了乘法溢出风险。 实例说明：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 # -*- coding: utf-8 -*- # data from：http://download.csdn.net/detail/lsldd/9346233 from matplotlib import pyplotimport scipy as spimport numpy as npfrom sklearn.datasets import load_filesfrom sklearn.cross_validation import train_test_splitfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.metrics import precision_recall_curvefrom sklearn.metrics import classification_report&apos;&apos;&apos;movie_reviews = load_files(&apos;data&apos;)#保存sp.save(&apos;movie_data.npy&apos;, movie_reviews.data)sp.save(&apos;movie_target.npy&apos;, movie_reviews.target)&apos;&apos;&apos;#读取movie_data = sp.load(&apos;movie_data.npy&apos;)movie_target = sp.load(&apos;movie_target.npy&apos;)x = movie_datay = movie_target#BOOL型特征下的向量空间模型，注意，测试样本调用的是transform接口count_vec = TfidfVectorizer(binary = False, decode_error = &apos;ignore&apos;,\ stop_words = &apos;english&apos;)#加载数据集，切分数据集80%训练，20%测试x_train, x_test, y_train, y_test\ = train_test_split(movie_data, movie_target, test_size = 0.2)x_train = count_vec.fit_transform(x_train)x_test = count_vec.transform(x_test)#调用MultinomialNB分类器clf = MultinomialNB().fit(x_train, y_train)doc_class_predicted = clf.predict(x_test) #print(doc_class_predicted)#print(y)print(np.mean(doc_class_predicted == y_test))#准确率与召回率precision, recall, thresholds = precision_recall_curve(y_test, clf.predict(x_test))answer = clf.predict_proba(x_test)[:,1]report = answer &gt; 0.5print(classification_report(y_test, report, target_names = [&apos;neg&apos;, &apos;pos&apos;])) 要注意的是选用的朴素贝叶斯分类器类别：MultinomialNB，这个分类器以出现次数作为特征值，使用的TF-IDF也能符合这类分布。 其他的朴素贝叶斯分类器如GaussianNB适用于高斯分布（正态分布）的特征，而BernoulliNB适用于伯努利分布（二值分布）的特征。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow之神经网络(二)]]></title>
    <url>%2F2017%2F05%2F27%2Ftensorflow%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[添加层 def add_layer()在 Tensorflow 里定义一个添加层的函数可以很容易的添加神经层,为之后的添加省下不少时间. 神经层里常见的参数通常有weights、biases和激励函数。 定义添加神经层的函数 $def add_layer()$ ,它有四个参数：输入值、输入的大小、输出的大小和激励函数，我们设定默认的激励函数是 $None$1def add_layer(inputs, in_size, out_size, activation_function=None): 接下来定义 $weights$ 和 $biases$1Weights = tf.Variable(tf.random_normal([in_size, out_size])) 在机器学习中，biases的推荐值不为0，这里是在0向量的基础上又加了0.1。1biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) 下面定义Wx_plus_b, 即神经网络未激活的值。其中，tf.matmul()是矩阵的乘法。1Wx_plus_b = tf.matmul(inputs, Weights) + biases 当 $activation_function$ ——激励函数为 $None$ 时，输出就是当前的预测值—— $Wx_plus_b$ , 不为 $None$ 时，就把$Wx_plus_b$ 传到 $activation_function()$ 函数中得到输出。1234if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) 最后，返回输出，添加一个神经层的函数—— $def add_layer()$ 就定义好了。1return outputs 完整代码：12345678910111213from __future__ import print_functionimport tensorflow as tfdef add_layer(inputs, in_size, out_size, activation_function=None): Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs 构造神经网络构造添加一个神经层的函数。12345678910111213from __future__ import print_functionimport tensorflow as tfdef add_layer(inputs, in_size, out_size, activation_function=None): Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs 构建所需的数据。 这里的 $x_data$ 和 $y_data$ 并不是严格的一元二次函数的关系，因为我们多加了一个 $noise$ ,这样看起来会更像真实情况。123x_data = np.linspace(-1,1,300, dtype=np.float32)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape).astype(np.float32)y_data = np.square(x_data) - 0.5 + noise 利用占位符定义我们所需的神经网络的输入。 $tf.placeholder()$ 就是代表占位符，这里的 $None$ 代表无论输入有多少都可以，因为输入只有一个特征，所以这里是1。12xs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1]) 接下来，就可以开始定义神经层了。 通常神经层都包括输入层、隐藏层和输出层。这里的输入层只有一个属性， 所以我们就只有一个输入；隐藏层我们可以自己假设，这里我们假设隐藏层有10个神经元； 输出层和输入层的结构是一样的，所以我们的输出层也是只有一层。 构建的是——输入层1个、隐藏层10个、输出层1个的神经网络。 下面，开始定义隐藏层,利用之前的 $add_layer()$ 函数，这里使用 Tensorflow 自带的激励函数 $tf.nn.relu$.1l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu) 接着，定义输出层。此时的输入就是隐藏层的输出——l1，输入有10层（隐藏层的输出层），输出有1层.1prediction = add_layer(l1, 10, 1, activation_function=None) 计算预测值prediction和真实值的误差，对二者差的平方求和再取平均.12loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1])) 接下来，是很关键的一步，如何让机器学习提升它的准确率。$tf.train.GradientDescentOptimizer()$ 中的值通常都小于1，这里取的是0.1，代表以0.1的效率来最小化误差loss.1train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss) 使用变量时，都要对它进行初始化，这是必不可少的。12# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法init = tf.global_variables_initializer() # 替换成这样就好 定义Session，并用 Session 来执行 init 初始化步骤。 （注意：在tensorflow中，只有 $session.run()$ 才会执行我们定义的运算。）12sess = tf.Session()sess.run(init) 下面，让机器开始学习。 比如这里让机器学习1000次。机器学习的内容是$train_step$ , 用 $Session$ 来 $run$ 每一次 $training$ 的数据，逐步提升神经网络的预测准确性。 (注意：当运算要用到$placeholder$ 时，就需要 $feed_dict$ 这个字典来指定输入。)123for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) 每50步我们输出一下机器学习的误差.123if i % 50 == 0: # to see the step improvement print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;)) 完整代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071from __future__ import print_functionimport tensorflow as tfimport numpy as npdef add_layer(inputs, in_size, out_size, activation_function=None): # add one more layer and return the output of this layer Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs# Make up some real datax_data = np.linspace(-1,1,300)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape)y_data = np.square(x_data) - 0.5 + noise# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1])# add hidden layerl1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)# add output layerprediction = add_layer(l1, 10, 1, activation_function=None)# the error between prediction and real dataloss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)# important step# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split(&apos;.&apos;)[1]) &lt; 12: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) if i % 50 == 0: # to see the step improvement print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;))# Outputs:误差在逐渐减小说明机器学习是有积极的效果的0.7670260.01291250.006911560.004450190.003976470.00374810.003877550.01682780.007168920.005184170.006679610.007396850.006269430.006096910.006333020.006363780.005784780.005543110.00595350.00645603 结果可视化构建图形，用散点图描述真实数据之间的关系。 （注意：plt.ion()用于连续显示。）构建图形，用散点图描述真实数据之间的关系。 （注意：$plt.ion()$ 用于连续显示。）123456# plot the real datafig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(x_data, y_data)plt.ion()#本次运行请注释，全局运行不要注释plt.show() 接下来显示预测数据。每隔50次训练刷新一次图形，用红色、宽度为5的线来显示我们的预测数据和输入之间的关系，并暂停0.1s。12345678910111213for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) if i % 50 == 0: # to visualize the result and improvement try: ax.lines.remove(lines[0]) except Exception: pass prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;) # plot the prediction lines = ax.plot(x_data, prediction_value, &apos;r-&apos;, lw=5) plt.pause(0.1) 最后，机器学习的结果为： 完整代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from __future__ import print_functionimport tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltdef add_layer(inputs, in_size, out_size, activation_function=None): Weights = tf.Variable(tf.random_normal([in_size, out_size])) biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) Wx_plus_b = tf.matmul(inputs, Weights) + biases if activation_function is None: outputs = Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs# Make up some real datax_data = np.linspace(-1, 1, 300)[:, np.newaxis]noise = np.random.normal(0, 0.05, x_data.shape)y_data = np.square(x_data) - 0.5 + noise##plt.scatter(x_data, y_data)##plt.show()# define placeholder for inputs to networkxs = tf.placeholder(tf.float32, [None, 1])ys = tf.placeholder(tf.float32, [None, 1])# add hidden layerl1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)# add output layerprediction = add_layer(l1, 10, 1, activation_function=None)# the error between prediction and real dataloss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction), reduction_indices=[1]))train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)# important stepsess = tf.Session()# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split(&apos;.&apos;)[1]) &lt; 12 and int((tf.__version__).split(&apos;.&apos;)[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess.run(init)# plot the real datafig = plt.figure()ax = fig.add_subplot(1,1,1)ax.scatter(x_data, y_data)plt.ion()plt.show()for i in range(1000): # training sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;) if i % 50 == 0: # to visualize the result and improvement try: ax.lines.remove(lines[0]) except Exception: pass prediction_value = sess.run(prediction, feed_dict=&#123;xs: x_data&#125;) # plot the prediction lines = ax.plot(x_data, prediction_value, &apos;r-&apos;, lw=5) plt.pause(1) 优化器包括以下几种模式:●Stochastic Gradient Descent (SGD)●Momentum●AdaGrad●RMSProp●Adam越复杂的神经网络 , 越多的数据 , 需要在训练神经网络的过程上花费的时间也就越多. 原因很简单, 就是因为计算量太大了. 可是往往有时候为了解决复杂的问题, 复杂的结构和大数据又是不能避免的, 所以我们需要寻找一些方法, 让神经网络聪明起来, 快起来. Stochastic Gradient Descent (SGD)所以, 最基础的方法就是 SGD 啦, 想像红色方块是我们要训练的 data, 如果用普通的训练方法, 就需要重复不断的把整套数据放入神经网络 NN训练, 这样消耗的计算资源会很大. 换一种思路, 如果把这些数据拆分成小批小批的, 然后再分批不断放入 NN 中计算, 这就是常说的 SGD 的正确打开方式了. 每次使用批数据, 虽然不能反映整体数据的情况, 不过却很大程度上加速了 NN 的训练过程, 而且也不会丢失太多准确率.如果运用上了 SGD, 你还是嫌训练速度慢, 那怎么办?没问题, 事实证明, SGD 并不是最快速的训练方法, 红色的线是 SGD, 但它到达学习目标的时间是在这些方法中最长的一种. 我们还有很多其他的途径来加速训练. Momentum 更新方法大多数其他途径是在更新神经网络参数那一步上动动手脚. 传统的参数 W 的更新是把原始的 W 累加上一个负的学习率(learning rate) 乘以校正值 (dx). 这种方法可能会让学习过程曲折无比, 看起来像 喝醉的人回家时, 摇摇晃晃走了很多弯路.所以我们把这个人从平地上放到了一个斜坡上, 只要他往下坡的方向走一点点, 由于向下的惯性, 他不自觉地就一直往下走, 走的弯路也变少了. 这就是 Momentum 参数更新. 另外一种加速方法叫AdaGrad. AdaGrad 更新方法这种方法是在学习率上面动手脚, 使得每一个参数更新都会有自己与众不同的学习率, 他的作用和 momentum 类似, 不过不是给喝醉酒的人安排另一个下坡, 而是给他一双不好走路的鞋子, 使得他一摇晃着走路就脚疼, 鞋子成为了走弯路的阻力, 逼着他往前直着走. 他的数学形式是这样的. 接下来又有什么方法呢? 如果把下坡和不好走路的鞋子合并起来, 是不是更好呢? 没错, 这样我们就有了 RMSProp 更新方法. RMSProp 更新方法有了 momentum 的惯性原则 , 加上 adagrad 的对错误方向的阻力, 我们就能合并成这样. 让 RMSProp同时具备他们两种方法的优势. 不过细心的同学们肯定看出来了, 似乎在 RMSProp 中少了些什么. 原来是我们还没把 Momentum合并完全, RMSProp 还缺少了 momentum 中的 这一部分. 所以, 我们在 Adam 方法中补上了这种想法. Adam 更新方法计算m 时有 momentum 下坡的属性, 计算 v 时有 adagrad 阻力的属性, 然后再更新参数时 把 m 和 V 都考虑进去. 实验证明, 大多数时候, 使用 adam 都能又快又好的达到目标, 迅速收敛. 所以说, 在加速神经网络训练的时候, 一个下坡, 一双破鞋子, 功不可没.]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN</tag>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow之神经网络(一)]]></title>
    <url>%2F2017%2F05%2F27%2Ftensorflow%E4%B9%8B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%80%2F</url>
    <content type="text"><![CDATA[基本使用使用 TensorFlow, 你必须明白 TensorFlow:●使用图 (graph) 来表示计算任务.●在被称之为 会话$(Session)$ 的上下文 (context) 中执行图.●使用 tensor 表示数据.●通过 变量 $(Variable)$ 维护状态.●使用 feed 和 fetch 可以为任意的操作 (arbitrary operation) 赋值或者从其中获取数据. 综述TensorFlow 是一个编程系统, 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op 获得 0 个或多个 $Tensor$, 执行计算, 产生 0 个或多个 $Tensor$. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 $[batch, height, width, channels]$. 一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 $会话$ 里被启动. $会话$ 将图的 op 分发到诸如 CPU 或 GPU 之类的 $设备$ 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 $numpy ndarray$ 对象; 在 C 和 C++ 语言中, 返回的 tensor 是 $tensorflow::Tensor$ 实例. 计算图TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤 被描述成一个图. 在执行阶段, 使用会话执行执行图中的 op. 例如, 通常在构建阶段创建一个图来表示和训练神经网络, 然后在执行阶段反复执行图中的训练 op. TensorFlow 支持 C, C++, Python 编程语言. 目前, TensorFlow 的 Python 库更加易用, 它提供了大量的辅助函数来简化构建图的工作, 这些函数尚未被 C 和 C++ 库支持. 三种语言的会话库 (session libraries) 是一致的. 构建图构建图的第一步, 是创建源 op (source op). 源 op 不需要任何输入, 例如 $常量 (Constant)$. 源 op 的输出被传递给其它 op 做运算. Python 库中, op 构造器的返回值代表被构造出的 op 的输出, 这些返回值可以传递给其它 op 构造器作为输入. TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点. 这个默认图对 许多程序来说已经足够用了.1234567891011121314import tensorflow as tf# 创建一个常量 op, 产生一个 1x2 矩阵. 这个 op 被作为一个节点# 加到默认图中.## 构造器的返回值代表该常量 op 的返回值.matrix1 = tf.constant([[3., 3.]])# 创建另外一个常量 op, 产生一个 2x1 矩阵.matrix2 = tf.constant([[2.],[2.]])# 创建一个矩阵乘法 matmul op , 把 &apos;matrix1&apos; 和 &apos;matrix2&apos; 作为输入.# 返回值 &apos;product&apos; 代表矩阵乘法的结果.product = tf.matmul(matrix1, matrix2) 默认图现在有三个节点, 两个 constant() op, 和一个matmul() op. 为了真正进行矩阵相乘运算, 并得到矩阵乘法的 结果, 你必须在会话里启动这个图. 在一个会话中启动图构造阶段完成后, 才能启动图. 启动图的第一步是创建一个 $Session$ 对象, 如果无任何创建参数, 会话构造器将启动默认图.123456789101112131415161718# 启动默认图.sess = tf.Session()# 调用 sess 的 &apos;run()&apos; 方法来执行矩阵乘法 op, 传入 &apos;product&apos; 作为该方法的参数. # 上面提到, &apos;product&apos; 代表了矩阵乘法 op 的输出, 传入它是向方法表明, 我们希望取回# 矩阵乘法 op 的输出.## 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的.# # 函数调用 &apos;run(product)&apos; 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行.## 返回值 &apos;result&apos; 是一个 numpy `ndarray` 对象.result = sess.run(product)print result# ==&gt; [[ 12.]]# 任务完成, 关闭会话.sess.close() $Session$ 对象在使用完后需要关闭以释放资源. 除了显式调用 close 外, 也可以使用 “with” 代码块 来自动完成关闭动作.123with tf.Session() as sess: result = sess.run([product]) print result 在实现上, TensorFlow 将图形定义转换成分布式执行的操作, 以充分利用可用的计算资源(如 CPU 或 GPU). 一般你不需要显式指定使用 CPU 还是 GPU, TensorFlow 能自动检测. 如果检测到 GPU, TensorFlow 会尽可能地利用找到的第一个 GPU 来执行操作. 如果机器上有超过一个可用的 GPU, 除第一个外的其它 GPU 默认是不参与计算的. 为了让 TensorFlow 使用这些 GPU, 你必须将 op 明确指派给它们执行. $with…Device$ 语句用来指派特定的 CPU 或 GPU 执行操作:123456with tf.Session() as sess: with tf.device(&quot;/gpu:1&quot;): matrix1 = tf.constant([[3., 3.]]) matrix2 = tf.constant([[2.],[2.]]) product = tf.matmul(matrix1, matrix2) ... 设备用字符串进行标识. 目前支持的设备包括:●”/cpu:0”: 机器的 CPU.●”/gpu:0”: 机器的第一个 GPU, 如果有的话.●”/gpu:1”: 机器的第二个 GPU, 以此类推. 交互式使用文档中的 Python 示例使用一个会话 $Session$ 来 启动图, 并调用 $Session.run()$ 方法执行操作. 为了便于使用诸如 $IPython$ 之类的 Python 交互环境, 可以使用 $InteractiveSession$ 代替 $Session$ 类, 使用 $Tensor.eval()$ 和 $Operation.run()$ 方法代替 $Session.run()$. 这样可以避免使用一个变量来持有会话.1234567891011121314# 进入一个交互式 TensorFlow 会话.import tensorflow as tfsess = tf.InteractiveSession()x = tf.Variable([1.0, 2.0])a = tf.constant([3.0, 3.0])# 使用初始化器 initializer op 的 run() 方法初始化 &apos;x&apos; x.initializer.run()# 增加一个减法 sub op, 从 &apos;x&apos; 减去 &apos;a&apos;. 运行减法 op, 输出结果 sub = tf.sub(x, a)print sub.eval()# ==&gt; [-2. -1.] TensorTensorFlow 程序使用 tensor 数据结构来代表所有的数据, 计算图中, 操作间传递的数据都是 tensor. 你可以把 TensorFlow tensor 看作是一个 n 维的数组或列表. 一个 tensor 包含一个静态类型 rank, 和 一个 shape.张量有多种. 零阶张量为 纯量或标量 (scalar) 也就是一个数值. 比如 [1]●一阶张量为 向量 (vector), 比如 一维的 [1, 2, 3]●二阶张量为 矩阵 (matrix), 比如 二维的 [[1, 2, 3],[4, 5, 6],[7, 8, 9]]●以此类推, 还有 三阶 三维的 … VariableVariables for more details. 变量维护图执行过程中的状态信息. 下面的例子演示了如何使用变量实现一个简单的计数器.1234567891011121314151617181920212223242526272829# 创建一个变量, 初始化为标量 0.state = tf.Variable(0, name=&quot;counter&quot;)# 创建一个 op, 其作用是使 state 增加 1one = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value)# 启动图后, 变量必须先经过`初始化` (init) op 初始化,# 首先必须增加一个`初始化` op 到图中.init_op = tf.initialize_all_variables()# 启动图, 运行 opwith tf.Session() as sess: # 运行 &apos;init&apos; op sess.run(init_op) # 打印 &apos;state&apos; 的初始值 print sess.run(state) # 运行 op, 更新 &apos;state&apos;, 并打印 &apos;state&apos; for _ in range(3): sess.run(update) print sess.run(state)# 输出:# 0# 1# 2# 3 代码中 $assign()$ 操作是图所描绘的表达式的一部分, 正如 $add()$ 操作一样. 所以在调用 $run()$ 执行表达式之前, 它并不会真正执行赋值操作. 通常会将一个统计模型中的参数表示为一组变量. 例如, 你可以将一个神经网络的权重作为某个变量存储在一个 tensor 中. 在训练过程中, 通过重复运行训练图, 更新这个 tensor. PlaceholderTensorflow 中的 $placeholder$ , $placeholder$ 是 Tensorflow 中的占位符，暂时储存变量. Tensorflow 如果想要从外部传入data, 那就需要用到 $tf.placeholder()$, 然后以这种形式传输数据 $sess.run(*, feed_dict={input: })$.12345678import tensorflow as tf#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)# mul = multiply 是将input1和input2 做乘法运算，并输出为 output ouput = tf.multiply(input1, input2) Fetch为了取回操作的输出内容, 可以在使用 $Session$ 对象的 $run()$ 调用 执行图时, 传入一些 tensor, 这些 tensor 会帮助你取回结果. 在之前的例子里, 我们只取回了单个节点 $state$, 但是你也可以取回多个 tensor:123456789101112input1 = tf.constant(3.0)input2 = tf.constant(2.0)input3 = tf.constant(5.0)intermed = tf.add(input2, input3)mul = tf.mul(input1, intermed)with tf.Session() as sess: result = sess.run([mul, intermed]) print result# 输出:# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)] 需要获取的多个 tensor 值，在 op 的一次运行中一起获得(而不是逐个去获取 tensor). 接下来, 传值的工作交给了 $sess.run()$ , 需要传入的值放在了 $feed_dict={}$ 并一一对应每一个 $input$. $placeholder$ 与 $feed_dict={}$ 是绑定在一起出现的。123with tf.Session() as sess: print(sess.run(ouput, feed_dict=&#123;input1: [7.], input2: [2.]&#125;))# [ 14.] Feed上述示例在计算图中引入了 tensor, 以常量或变量的形式存储. TensorFlow 还提供了 feed 机制, 该机制 可以临时替代图中的任意操作中的 tensor 可以对图中任何操作提交补丁, 直接插入一个 tensor. feed 使用一个 tensor 值临时替换一个操作的输出结果. 你可以提供 feed 数据作为 $run()$ 调用的参数. feed 只在调用它的方法内有效, 方法结束, feed 就会消失. 最常见的用例是将某些特殊的操作指定为 “feed” 操作, 标记的方法是使用 tf.placeholder() 为这些操作创建占位符.123456789input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.mul(input1, input2)with tf.Session() as sess: print sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;)# 输出:# [array([ 14.], dtype=float32)] for a larger-scale example of feeds. 如果没有正确提供 feed, $placeholder()$ 操作将会产生错误. 例子简单的阐述了 tensorflow 当中如何用代码来运行搭建的结构.123456import tensorflow as tfimport numpy as np# create datax_data = np.random.rand(100).astype(np.float32)y_data = x_data*0.1 + 0.3 接着, 我们用 $tf.Variable$ 来创建描述 $y$ 的参数. 我们可以把 y_data = x_data0.1 + 0.3 想象成 y=Weights x + biases, 然后神经网络也就是学着把 Weights 变成 0.1, biases 变成 0.3.1234Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))biases = tf.Variable(tf.zeros([1]))y = Weights*x_data + biases 接着就是计算 y 和 y_data 的误差:1loss = tf.reduce_mean(tf.square(y-y_data)) 反向传递误差的工作就教给$optimizer$了, 我们使用的误差传递方法是梯度下降法: $Gradient Descent$ 让后我们使用 $optimizer$ 来进行参数的更新.12optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss) 到目前为止, 我们只是建立了神经网络的结构, 还没有使用这个结构. 在使用这个结构之前, 必须先初始化所有之前定义的$Variable$, 所以这一步是很重要的.12# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法init = tf.global_variables_initializer() # 替换成这样就好 接着,我们再创建会话 $Session$. 我们会在下一节中详细讲解 $Session$. 我们用 $Session$ 来执行 $init$ 初始化步骤. 并且, 用 $Session$ 来 $run$ 每一次 training 的数据. 逐步提升神经网络的预测准确性.1234567sess = tf.Session()sess.run(init) # Very importantfor step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases)) 完整代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from __future__ import print_functionimport tensorflow as tfimport numpy as np# create datax_data = np.random.rand(100).astype(np.float32)y_data = x_data*0.1 + 0.3### create tensorflow structure start ###Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))biases = tf.Variable(tf.zeros([1]))y = Weights*x_data + biasesloss = tf.reduce_mean(tf.square(y-y_data))optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)init = tf.initialize_all_variables()### create tensorflow structure end ###sess = tf.Session()# tf.initialize_all_variables() no long valid from# 2017-03-02 if using tensorflow &gt;= 0.12if int((tf.__version__).split(&apos;.&apos;)[1]) &lt; 12 and int((tf.__version__).split(&apos;.&apos;)[0]) &lt; 1: init = tf.initialize_all_variables()else: init = tf.global_variables_initializer()sess.run(init)for step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases))#Outputs：0 [ 0.79606164] [-0.11589886]20 [ 0.27680936] [ 0.20445168]40 [ 0.14386448] [ 0.27629551]60 [ 0.11088227] [ 0.29411921]80 [ 0.10269976] [ 0.29854104]100 [ 0.10066979] [ 0.29963806]120 [ 0.10016618] [ 0.29991022]140 [ 0.10004124] [ 0.29997772]160 [ 0.10001024] [ 0.29999447]180 [ 0.10000254] [ 0.29999864]200 [ 0.10000063] [ 0.29999968] 激活函数激励函数运行时激活神经网络中某一部分神经元，将激活信息向后传入下一层的神经系统。激励函数的实质是非线性方程。 Tensorflow 的神经网络 里面处理较为复杂的问题时都会需要运用激励函数 $activation function$. 这里的 AF 就是指的激励函数. 激励函数拿出自己最擅长的”掰弯利器”, 套在了原函数上 用力一扭, 原来的 Wx 结果就被扭弯了. 其实这个 AF, 掰弯利器, 也不是什么触不可及的东西. 它其实就是另外一个非线性函数. 比如说relu, sigmoid, tanh. 将这些掰弯利器嵌套在原有的结果之上, 强行把原有的线性结果给扭曲了. 使得输出结果 y 也有了非线性的特征. 举个例子, 比如我使用了 relu 这个掰弯利器, 如果此时 Wx 的结果是1, y 还将是1, 不过 Wx 为-1的时候, y 不再是-1, 而会是0. 你甚至可以创造自己的激励函数来处理自己的问题, 不过要确保的是这些激励函数必须是可以微分的, 因为在 backpropagation 误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去. 想要恰当使用这些激励函数, 还是有窍门的. 比如当你的神经网络层只有两三层, 不是很多的时候, 对于隐藏层, 使用任意的激励函数, 随便掰弯是可以的, 不会有特别大的影响. 不过, 当你使用特别多层的神经网络, 在掰弯的时候, 玩玩不得随意选择利器. 因为这会涉及到梯度爆炸, 梯度消失的问题. 最后我们说说, 在具体的例子中, 默认首选的激励函数是哪些. 在少量层结构中, 我们可以尝试很多种不同的激励函数. 在卷积神经网络 Convolutional neural networks 的卷积层中, 推荐的激励函数是 relu. 在循环神经网络中 recurrent neural networks, 推荐的是 tanh 或者是 relu.]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>CNN</tag>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn学习笔记]]></title>
    <url>%2F2017%2F05%2F27%2Fsklearn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[官网：http://sklearn.lzjqsdd.com/ 1 scikit-learn基础介绍1.1 估计器（Estimator）估计器，很多时候可以直接理解成分类器，主要包含两个函数： ●fit()：训练算法，设置内部参数。接收训练集和类别两个参数。●predict()：预测测试集类别，参数为测试集。大多数scikit-learn估计器接收和输出的数据格式均为numpy数组或类似格式。 1.2 转换器（Transformer）转换器用于数据预处理和数据转换，主要是三个方法： ●fit()：训练算法，设置内部参数。●transform()：数据转换。●fit_transform()：合并fit和transform两个方法。 1.3 流水线（Pipeline）sklearn.pipeline包流水线的功能：●跟踪记录各步骤的操作（以方便地重现实验结果）●对各步骤进行一个封装●确保代码的复杂程度不至于超出掌控范围 基本使用方法 流水线的输入为一连串的数据挖掘步骤，其中最后一步必须是估计器，前几步是转换器。输入的数据集经过转换器的处理后，输出的结果作为下一步的输入。最后，用位于流水线最后一步的估计器对数据进行分类。每一步都用元组（ ‘名称’，步骤）来表示。现在来创建流水线。1234scaling_pipeline = Pipeline([ (&apos;scale&apos;, MinMaxScaler()), (&apos;predict&apos;, KNeighborsClassifier())]) 1.4 预处理主要在sklearn.preprcessing包下。 规范化：●MinMaxScaler :最大最小值规范化●Normalizer :使每条数据各特征值的和为1●StandardScaler :为使各特征的均值为0，方差为1 编码：●LabelEncoder ：把字符串类型的数据转化为整型●OneHotEncoder ：特征用一个二进制数字来表示●Binarizer :为将数值型特征的二值化●MultiLabelBinarizer：多标签二值化 1.5 特征1.5.1 特征抽取包：sklearn.feature_extraction特征抽取是数据挖掘任务最为重要的一个环节，一般而言，它对最终结果的影响要高过数据挖掘算法本身。只有先把现实用特征表示出来，才能借助数据挖掘的力量找到问题的答案。特征选择的另一个优点在于：降低真实世界的复杂度，模型比现实更容易操纵。一般最常使用的特征抽取技术都是高度针对具体领域的，对于特定的领域，如图像处理，在过去一段时间已经开发了各种特征抽取的技术，但这些技术在其他领域的应用却非常有限。 ●DictVectorizer： 将dict类型的list数据，转换成numpy array●FeatureHasher ： 特征哈希，相当于一种降维技巧●image：图像相关的特征抽取●text： 文本相关的特征抽取●text.CountVectorizer：将文本转换为每个词出现的个数的向量●text.TfidfVectorizer：将文本转换为tfidf值的向量●text.HashingVectorizer：文本的特征哈希 示例CountVectorize只数出现个数 TfidfVectorizer：个数+归一化（不包括idf）TF-IDF（Term Frequency-Inverse Document Frequency，词频和逆向文件频率）对每个单词做进一步考量。 TF（词频）的计算很简单，就是针对一个文件t，某个单词Nt 出现在该文档中的频率。比如文档“I love this movie”，单词“love”的TF为1/4。如果去掉停用词“I”和”it“，则为1/2。 IDF（逆向文件频率）的意义是，对于某个单词t，凡是出现了该单词的文档数Dt，占了全部测试文档D的比例，再求自然对数。 比如单词“movie“一共出现了5次，而文档总数为12，因此IDF为ln(5/12)。 很显然，IDF是为了凸显那种出现的少，但是占有强烈感情色彩的词语。比如“movie”这样的词的IDF=ln(12/5)=0.88，远小于“love”的IDF=ln(12/1)=2.48。 TF-IDF就是把二者简单的乘在一起即可。这样，求出每个文档中，每个单词的TF-IDF，就是我们提取得到的文本特征值。 sklearn.feature_extraction.DictVectorizer类基于One-Hot Encoding 独热编码提取分类特征；sklearn.feature_extraction.text.CountVectorizer类基于词库模型将文字转换成特征向量；sklearn.feature_extraction.text.TfidfVectorizer类可以统计TF-IDF词频；12345678910111213141516# DictVectorizer()将dict类型的list数据转化成numpy array# fit()：训练算法，设置内部参数# transform()：数据转换# fit_transform()：合并fit和transform两个方法from sklearn.feature_extraction import DictVectorizeronehot_encoder = DictVectorizer()instances = [&#123;&apos;city&apos;: &apos;New York&apos;&#125;,&#123;&apos;city&apos;: &apos;Los Angeles&apos;&#125;, &#123;&apos;city&apos;: &apos;San Antonio&apos;&#125;]print(onehot_encoder.fit_transform(instances).toarray())&apos;&apos;&apos;otu:[[ 0. 1. 0.] [ 1. 0. 0.] [ 0. 0. 1.]]&apos;&apos;&apos; 12345678910111213141516171819# CountVectorizer类通过正则表达式用空格分割句子，然后抽取长度大于等于2的字母序列from sklearn.feature_extraction.text import CountVectorizercorpus = [ &apos;Today the weather is sunny&apos;, &apos;Sunny day weather is suitable to exercise &apos;, &apos;I ate a Hotdog&apos; ] vectorizer = CountVectorizer() print (vectorizer.fit_transform(corpus).todense()) print (vectorizer.vocabulary_) &apos;&apos;&apos;out:[[0 0 0 0 1 0 1 1 0 1 1] [0 1 1 0 1 1 1 0 1 0 1] [1 0 0 1 0 0 0 0 0 0 0]]&#123;u&apos;ate&apos;: 0, u&apos;is&apos;: 4, u&apos;sunny&apos;: 6, u&apos;to&apos;: 8, u&apos;weather&apos;: 10, u&apos;today&apos;: 9, u&apos;the&apos;: 7, u&apos;suitable&apos;: 5, u&apos;day&apos;: 1, u&apos;exercise&apos;: 2, u&apos;hotdog&apos;: 3&#125;&apos;&apos;&apos; 1234567891011121314from sklearn.feature_extraction.text import TfidfVectorizer corpus = [ &apos;The dog ate a sandwich and I ate a sandwich&apos;, &apos;The wizard transfigured a sandwich&apos; ] vectorizer = TfidfVectorizer(stop_words=&apos;english&apos;) print(vectorizer.fit_transform(corpus).todense()) print(vectorizer.vocabulary_) out: [[ 0.75458397 0.37729199 0.53689271 0. 0. ] [ 0. 0. 0.44943642 0.6316672 0.6316672 ]] &#123;&apos;wizard&apos;: 4, &apos;transfigured&apos;: 3, &apos;ate&apos;: 0, &apos;dog&apos;: 1, &apos;sandwich&apos;: 2&#125; #通过TF-IDF加权之后，我们会发现在文集中较常见的词，如sandwich被调整了。 实例说明123456789101112131415161718192021222324252627# -*- coding: utf-8 -*- import scipy as sp import numpy as np from sklearn.datasets import load_files from sklearn.cross_validation import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer movie_reviews = load_files(&apos;data&apos;) doc_terms_train, doc_terms_test, y_train, y_test = train_test_split(movie_reviews.data, movie_reviews.target, test_size = 0.3) count_vec = TfidfVectorizer(binary = False, decode_error = &apos;ignore&apos;,stop_words = &apos;english&apos;) x_train = count_vec.fit_transform(doc_terms_train) x_test = count_vec.transform(doc_terms_test) x = count_vec.transform(movie_reviews.data) y = movie_reviews.target print(doc_terms_train) print(count_vec.get_feature_names()) print(x_train.toarray()) print(movie_reviews.target) &apos;&apos;&apos;词频的计算使用的是sklearn的TfidfVectorizer。这个类继承于CountVectorizer，在后者基本的词频统计基础上增加了如TF-IDF之类的功能。我们会发现这里计算的结果跟我们之前计算不太一样。因为这里count_vec构造时默认传递了max_df=1，因此TF-IDF都做了规格化处理，以便将所有值约束在[0,1]之间。count_vec.fit_transform的结果是一个巨大的矩阵。可以看到上表中有大量的0，因此sklearn在内部实现上使用了稀疏矩阵。本例子数据较小。&apos;&apos;&apos; data地址：http://pan.baidu.com/s/1bpEN00v 1.5.2 特征选择包：sklearn.feature_selection特征选择的原因如下：(1)降低复杂度(2)降低噪音(3)增加模型可读性 ●VarianceThreshold： 删除特征值的方差达不到最低标准的特征●SelectKBest： 返回k个最佳特征●SelectPercentile： 返回表现最佳的前r%个特征单个特征和某一类别之间相关性的计算方法有很多。最常用的有卡方检验（χ2）。其他方法还有互信息和信息熵。 ●chi2： 卡方检验（χ2） 1.6 降维包：sklearn.decomposition ●主成分分析算法（Principal Component Analysis， PCA）的目的是找到能用较少信息描述数据集的特征组合。它意在发现彼此之间没有相关性、能够描述数据集的特征，确切说这些特征的方差跟整体方差没有多大差距，这样的特征也被称为主成分。这也就意味着，借助这种方法，就能通过更少的特征捕获到数据集的大部分信息。 1.7 组合包：sklearn.ensemble组合技术即通过聚集多个分类器的预测来提高分类准确率。常用的组合分类器方法：(1)通过处理训练数据集。即通过某种抽样分布，对原始数据进行再抽样，得到多个训练集。常用的方法有装袋（bagging）和提升（boosting）。(2)通过处理输入特征。即通过选择输入特征的子集形成每个训练集。适用于有大量冗余特征的数据集。随机森林（Random forest）就是一种处理输入特征的组合方法。(3)通过处理类标号。适用于多分类的情况，将类标号随机划分成两个不相交的子集，再把问题变为二分类问题，重复构建多次模型，进行分类投票。 ●BaggingClassifier： Bagging分类器组合●BaggingRegressor： Bagging回归器组合●AdaBoostClassifier： AdaBoost分类器组合●AdaBoostRegressor： AdaBoost回归器组合●GradientBoostingClassifier：GradientBoosting分类器组合●GradientBoostingRegressor： GradientBoosting回归器组合●ExtraTreeClassifier：ExtraTree分类器组合●ExtraTreeRegressor： ExtraTree回归器组合●RandomTreeClassifier：随机森林分类器组合●RandomTreeRegressor： 随机森林回归器组合 使用举例123AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=&quot;SAMME&quot;,n_estimators=200) 解释装袋（bagging）：根据均匀概率分布从数据集中重复抽样（有放回），每个自助样本集和原数据集一样大，每个自助样本集含有原数据集大约63%的数据。训练k个分类器，测试样本被指派到得票最高的类。提升（boosting）：通过给样本设置不同的权值，每轮迭代调整权值。不同的提升算法之间的差别，一般是（1）如何更新样本的权值，（2）如何组合每个分类器的预测。其中Adaboost中，样本权值是增加那些被错误分类的样本的权值，分类器C_i的重要性依赖于它的错误率。Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。 1.8 模型评估（度量）包：sklearn.metricssklearn.metrics包含评分方法、性能度量、成对度量和距离计算。分类结果度量参数大多是y_true和y_pred。 ●accuracy_score：分类准确度●condusion_matrix ：分类混淆矩阵●classification_report：分类报告●precision_recall_fscore_support：计算精确度、召回率、f、支持率●jaccard_similarity_score：计算jcaard相似度●hamming_loss：计算汉明损失●zero_one_loss：0-1损失●hinge_loss：计算hinge损失●log_loss：计算log损失其中，F1是以每个类别为基础进行定义的，包括两个概念：准确率（precision）和召回率（recall）。准确率是指预测结果属于某一类的个体，实际属于该类的比例。召回率是被正确预测为某类的个体，与数据集中该类个体总数的比例。F1是准确率和召回率的调和平均数。 回归结果度量●explained_varicance_score：可解释方差的回归评分函数●mean_absolute_error：平均绝对误差●mean_squared_error：平均平方误差 多标签的度量●coverage_error：涵盖误差●label_ranking_average_precision_score：计算基于排名的平均误差Label ●ranking average precision (LRAP) 聚类的度量●adjusted_mutual_info_score：调整的互信息评分●silhouette_score：所有样本的轮廓系数的平均值●silhouette_sample：所有样本的轮廓系数 1.9 交叉验证包：sklearn.cross_validation ●KFold：K-Fold交叉验证迭代器。接收元素个数、fold数、是否清洗●LeaveOneOut：LeaveOneOut交叉验证迭代器●LeavePOut：LeavePOut交叉验证迭代器●LeaveOneLableOut：LeaveOneLableOut交叉验证迭代器●LeavePLabelOut：LeavePLabelOut交叉验证迭代器 LeaveOneOut(n) 相当于 KFold(n, n_folds=n) 相当于LeavePOut(n, p=1)。LeaveP和LeaveOne差别在于leave的个数，也就是测试集的尺寸。LeavePLabel和LeaveOneLabel差别在于leave的Label的种类的个数LeavePLabel这种设计是针对可能存在第三方的Label，比如我们的数据是一些季度的数据。那么很自然的一个想法就是把1,2,3个季度的数据当做训练集，第4个季度的数据当做测试集。这个时候只要输入每个样本对应的季度Label，就可以实现这样的功能。以下是实验代码，尽量自己多实验去理解。123456789101112131415161718192021222324252627#coding=utf-8import numpy as npimport sklearnfrom sklearnimport cross_validationX = np.array([[1, 2], [3, 4], [5, 6], [7, 8],[9, 10]])y = np.array([1, 2, 1, 2, 3])def show_cross_val(method): if method == &quot;lolo&quot;: labels = np.array([&quot;summer&quot;, &quot;winter&quot;, &quot;summer&quot;, &quot;winter&quot;, &quot;spring&quot;]) cv = cross_validation.LeaveOneLabelOut(labels) elif method == &apos;lplo&apos;: labels = np.array([&quot;summer&quot;, &quot;winter&quot;, &quot;summer&quot;, &quot;winter&quot;, &quot;spring&quot;]) cv = cross_validation.LeavePLabelOut(labels,p=2) elif method == &apos;loo&apos;: cv = cross_validation.LeaveOneOut(n=len(y)) elif method == &apos;lpo&apos;: cv = cross_validation.LeavePOut(n=len(y),p=3) for train_index, test_index in cv: print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index) X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] print &quot;X_train: &quot;,X_train print &quot;y_train: &quot;, y_train print &quot;X_test: &quot;,X_test print &quot;y_test: &quot;,y_testif __name__ == &apos;__main__&apos;: show_cross_val(&quot;lpo&quot;) 常用方法●train_test_split：分离训练集和测试集（不是K-Fold）●cross_val_score：交叉验证评分，可以指认cv为上面的类的实例●cross_val_predict：交叉验证的预测。 1.10 网格搜索包：sklearn.grid_search网格搜索最佳参数 ●GridSearchCV：搜索指定参数网格中的最佳参数●ParameterGrid：参数网格●ParameterSampler：用给定分布生成参数的生成器●RandomizedSearchCV：超参的随机搜索通过bestestimator.get_params()方法，获取最佳参数。 1.11 多分类、多标签分类包：sklearn.multiclass ●OneVsRestClassifier：1-rest多分类（多标签）策略●OneVsOneClassifier：1-1多分类策略●OutputCodeClassifier：1个类用一个二进制码表示示例代码12345678910111213141516171819202122232425262728293031#coding=utf-8from sklearn import metricsfrom sklearn import cross_validationfrom sklearn.svm import SVCfrom sklearn.multiclass import OneVsRestClassifierfrom sklearn.preprocessing import MultiLabelBinarizerimport numpy as npfrom numpy import randomX=np.arange(15).reshape(5,3)y=np.arange(5)Y_1 = np.arange(5)random.shuffle(Y_1)Y_2 = np.arange(5)random.shuffle(Y_2)Y = np.c_[Y_1,Y_2]def multiclassSVM(): X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2,random_state=0) model = OneVsRestClassifier(SVC()) model.fit(X_train, y_train) predicted = model.predict(X_test) print predicteddef multilabelSVM(): Y_enc = MultiLabelBinarizer().fit_transform(Y) X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y_enc, test_size=0.2, random_state=0) model = OneVsRestClassifier(SVC()) model.fit(X_train, Y_train) predicted = model.predict(X_test) print predictedif __name__ == &apos;__main__&apos;: multiclassSVM() # multilabelSVM() 上面的代码测试了svm在OneVsRestClassifier的包装下，分别处理多分类和多标签的情况。特别注意，在多标签的情况下，输入必须是二值化的。所以需要MultiLabelBinarizer()先处理。 2 具体模型2.1 朴素贝叶斯（Naive Bayes）包：**sklearn.cross_validation朴素贝叶斯的特点是分类速度快，分类效果不一定是最好的。 ●GasussianNB：高斯分布的朴素贝叶斯●MultinomialNB：多项式分布的朴素贝叶斯●BernoulliNB：伯努利分布的朴素贝叶斯所谓使用什么分布的朴素贝叶斯，就是假设P(x_i|y)是符合哪一种分布，比如可以假设其服从高斯分布，然后用最大似然法估计高斯分布的参数。高斯分布 多项式分布 伯努利分布 3 scikit-learn扩展3.0 概览具体的扩展，通常要继承sklearn.base包下的类。 ●BaseEstimator： 估计器的基类●ClassifierMixin ：分类器的混合类●ClusterMixin：聚类器的混合类●RegressorMixin ：回归器的混合类●TransformerMixin ：转换器的混合类 关于什么是Mixin（混合类），具体可以看这个知乎链接。简单地理解，就是带有实现方法的接口，可以将其看做是组合模式的一种实现。举个例子，比如说常用的TfidfTransformer，继承了BaseEstimator， TransformerMixin，因此它的基本功能就是单一职责的估计器和转换器的组合。 3.1 创建自己的转换器在特征抽取的时候，经常会发现自己的一些数据预处理的方法，sklearn里可能没有实现，但若直接在数据上改，又容易将代码弄得混乱，难以重现实验。这个时候最好自己创建一个转换器，在后面将这个转换器放到pipeline里，统一管理。例如《Python数据挖掘入门与实战》书中的例子，我们想接收一个numpy数组，根据其均值将其离散化，任何高于均值的特征值替换为1，小于或等于均值的替换为0。代码实现：12345678910111213141516from sklearn.base import TransformerMixinfrom sklearn.utils import as_float_arrayclass MeanDiscrete(TransformerMixin): #计算出数据集的均值，用内部变量保存该值。 def fit(self, X, y=None): X = as_float_array(X) self.mean = np.mean(X, axis=0) #返回self，确保在转换器中能够进行链式调用（例如调用transformer.fit(X).transform(X)） return self def transform(self, X): X = as_float_array(X) assert X.shape[1] == self.mean.shape[0] return X &gt; self.mean]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SRILM训练语言模型实战]]></title>
    <url>%2F2017%2F05%2F24%2FSRILM%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[SRILM基本使用方法1、从语料库中生成n-gram计数文件：1ngram-count -text train.txt -order 3 -write train.txt.count -text指向输入文件-order指向生成几元的n-gram,即n-write指向输出文件 2、从上一步生成的计数文件中训练语言模型：1ngram-count -read train.txt.count -order 3 -lm LM -interpolate -kndiscount -read指向输入文件，为上一步的输出文件-order与上同-lm指向训练好的语言模型输出文件最后两个参数为所采用的平滑方法，-interpolate为插值平滑，-kndiscount为 modified Kneser-Ney 打折法，这两个是联合使用的 3、利用上一步生成的语言模型计算测试集的困惑度：1ngram -ppl test.txt -order 3 -lm LM &gt; result -ppl为对测试集句子进行评分(logP(T)，其中P(T)为所有句子的概率乘积）和计算测试集困惑度的参数result为输出结果文件其他参数同上。如果想要每条句子单独打分，则使用以下命令：1ngram -ppl test.txt -order 3 -lm LM -debug 1 &gt; result 安装SRILM首先安装tcl1、去官网下载tcl8.6.6-src.tar.gz(http://www.tcl.tk/software/tcltk/download.html)2、解压到/home/user目录3、cd /tcl8.6.6/unix/4、make5、sudo make install 安装SRILM1、官网下载http://www.speech.sri.com/projects/srilm/download.html2、解压到/home/user/srilm/3、修改MakeFile文件：1234# SRILM = /home/wm/srilmSRILM = /home/wm/srilm#MACHINE_TYPE := $(shell $(SRILM)/sbin/machine-type)MACHINE_TYPE := i686-m64 4、修改srilm/common/makefile.i686-m64找到：GAWK = /usr/bin/awk修改为：GAWK = /usr/bin/gawk5、srilm目录下面执行：make World6、继续执行make test7、添加环境变量：1export PATH="$PATH:/home/user/srilm/bin/i686-m64:/home/user/srilm/bin" 添加之后执行ngram-count还是找不到命令，解决方法：i686-m6目录下创建新目录TEST，创建链接，把命令链接到TEST下面 训练语言模型1、需要训练好的语料数据训练的语料和训练结果是相关的，假如语料是言情小说那么训练的language model也是言情风格的。接下来要对语料进行分词，英文不需要分词，处理标点符号。 2、训练先从语料库生成n-gram计数文件1ngram-count -text test.pos -order 3 -write train.txt.count 生成的train.text.count第一列为n元词，第二列为相应的频率统计，结果如下： 3、接着从上一步的结果计数文件中训练语言模型1ngram-count -read train.txt.count -order 3 -lm LM -interpolate -kndiscount 看看结果，一元词有14081个，二元词有63891个，三元词有6644个以第一个为例：-0.3054161 ！ -0.4752966-0.3054161 ：log(概率)，以10为底-0.4752966 ：log(回退权重)，以10为底 4、利用上一步生成的语言模型计算测试集的困惑度：1ngram -ppl test.txt -order 3 -lm LM -debug 1 &gt; result ppl为对测试集句子进行评分(logP(T)，其中P(T)为所有句子的概率乘积）和计算测试集困惑度的参数，结果如下：0 zeroprobs, logprob= -61.72443 ppl= 58.01698 ppl1= 65.37691分别表示：无0概率；logP(T)=-105980，ppl==90.6875, ppl1= 107.805，均为困惑度。但公式稍有不同，如下： ppl=10^{-{logP(T)}/{Sen+Word}}; ppl1=10^{-{logP(T)}/Word}其中Sen和Word分别代表句子和单词数 参考：斯坦福课程-语言模型、Linux安装tcl、Linux安装SRILM]]></content>
      <categories>
        <category>语言模型</category>
      </categories>
      <tags>
        <tag>SRILM</tag>
        <tag>ngram</tag>
        <tag>LM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何选择合适的机器学习算法]]></title>
    <url>%2F2017%2F05%2F23%2F%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[机器学习算法速查表  如果你想要降维，那么使用主成分分析。 如果你需要得到快速的数值型预测，那么使用决策树或 logistic 回归。 如果你需要层级结果，那么使用层级聚类。 机器学习算法分类监督学习监督学习算法基于一组样本对作出预测。例如，以往销售业绩可以用来预测未来的价格走势。借助监督学习，我们会有一组由标注训练数据组成的输入变量和一组希望预测的输出变量。我们可以使用算法分析训练数据来学习一个将输入映射到输出的函数。算法推断的函数可通过概括训练数据预测未知情景中的结果进而预测未知的新实例。 分类：当数据被用于预测类别时，监督学习也可处理这类分类任务。给一张图片贴上猫或狗的标签就是这种情况。当分类标签只有两个时，这就是二元分类；超过两个则是多元分类。 回归：当预测为连续数值型时，这就是一个回归问题。 预测：这是一个基于过去和现在的数据预测未来的过程，其最大应用是趋势分析。一个典型实例是根据今年和前年的销售业绩以预测下一年的销售业绩。 半监督学习监督学习的主要挑战是标注数据价格昂贵且非常耗时。如果标签有限，你可以使用非标注数据来提高监督学习。由于在这一情况中机器并非完全有监督，所以称之为半监督。通过半监督学习，你可以使用只包含少量标注数据的非标注实例提升学习精确度。 无监督学习：在无监督学习之中，机器完全采用非标注数据，其被要求发现隐藏在数据之下的内在模式，比如聚类结构、低维流形或者稀疏树和图。 聚类：把一组数据实例归为一类，从而一个类（一个集群）之中的实例与其他类之中的实例更相似（根据一些指标），其经常被用于把整个数据集分割为若干个类。这种分析可在每一分类之中进行，从而帮助用户需要内在模式。 降维：减少考虑的变量数量。在很多应用中，原始数据有非常高的特征维度，并且一些特征是多余的且与任务不相关。降维将有助于发现真实、潜在的关系。 强化学习基于来自环境的反馈，强化学习分析和优化智能体的行为。机器尝试不同的策略，从而发现哪种行为能产生最大的回报，因此智能体不是被告知应该采取哪种行为。试错和延迟的 reward 是将强化学习与其他技术区分的特点。 如何选择线性回归和 Logistic 回归 线性回归（linear regression）是一种对连续型因变量$y$与单个或多个特征$X$之间的关系进行建模的方法。$y$和 $X$之间的关系可被线性建模成 如下形式：$y=\beta^TX+\epsilon$.当存在训练样本${x_i,yi}{i=1}^N$ 时，参数向量$\beta$可从训练样本中学到。 如果因变量不连续且为类别，那么线性回归可以转为使用一个 Sigmoid 函数的 logistic 回归。logistic 回归是一种简便，快速而且强大的分类算法。这里讨论二值情况，即因变量$y$ 只有两个值${yi\in(-1,1)}{i=1}^N$（这可以很容易被扩展为多类分类问题）。 在 logistic 回归中，我们使用不同的假设类别来尝试预测一个给定样例是属于「1」类还是「-1」类的概率。具体而言，我们将尝试学习如下形式的一个函数：$p(y_i=1|x_i )=\sigma(\beta^T x_i )$ 以及 $p(y_i=-1|x_i )=1-\sigma(\beta^T x_i )$. 其中$\sigma(x)=\frac{1}{1+exp(-x)}$是一个 sigmoid 函数。当存在训练样本${x_i,yi}{i=1}^N$ 时，参数向量$β$能在给定数据集下，最大化$β$对数似然值来学习。 线性 SVM 和核 SVM核（kernel）技巧可被用于将非线性可分函数映射成高维的线性可分函数。支持向量机（SVM）训练算法可以找到由超平面的法向量 w 和偏置项 b 表示的分类器。这个超平面（边界）可以按照最大间隔的方式来分开不同的类别。这个问题可以被转换一个条件优化问题：$$\begin{equation}\begin{aligned}&amp; \underset{w}{\text{minimize}}&amp; &amp; ||w|| \&amp; \text{subject to}&amp; &amp; y_i(w^T X_i-b) \geq 1, \; i = 1, \ldots, n.\end{aligned}\end{equation}$$当类别不是线性可分的时候，核技巧可被用于将非线性可分空间映射到高维的线性可分空间。 当因变量不是数值型时，logistic 回归和 SVM 应该被用作分类的首要尝试。这些模型可以轻松实现，它们的参数易于调节，而且其性能也相当好。所以这些模型非常适合初学者。 树和集成树决策树、随机森林和梯度提升（gradient boosting）全都是基于决策树的算法。决策树有很多变体，但它们所做的事情都一样——将特征空间细分为基本具有相同标签的区域。决策树易于理解和实现。但是，它们往往会过拟合数据，并且在树上面走得非常深。随机森林和梯度提升是两种流行的使用树算法来实现良好准确度的集成方法，该两种集成方法同时还能克服过拟合的问题。 神经网络和深度学习神经网络凭借其并行和分布式处理的能力而在 1980 年代中期兴起。但该领域的研究受到了反向传播训练算法的低效性的阻碍，而反向传播算法在神经网络参数的优化上得到了广泛的应用。支持向量机（SVM）和其它更简单的模型（可以通过解决凸优化问题而轻松训练）逐渐在机器学习领域替代的神经网络。 在最近几年，无监督预训练和层次方式的贪婪训练等新的和改进过的训练技术导致了人们对神经网络的兴趣的复兴。逐渐增强的计算能力（比如 GPU 和大规模并行处理（MPP））也促进了神经网络的复兴。神经网络研究的复兴已经为我们带来了数千层的模型。 换句话说，浅层神经网络已经发展成了深度学习神经网络。深度神经网络已经在监督学习领域取得了巨大的成功。当被用于语音识别和图像识别，深度学习的水平已经达到甚至超过了人类水平。当被应用于无监督学习任务（比如特征提取）时，深度学习也可以从原始图像和语音中提取出特征，且仅需要非常少的人类干预。 神经网络由 3 个部分组成：输入层、隐藏层和输出层。当输出层是一个分类变量时，那么该神经网络可以解决分类问题。当输出层是一个连续变量时，那么该网络可被用于执行回归。当输出层和输入层一样时，该网络可被用于提取内在的特征。隐藏层的数量定义了模型复杂度和建模能力。 k-均值/k-模式、高斯混合模型（GMM）聚类 k-均值/k-模式，GMM 聚类的目标是将 n 个观察分区成 k 个集群。k-均值聚类定义为硬分配标准：其样本会被而且仅可被分配给一个集群。然而，GMM 可以为每个样本定义一个软分配（soft assignment）。每个样本都有一个与每个集群相关的概率。当给定了集群的数量 k 时，这两个算法都很简单快速。 DBSCAN当聚类的数量 k 给定时，可以通过密度扩散（density diffusion）来连接样本，从而使用 DBSCAN（基于密度的空间聚类（density-based spatial clustering））。 层次聚类层次分区可以使用树结构（树形图）来进行可视化。其不需要集群的数量作为输入，且其分区可以使用不同的 K 而在不同的粒度水平下查看（即可以细化/粗化集群）。 PCA、SVD 和 LDA我们通常并不想直接给机器学习算法送入大量特征，因为一些特征可能是无关的或者「固有的（intrinsic）」的维度可能少于特征的数量。主成分分析（PCA）、奇异值分解（Singular Value Decomposition）和隐狄利克雷分布（LDA）都可以被用于执行降维。 PCA 是一种无监督聚类方法，其可以将原有的数据空间映射到一个更低维的空间，同时还能保留尽可能多的信息。PCA 基本上是在寻找一个保留了最大数据方差的子空间，且该子空间由数据的协方差矩阵的主要特征向量所定义。 SVD 和 PCA 有某种程度的联系——中心数据矩阵的 SVD（特征 vs. 样本）能提供定义由 PCA 所找到的同样子空间的主左奇异向量（dominant left singular vectors）。但是，SVD 是一种更加通用的技术，因为其也能做一些 PCA 可能做不到的事情。比如，一个用户 vs. 电影矩阵的 SVD 可以提取用户资料和电影资料，然后将其用在推荐系统中。此外，SVD 也被广泛用作主题建模工具，在自然语言处理领域被称为潜在语义分析。 自然语言处理领域的一个相关技术是隐狄利克雷分布（LDA）。LDA 是概率主题模型，其可以将文档分解为主题，分解方式就像高斯混合模型（GMM）将连续数据分解成高斯密度（Gaussian densities）。不同于 GMM，LDA 建模的是离散数据（文档中的词），并且会限制其主题以按狄利克雷分布而先验地分布。 原文链接]]></content>
      <categories>
        <category>归纳总结</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deepin 15.4 搭建LAMP环境&Python操作数据库]]></title>
    <url>%2F2017%2F05%2F17%2FDeepin-15-4-%E6%90%AD%E5%BB%BALAMP%E7%8E%AF%E5%A2%83-Python%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1.安装Apache1234$ sudo apt-get install apache2$ apache2 -v Server version: Apache/2.4.23 (Debian) Server built: 2016-09-29T10:03:31 执行apache2 出现bash: apache2: 未找到命令搜索apachetcl 将其路径/usr/sbin加入到.bashrc里面即可看到版本信息12$ export PATH=/usr/sbin:$PATH$ ~source ~/.bashrc 注：默认工程目录为：/var/www/html然后浏览器访问：http://localhost 出现下图则安装成功. 2.安装PHP1$ sudo apt-get install php5 再重启Apache2 :1service apache2 restart 3.测试PHP新建文件info.php：1$ vi /var/www/html/info.php 内容：&lt;?php phpinfo(); ?&gt;浏览器输入：http://localhost/info.php出现下面的图片即安装成功. 出现错误：Not FoundThe requested URL /test.php was not found on this server.Apache/2.4.23 (Debian) Server at localhost Port 80说明当前网站目录中没有test.php这个文件 4.安装MySQL1$ sudo apt-get install mysql-server mysql-client New password for the MySQL “root”user:&lt;–输入root密码Repeat password for the MySQL “root”user:&lt;–再输入一次 5.安装其他1234$ sudo apt-get install libapache2-mod-php5$ sudo apt-get install libapache2-mod-auth-mysql$ sudo apt-get install php5-mysql$ sudo apt-get install php5-gd 6.安装phpmyadmin管理Mysql1$ sudo apt-get install phpmyadmin 这里选择apache2给phpmyadmin添加软链接:sudo ln -s /usr/share/phpmyadmin /var/www/html phpmyadmin测试：在浏览器地址栏中打开http://localhost/phpmyadmin 如下所示：用户名初始为root密码是MySQL设置的密码，登录进来 7.Apache配置启用mod_rewrite模块。12$ sudo a2enmod rewrite$ sudo service apache2 restart 编辑/etc/apache2/apache2.conf，在最后添加一行，设置Apache支持.htm、.html和.php。1AddType application/x-httpd-php .php .htm .html 至此，配置完成。—————————————————————————————————— 8.Python操作数据库（CURD）安装MySQLdb1$ sudo pip install mysql-python 加载包12import MySQLdbimport MySQLdb.cursors 建立连接123db = MySQLdb.connect(host='127.0.0.1', user='root', passwd='123456', db='douban', port=3306, charset='utf8', cursorclass=MySQLdb.cursors.DictCursor)db.autocommit(True)cursor = db.cursor() 以豆瓣电影数据为例对MySQL数据库进行Creat,Update,Read,Delete操作首先在phpmyadmin里面创建数据表，如下图 ①创建数据库，读入数据1234567891011121314# Createfr = open('douban_movie.txt', 'r')count = 0for line in fr: count += 1 print count if count == 1: continue line = line.strip().split('^') # execute()函数第一个参数为要执行的SQL命令 cursor.execute("insert into movie(name, url, score, length, description) values(%s, %s, %s, %s, %s)", [line[1], line[2], line[4], line[-3], line[-1]])fr.close() ②更新操作1cursor.execute(&quot;update movie set title=%s, length=%s where id=%s&quot;, [&apos;湄公河行动&apos;, 120, 1]) ③读取数据123456789101112cursor.execute(&quot;select * from movie&quot;)movies = cursor.fetchall()print type(movies), len(movies), movies[0]cursor.execute(&quot;select id, name, url from movie&quot;)movie = cursor.fetchone()print type(movie), len(movie), moviecursor.execute(&quot;select id, name, url from movie order by id desc&quot;)movie = cursor.fetchone()print type(movie), len(movie), movie ④删除操作1cursor.execute(&quot;delete from movie where id=%s&quot;, [1])]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[摔跤吧爸爸]]></title>
    <url>%2F2017%2F05%2F16%2F%E6%91%94%E8%B7%A4%E5%90%A7%E7%88%B8%E7%88%B8%2F</url>
    <content type="text"><![CDATA[我慢慢地、慢慢地了解到所谓父女母子一场只不过意味着你和他的缘分就是今生今世不断地在目送他的背影渐行渐远你站立在小路的这一端看着他逐渐消失在小路转弯的地方而且，他用背影默默告诉你不必追 ——— 来自于 Deepin 15.4 桌面版 &amp; Moeditor V_0.1.1]]></content>
      <categories>
        <category>影评</category>
      </categories>
      <tags>
        <tag>Movie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras下可视化神经网络]]></title>
    <url>%2F2017%2F05%2F15%2FKeras%E4%B8%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[windows 10 &amp; Python 3.51.下载graphviz-2.3.8.msi并安装，官网：http://www.graphviz.org/Download_windows.php添加环境变量：C:\Program Files (x86)\Graphviz2.38\bin2.pip install graphviz (0.7)3.pip install pydot (1.2.3)123456789101112131415161718192021222324import numpy as npfrom keras.models import Sequentialfrom keras.layers.core import Dense, Activationfrom keras.optimizers import SGDfrom keras.utils import np_utils from keras.utils import plot_model# keras 1.0 use:# from keras.utils.visualize_util import plotdef run(): # 构建神经网络 model = Sequential() model.add(Dense(4, input_dim=2, init='uniform')) model.add(Activation('relu')) model.add(Dense(2, init='uniform')) model.add(Activation('sigmoid')) sgd = SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True) model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy']) # 神经网络可视化 plot_model(model, to_file='model.png')if __name__ == '__main__': run() 可视化结果： Ubuntu 16.04 &amp; Python 2.71.sudo pip install graphviz (0.7)2.sudo apt-get install graphviz (2.38.0-16)3.sudo pip install pydot==1.1.0 (1.2.3的版本find_graphviz函数会报错)来个VAE网络试试：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119# -*- coding: utf-8 -*-’‘’Reference: "Auto-Encoding Variational Bayes" https://arxiv.org/abs/1312.6114 ‘’‘import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm from keras.layers import Input, Dense, Lambda from keras.models import Model from keras import backend as K from keras import objectives from keras.datasets import mnist from keras.utils import plot_modelimport sys saveout = sys.stdout file = open('variational_autoencoder.txt','w') sys.stdout = file batch_size = 100 original_dim = 784 #28*28 latent_dim = 2 intermediate_dim = 256 nb_epoch = 50 epsilon_std = 1.0 #my tips:encoding x = Input(batch_shape=(batch_size, original_dim)) h = Dense(intermediate_dim, activation='relu')(x) z_mean = Dense(latent_dim)(h) z_log_var = Dense(latent_dim)(h) #my tips:Gauss sampling,sample Z def sampling(args): z_mean, z_log_var = args epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std) return z_mean + K.exp(z_log_var / 2) * epsilon # note that "output_shape" isn't necessary with the TensorFlow backend # my tips:get sample z(encoded) z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # we instantiate these layers separately so as to reuse them later decoder_h = Dense(intermediate_dim, activation='relu') decoder_mean = Dense(original_dim, activation='sigmoid') h_decoded = decoder_h(z) x_decoded_mean = decoder_mean(h_decoded) #my tips:loss(restruct X)+KL def vae_loss(x, x_decoded_mean): #my tips:logloss xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean) #my tips:see paper's appendix B kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) return xent_loss + kl_loss vae = Model(x, x_decoded_mean) vae.compile(optimizer='rmsprop', loss=vae_loss) # train the VAE on MNIST digits (x_train, y_train), (x_test, y_test) = mnist.load_data(path='mnist.pkl.gz') x_train = x_train.astype('float32') / 255. x_test = x_test.astype('float32') / 255. x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:]))) vae.fit(x_train, x_train, shuffle=True, nb_epoch=nb_epoch, verbose=2, batch_size=batch_size, validation_data=(x_test, x_test)) # build a model to project inputs on the latent space encoder = Model(x, z_mean) # display a 2D plot of the digit classes in the latent space x_test_encoded = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(6, 6)) plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test) plt.colorbar() plt.show() # build a digit generator that can sample from the learned distribution decoder_input = Input(shape=(latent_dim,)) _h_decoded = decoder_h(decoder_input) _x_decoded_mean = decoder_mean(_h_decoded) generator = Model(decoder_input, _x_decoded_mean) # display a 2D manifold of the digits n = 15 # figure with 15x15 digits digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian # to produce values of the latent variables z, since the prior of the latent space is Gaussian grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) grid_y = norm.ppf(np.linspace(0.05, 0.95, n)) for i, yi in enumerate(grid_x): for j, xi in enumerate(grid_y): z_sample = np.array([[xi, yi]]) x_decoded = generator.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) plt.imshow(figure, cmap='Greys_r') plt.show() plot_model(vae,to_file='variational_autoencoder_vae.png',show_shapes=True) plot_model(encoder,to_file='variational_autoencoder_encoder.png',show_shapes=True) plot_model(generator,to_file='variational_autoencoder_generator.png',show_shapes=True) sys.stdout.close() sys.stdout = saveout 可视化结果：]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现基于词典的文本情感分析]]></title>
    <url>%2F2017%2F05%2F15%2FPython%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8E%E8%AF%8D%E5%85%B8%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[情感分析目前比较流行的方法有两种，一是词库法、二是机器学习法。机器学习法则是在已知分类语料的情况下，构建文档–词条矩阵，然后应该各种分类算法（knn、NB、RF、SVM、DL等），预测出其句子的情感。 通过词库的方式定性每一句话的情感没有什么高深的理论基础，其思想就是对每一句话进行分词，然后对比正面词库与负面词库，从而计算出句子的正面得分（词中有多少是正面的）与负面得分（词中有多少是负面的），以及综合得分（正面得分-负面得分）。虽然该方法通俗易懂，但是非常耗人力成本，如正负面词库的构建、自定义词典的导入等。 以某汽车的空间评论数据作为分析对象，来给每条评论打上正面或负面的标签：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153# -*- coding: utf-8 -*-"""Created on Sun May 14 16:04:08 2017wordcloud required C++ 14.0running on python 3.5@author: wangmin"""import jiebaimport collectionsimport numpy as npfrom PIL import Imagefrom wordcloud import WordCloud, ImageColorGeneratorimport matplotlib.pyplot as plt # 读入评论数据，正负情感词典并合并evaluation = []stopwords = []pos = []neg = []mydict = []infile = open("evaluation.csv", 'r')for line in infile: data = line.rstrip().split(',') evaluation.append(data[1])del evaluation[0] infile = open("negative.csv", 'r')for line in infile: data = line.rstrip().split(',') neg.append(data[1])infile = open("positive.csv", 'r')for line in infile: data = line.rstrip().split(',') pos.append(data[1])mydict = pos + neg file = open("stopwords.csv", 'r')for s in file: data = s.rstrip().split(',') stopwords.append(data[1]) # 对每条评论分词,并保存分词结果eva = []for i in range(len(evaluation)): seg_list = jieba.cut(evaluation[i], cut_all=False) seg_list = list(seg_list) eva.append(seg_list) # 删除一个字的词new_eva = evatmp = []t = 0for j in range(3321): for k in range(len(eva[j])): if len(eva[j][k]) &gt;= 2: tmp.append(eva[j][k]) new_eva[t] = tmp tmp = [] t=t+1 # 删除停止词(对分析没有意义的词)#for word in stopwords: # 自定义情感类型得分函数def GetScore(list): neg_s = 0 pos_s = 0 for w in list: if (w in neg) == True: neg_s = neg_s + 1 elif (w in pos) == True: pos_s = pos_s + 1 if (neg_s-pos_s) &gt; 0: score = 'NEGATIVE' return score elif (neg_s-pos_s) &lt; 0: score = 'POSITIVE' return score else: score = 'NEUTRAL' return score # 计算每条评论的正负得分Score = []for l in range(len(new_eva)): Score.append(GetScore(new_eva[l])) ''' def find_all_index(arr,item): return [i for i,a in enumerate(arr) if a==item] NEG=find_all_index(Score,'NEGATIVE')POS=find_all_index(Score,'POSITIVE')NEU=find_all_index(Score,'NEUTRAL') print(len(NEG))print(len(POS))print(len(NEU)) ''' # 统计词频wf = &#123;&#125;for p in range(len(new_eva)): for word in new_eva[p]: if word not in wf: wf[word]=0 wf[word]+=1def Sort_by_count(d): d = collections.OrderedDict(sorted(d.items(),key = lambda t: -t[1])) return dwf = Sort_by_count(wf) top_key = []top_word = []for key in wf.items(): top_key.append(key)top_word = top_key[1:51] print(top_key[0:49])#for key,values in wf.items():# print(key + "%d" % values)# 绘制词云word_space_split = 'a'for i in range(3322): new_eva[i] = " ".join(new_eva[i]) word_space_split += new_eva[i] word_space_split = word_space_split.replace('word','') # mask = np.array(Image.open('C:/Users/wangmin/Pictures/aaa/abc.png')) wc = WordCloud(font_path='C:\Windows\Fonts\STSONG.TTF',#设置字体 background_color="black", #背景颜色 scale=5, margin=1, stopwords = stopwords, #设置停用词 max_words=50,# 词云显示的最大词数 max_font_size=150, #字体最大值 random_state=30,) # 设置有多少种随机生成状态，即有多少种配色方案wc.generate(word_space_split) #image_colors = ImageColorGenerator(abel_mask) # 显示图片 plt.imshow(wc) plt.axis("off") plt.show() 通过文字云绘制结果可以判断，消费者还是非常认可该款汽车的空间大小，普遍表示满意。 数据及源码地址：链接: https://pan.baidu.com/s/1o8QBDOI 密码: 2xw8]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>文本情感分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[没有故事的男生(转)]]></title>
    <url>%2F2017%2F05%2F15%2F%E6%B2%A1%E6%9C%89%E6%95%85%E4%BA%8B%E7%9A%84%E7%94%B7%E7%94%9F-%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[擦肩而过的人很多，不知该和谁说再见。 回首小半生，也算行过很长的路，遇到的旅人口中有风流往韵，有声色犬马，还有姑娘绯红的脖颈与娇羞的脸。 你听完只是笑笑，行囊里有书有酒，还有迟迟未寄出的信。 恍恍惚惚二十年，既不坎坷，也不多舛，你想把心中的故事都讲完，却在开口的一瞬间停顿，任由盛情满怀，却感染不了在座的人。 仿佛所有的妙趣横生，都被旁人说尽了。 故事里的千肠百转，诗人眼里的江河湖海，都和自己的前尘往事毫无瓜葛。 合上书页，你只叹下笔的那人在矫情什么呢？ 日子不都是一样过？ 我也曾感慨过，那些即将迎来的故事，是否像远方一样充满不定，那些还未见的人，是否当得有趣，笑的大声忘我，哭的撕心裂肺，擦干眼泪还能再讲一百零八个段子，主角是他和她，亦或素未谋面的甲乙丙，却从来都不是我。 可这不值得感伤。 李白曾说今人不见古时月，今月曾经照古人。岁月蹉跎，人来人走，换了一茬又一茬。有故事的人多，没故事的人更多，像遍植大山的高树，像洒满小路的叶子，秋风一起，都逃不过枯黄。 所以我不急，更不怕没有故事可以讲。 毕竟许多故事的背后，都有悲欢离合，我不曾经历，不曾感悟，我只是走在属于自己的路上，走的安分守己，心安理得。 旁人的故事再多，那是他经过点缀后如画的人生，我的故事再少，也有着淡泊如水的平静，偶尔有月光坠进我这一池，能激起丝丝涟漪，也算难得的惊喜。 我很庆幸，庆幸自己没有那么多故事。 每次有故事，都注定有牺牲。 你回溯过往，引经据典，三言两语间引得满堂喝彩，那只能算作演讲是强项。 真正的感怀，是求而不得，有苦说不出，有欢愉无人懂。 所谓的故事，无非是过眼而退的风景，行过的路，遇见的人。 这些复杂的意象交织成一个个完整或破碎的梦，在你需要时蔓上心头，让你跟着哭，跟着笑，一遍遍的告诉别人其实我也经历过。 一番番青春未尽游丝逸，思悄悄木叶缤纷霜雪催。经历那么多，也敌不过四季更替，岁月往复。 有时候我就想，自己或许只是一道风景。落在不同的人眼里，我也许是风，是冥顽的石头，亦或灿烂的花火。 我用自己的脊背，撑起一副庸而不得志，碌而无所为的躯干，行走在一条喧嚣络绎的大道上，与身旁的行人擦肩而过，笑与他们一起笑，悲与他们一起悲。 好似我这一生，从未活出过自己的喜怒哀乐。 我从来都不是故事里的主角，可我低头看看自己的脚尖，也曾沾染过几多泥泞，我紧紧衣领，或坐或站，都是个堂堂正正的人。 我无愧于江河湖海，即便是诗人也有举世不得志的抱负，一辈子被贬被罚，流离失所，故事倒真他妈多，可到头来，能懂的人，到底有几何？ 你若说我没故事，可我明明一路向前，也时常回头看看飘过的风景，也会在寂静辽阔的星夜，想想许多年前的梦。 比起那些整天活在故事里的人，我无比庆幸，我庆幸自己活的真实，不必整日沉沦在往昔的虚荣中，更不必被困于曾经的恩怨情仇，仅仅一句话一首歌就勾动我那几分愁绪，恨不能矫情到死。 我起码真实的活着，我守望着明天的日落与黄昏，期盼着即将到来的人。 我一直坚信得之我幸，失之我命，故事不来，我就安稳的过。 这日子那么多，总有一天不同。剧本若早就写好，我就尽力把自己的戏份演足。 那些注定精彩一生的人，也都在各自的舞台上扮着生旦净末丑。 听到好的，见到妙的，我也不会吝啬吆喝。 诚然观众们都喜个声响，乐个乖张，他们听着别人的故事入梦，感慨，安慰自我。 即便我是配角，戏不多，可大幕拉开，就算剧院里空无一人。 我自有千种风情，何须与人说。 作者：钱品聚链接：https://www.zhihu.com/question/59689185/answer/169241704来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>知乎</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自编码器AutoEncoder]]></title>
    <url>%2F2017%2F05%2F14%2F%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8AutoEncoder%2F</url>
    <content type="text"><![CDATA[自动编码器(Autoencoders，AE)是一种前馈无返回的神经网络，有一个输入层，一个隐含层，一个输出层，典型的自动编码器结构如图1所示，在输入层输入X，同时在输出层得到相应的输出Z，层与层之间都采用S型激活函数进行映射。 图1 典型的自动编码器结构 输入层到隐含层的映射关系可以看作是一个编码过程，通过映射函数f把输出向量x映射到隐含层输出y。从隐含层到输出层的过程相当于一个解码过程，把隐含层输出y映射通过映射函数g回去“重构”向量z。对于每一个输入样本x(i)而言，经过自动编码器之后都会转化为一个对应的输出向量z(i)=g[f(x(i))]。当自动编码器训练完成之后，输入X与输出Z完全相同，则对应的隐含层的输出可以看作是输入X的一种抽象表达，因此它可以用于提取输入数据的特征。此外，因为它的隐含层节点数少于输入节点数，因此自动编码器也可以用于降维和数据压缩。网络参数的训练方面，自动编码器采用反向传播法来进行训练，但自动编码器需要大量的训练样本，随着网络结构越变越复杂，网络计算量也随之增大。 对自动编码器结构进行改进得到其他类型的自动编码器，比较典型的是稀疏自动编码器、降噪自动编码器。降噪自动编码器（Denoising Autoencoder，DAE）是对输入数据进行部分“摧毁”，然后通过训练自动编码器模型，重构出原始输入数据，以提高自动编码器的鲁棒性。对输入数据进行“摧毁”的过程其实类似于对数据加入噪声。稀疏自动编码器则是对自动编码器加入一个正则化项，约束隐含层神经元节点大部分输出0，少部分输出非0。稀疏编码器大大减小了需要训练的参数的数目，降低了训练的难度，同时克服了自动编码器容易陷入局部及小值和存在过拟合的问题。降噪编码器采用有噪声的输入数据来训练网络参数，提高了自动编码器的泛化能力。 搭建一个自动编码器需要完成下面三样工作：搭建编码器，搭建解码器，设定一个损失函数，用以衡量由于压缩而损失掉的信息。编码器和解码器一般都是参数化的方程，并关于损失函数可导，典型情况是使用神经网络。编码器和解码器的参数可以通过最小化损失函数而优化，例如SGD。 自编码器只是一种思想，在具体实现中，encoder和decoder可以由多种深度学习模型构成，例如全连接层、卷积层或LSTM等，以下使用Keras来实现用于图像去噪的卷积自编码器。 单隐藏层的自编码器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# coding:utf8# runing on python 2.7from keras.layers import Input, Densefrom keras.models import Modelfrom keras.datasets import mnistimport matplotlib.pyplot as pltimport numpy as np# 构建单隐藏层的自编码器# this is the size of our encoded representationsencoding_dim = 32 # 32 floats -&gt; compression of factor 24.5, assuming the input is 784 floats# this is our input placeholderinput_img = Input(shape=(784,))# "encoded" is the encoded representation of the inputencoded = Dense(encoding_dim, activation='relu')(input_img)# "decoded" is the lossy reconstruction of the inputdecoded = Dense(784, activation='sigmoid')(encoded)# this model maps an input to its reconstructionautoencoder = Model(input=input_img, output=decoded)# 定义编码器# this model maps an input to its encoded representationencoder = Model(input=input_img, output=encoded)# 定义解码器# create a placeholder for an encoded (32-dimensional) inputencoded_input = Input(shape=(encoding_dim,))# retrieve the last layer of the autoencoder modeldecoder_layer = autoencoder.layers[-1]# create the decoder modeldecoder = Model(input=encoded_input, output=decoder_layer(encoded_input))autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')#准备数据(x_train, _), (x_test, _) = mnist.load_data()x_train = x_train.astype('float32') / 255.x_test = x_test.astype('float32') / 255.x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))print x_train.shapeprint x_test.shape# 训练模型autoencoder.fit(x_train, x_train, nb_epoch=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))# 在测试集上进行编码和解码# encode and decode some digits# note that we take them from the *test* setencoded_imgs = encoder.predict(x_test)decoded_imgs = decoder.predict(encoded_imgs)# 显示结果n = 10 # how many digits we will displayplt.figure(figsize=(20, 4))for i in range(n): # display original ax = plt.subplot(2, n, i + 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # display reconstruction ax = plt.subplot(2, n, i + 1 + n) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False)plt.show() 50个epoch后，看起来自编码器优化的不错了，损失是0.10，我们可视化一下重构出来的输出.上面是原始图像，下面为重构图像，用此方法丢失了太多细节。 稀疏约束的自编码器上面的隐层有32个神经元，这种情况下，一般而言自编码器学到的是PCA的一个近似。但是如果我们对隐层单元施加稀疏性约束的话，会得到更为紧凑的表达，只有一小部分神经元会被激活。在Keras中，可以通过添加一个activity_regularizer达到对某层激活值进行约束的目的：123456789from keras import regularizersencoding_dim = 32input_img = Input(shape=(784,))# add a Dense layer with a L1 activity regularizerencoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_img)decoded = Dense(784, activation='sigmoid')(encoded)autoencoder = Model(input_img, decoded) 因为添加了正则性约束，所以模型过拟合的风险降低，可以训练多几次，这次训练100个epoch，得到损失为0.11，多出来的0.01基本上是由于正则项造成的。可视化结果如下： 结果上没有什么差别，区别在于编码出来的码字更加稀疏了。稀疏自编码器的在10000个测试图片上的码字均值为3.33，而之前的为7.30 多隐藏层的自编码器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# coding:utf8# runing on python 2.7from keras.layers import Input, Densefrom keras.models import Modelfrom keras.datasets import mnistimport matplotlib.pyplot as pltimport numpy as npencoding_dim = 32input_img = Input(shape=(784,))encoded = Dense(128, activation='relu')(input_img)encoded = Dense(64, activation='relu')(encoded)encoded = Dense(32, activation='relu')(encoded)decoded = Dense(64, activation='relu')(encoded)decoded = Dense(128, activation='relu')(decoded)decoded = Dense(784, activation='sigmoid')(decoded)autoencoder = Model(input=input_img, output=decoded)autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')#准备数据(x_train, _), (x_test, _) = mnist.load_data()x_train = x_train.astype('float32') / 255.x_test = x_test.astype('float32') / 255.x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))print x_train.shapeprint x_test.shape# 训练模型autoencoder.fit(x_train, x_train, nb_epoch=100, batch_size=256, shuffle=True, validation_data=(x_test, x_test))decoded_imgs = autoencoder.predict(x_test)# 显示结果n = 10 # how many digits we will displayplt.figure(figsize=(20, 4))for i in range(n): # display original ax = plt.subplot(2, n, i + 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # display reconstruction ax = plt.subplot(2, n, i + 1 + n) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False)plt.show() 100个epoch后，loss大概是0.097，比之前的模型好那么一点点。 卷积自编码器由于输入是图像，因此使用卷积神经网络（convnets）作为编码器和解码器是有意义的。在实际设置中，应用于图像的自动编码器始终是卷积自动编码器 - 它们的性能要好得多。编码器将由栈Conv2D和MaxPooling2D层组成（最大池用于空间下采样），而解码器将由Conv2D和UpSampling2D层组成。12 自编码器图像去噪12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485# coding:utf8# runing on python 2.7from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, UpSampling2Dfrom keras.models import Modelfrom keras.datasets import mnistimport matplotlib.pyplot as pltfrom keras.callbacks import TensorBoardimport numpy as np# 获取数据集MNIST，将像素点值转化到0-1区间，并且重塑为N×1×28×28的四维tensor(x_train, _), (x_test, _) = mnist.load_data()x_train = x_train.astype('float32') / 255.x_test = x_test.astype('float32') / 255.x_train = np.reshape(x_train, (len(x_train),28, 28,1))x_test = np.reshape(x_test, (len(x_test),28, 28,1))# 添加噪声，即叠加一个随机的高斯白噪声，并限制加噪之后的值仍处于0-1区间noise_factor = 0.5x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)x_train_noisy = np.clip(x_train_noisy, 0., 1.)x_test_noisy = np.clip(x_test_noisy, 0., 1.)input_img = Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format# 定义encoder部分，由两个32×3×3的卷积层和两个2×2的最大池化层组成x = Convolution2D(32, (3, 3), activation='relu', padding='same')(input_img)x = MaxPooling2D((2, 2), padding='same')(x)x = Convolution2D(32, (3, 3), activation='relu', padding='same')(x)encoded = MaxPooling2D((2, 2), padding='same')(x)# at this point the representation is (7, 7, 32)# 定义decoder部分，由两个32×3×3的卷积层和两个2×2的上采样层组成。x = Convolution2D(32, (3, 3), activation='relu', padding='same')(encoded)x = UpSampling2D((2, 2))(x)x = Convolution2D(32, (3, 3), activation='relu', padding='same')(x)x = UpSampling2D((2, 2))(x)decoded = Convolution2D(1, (3, 3), activation='sigmoid', padding='same')(x)# 输入和输出连接起来，构成autoencoder并compileautoencoder = Model(input_img, decoded)autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')# 使用x_train作为输入和输出来训练我们的autoencoder，并使用x_test进行validationautoencoder.fit(x_train_noisy, x_train, epochs=100, batch_size=128, shuffle=True, validation_data=(x_test_noisy, x_test), callbacks=[TensorBoard(log_dir='/tmp/tb', histogram_freq=0, write_graph=False)])# 使用autoencoder对x_test预测，并将预测结果绘制出来，和原始加噪图像进行对比decoded_imgs = autoencoder.predict(x_test_noisy)n = 10plt.figure(figsize=(20, 4))for i in range(n): # display original ax = plt.subplot(2, n, i + 1) plt.imshow(x_test_noisy[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) # display reconstruction ax = plt.subplot(2, n, i + 1 + n) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False)plt.show()# 加噪之后的结果# n = 10# plt.figure(figsize=(20, 2))# for i in range(n):# ax = plt.subplot(1, n, i + 1)# plt.imshow(x_test_noisy[i].reshape(28, 28))# plt.gray()# ax.get_xaxis().set_visible(False)# ax.get_yaxis().set_visible(False)# plt.show() 运行结果：上面一行是添加噪音的图像，下面一行是去噪之后的结果 Seq2Seq自编码器如果输入的是序列，而不是向量或2D图像，首先使用LSTM编码器将输入序列转换成包含整个序列信息的单个向量，然后重复该向量n时间（n输出序列中的时间步长数），并运行一个LSTM解码器将该恒定序列转换成目标序列。1234567891011from keras.layers import Input, LSTM, RepeatVectorfrom keras.models import Modelinputs = Input(shape=(timesteps, input_dim))encoded = LSTM(latent_dim)(inputs)decoded = RepeatVector(timesteps)(encoded)decoded = LSTM(input_dim, return_sequences=True)(decoded)sequence_autoencoder = Model(inputs, decoded)encoder = Model(inputs, encoded) 变分自编码 VAEVAE结构概率解释的神经网络通过假设每个参数的概率分布来降低网络中每个参数的单个值的刚性约束。例如，在经典神经网络中计算权重w_i=0.7，在概率版本中，计算均值大约为u_i = 0.7和方差为v_i = 0.1的高斯分布，即w_i =N（0.7,0.1）。这个假设将输入，隐藏表示以及神经网络的输出转换为概率随机变量。这类网络被称为贝叶斯神经网络或BNN。 encoder、decoder:均可为任意结构encoder 又称 recognition modeldecoder 又称 generative modelencoder 的输出（2×m 个数）视作分别为 m 个高斯分布的均值（z_mean）和方差的对数一段VAE伪代码：1234567891011121314151617181920212223242526network= &#123; # encoder encoder_x = Input_layer(size=input_size, input=data) encoder_h = Dense_layer(size=hidden_size, input= encoder_x) # the re-parameterized distributions that are inferred from data z_mean = Dense(size=number_of_distributions, input=encoder_h) z_variance = Dense(size=number_of_distributions, input=encoder_h) epsilon= random(size=number_of_distributions) # decoder network needs a sample from the code distribution z_sample= z_mean + exp(z_variance / 2) * epsilon #decoder decoder_h = Dense_layer(size=hidden_size, input=z_sample) decoder_output = Dense_layer(size=input_size, input=decoder_h)&#125;cost=&#123; reconstruction_loss = input_size * crossentropy(data, decoder_output) kl_loss = - 0.5 * sum(1 + z_variance - square(z_mean) - exp(z_variance)) cost_total= reconstruction_loss + kl_loss&#125;stochastic_gradient_descent(data, network, cost_total) 采样（sampling）根据 encoder 输出的均值与方差，生成服从相应高斯分布的随机数：12epsilon = K.random_normal(shape=(batch_size, m), mean=0.,std=epsilon_std) # 标准高斯分布z = z_mean + exp(z_log_var / 2) * epsilon z 就可以作为上面定义的 decoder 的输入，进而产生 n 维的输出 x^这里运用了 reparemerization 的技巧。由于 z∼N(μ,σ)，我们应该从 N(μ,σ) 采样，但这个采样操作对 μ 和 σ 是不可导的，导致常规的通过误差反传的梯度下降法（GD）不能使用。通过 reparemerization，我们首先从 N(0,1) 上采样 ϵ，然后，z=σ⋅ϵ+μ。这样，z∼N(μ,σ)，而且，从 encoder 输出到 z，只涉及线性操作，（ϵ 对神经网络而言只是常数），因此，可以正常使用 GD 进行优化。 优化目标encoder 和 decoder 组合在一起，我们能够对每个 x∈X，输出一个相同维度的 x^。我们目标是，令 x^ 与 x 自身尽量的接近。即 x 经过编码（encode）后，能够通过解码（decode）尽可能多的恢复出原来的信息。由于 x∈[0,1]，因此用交叉熵（cross entropy）度量 x 与 x^ 差异：$$xent = \sum_{i=1}^n-[x_i\cdot\log(\hat{x}_i)+(1-x_i)\cdot\log(1-\hat{x}_i)]$$xent 越小，x 与 x^ 越接近。 也可以用均方误差来度量：$$mse=\sum_{i=1}^n(x_i - \hat{x}_i)^2$$mse 越小，两者越接近。 另外，需要对 encoder 的输出 z_mean（μ）及 z_log_var（logσ2）加以约束。这里使用的是 KL 散度： $$KL = -0.5 * (1+\log\sigma^2-\mu^2-\sigma^2)=-0.5(1+\log\sigma^2-\mu^2-exp(\log\sigma^2))$$总的优化目标（最小化）为：$$loss = xent + KL$$或者$$loss = mse + KL$$ 综上所述，有了目标函数，并且从输入到输出的所有运算都可导，就可以通过 SGD 或其改进方法来训练这个网络了。训练过程只用到 x（同时作为输入和目标输出），而与 x 的标签无关，因此，这是无监督学习。 代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm from keras.layers import Input, Dense, Lambda from keras.models import Model from keras import backend as K from keras import objectives from keras.datasets import mnist from keras.utils import plot_modelimport sys saveout = sys.stdout file = open(&apos;variational_autoencoder.txt&apos;,&apos;w&apos;) sys.stdout = file batch_size = 100 original_dim = 784 #28*28 latent_dim = 2 intermediate_dim = 256 nb_epoch = 50 epsilon_std = 1.0 #my tips:encoding x = Input(batch_shape=(batch_size, original_dim)) h = Dense(intermediate_dim, activation=&apos;relu&apos;)(x) z_mean = Dense(latent_dim)(h) z_log_var = Dense(latent_dim)(h) #my tips:Gauss sampling,sample Z def sampling(args): z_mean, z_log_var = args epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std) return z_mean + K.exp(z_log_var / 2) * epsilon # note that &quot;output_shape&quot; isn&apos;t necessary with the TensorFlow backend # my tips:get sample z(encoded) z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # we instantiate these layers separately so as to reuse them later decoder_h = Dense(intermediate_dim, activation=&apos;relu&apos;) decoder_mean = Dense(original_dim, activation=&apos;sigmoid&apos;) h_decoded = decoder_h(z) x_decoded_mean = decoder_mean(h_decoded) #my tips:loss(restruct X)+KL def vae_loss(x, x_decoded_mean): #my tips:logloss xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean) #my tips:see paper&apos;s appendix B kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) return xent_loss + kl_loss vae = Model(x, x_decoded_mean) vae.compile(optimizer=&apos;rmsprop&apos;, loss=vae_loss) # train the VAE on MNIST digits (x_train, y_train), (x_test, y_test) = mnist.load_data(path=&apos;mnist.pkl.gz&apos;) x_train = x_train.astype(&apos;float32&apos;) / 255. x_test = x_test.astype(&apos;float32&apos;) / 255. x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:]))) vae.fit(x_train, x_train, shuffle=True, nb_epoch=nb_epoch, verbose=2, batch_size=batch_size, validation_data=(x_test, x_test)) # build a model to project inputs on the latent space encoder = Model(x, z_mean) # display a 2D plot of the digit classes in the latent space x_test_encoded = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(6, 6)) plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test) plt.colorbar() plt.show() # build a digit generator that can sample from the learned distribution decoder_input = Input(shape=(latent_dim,)) _h_decoded = decoder_h(decoder_input) _x_decoded_mean = decoder_mean(_h_decoded) generator = Model(decoder_input, _x_decoded_mean) # display a 2D manifold of the digits n = 15 # figure with 15x15 digits digit_size = 28 figure = np.zeros((digit_size * n, digit_size * n)) # linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian # to produce values of the latent variables z, since the prior of the latent space is Gaussian grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) grid_y = norm.ppf(np.linspace(0.05, 0.95, n)) for i, yi in enumerate(grid_x): for j, xi in enumerate(grid_y): z_sample = np.array([[xi, yi]]) x_decoded = generator.predict(z_sample) digit = x_decoded[0].reshape(digit_size, digit_size) figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit plt.figure(figsize=(10, 10)) plt.imshow(figure, cmap=&apos;Greys_r&apos;) plt.show() plot_model(vae,to_file=&apos;variational_autoencoder_vae.png&apos;,show_shapes=True) plot_model(encoder,to_file=&apos;variational_autoencoder_encoder.png&apos;,show_shapes=True) plot_model(generator,to_file=&apos;variational_autoencoder_generator.png&apos;,show_shapes=True) sys.stdout.close() sys.stdout = saveout VAE形状： 编码器形状： 代码中将编码得到的均值U可视化结果： 生成器形状： 可将从二维高斯分布中随机采样得到的Z，解码成手写数字图片代码中将解码得到的图像可视化： 小结学习算法的最好方式还是读代码，网上有许多基于不同框架的 VAE 参考实现，如tensorflow 、theano、keras、torch]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>keras</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言之ggplot绘图]]></title>
    <url>%2F2017%2F05%2F10%2FR%E8%AF%AD%E8%A8%80%E4%B9%8Bggplot%E7%BB%98%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[#条形图、折线图、散点图、face_wrap() ggplot() + geom_bar() # geom_line,point,geom_area # eg library(ggplot2) library(gcookbook) ggplot(BOD) + geom_line(aes(x=Time,y=demand)) + geom_point(aes(x=Time,y=demand)) us &lt;- uspopage ggplot(us)+geom_area(aes(x=Year,y=Thousands))+facet_wrap(~AgeGroup) ggplot(cabbage_exp) + geom_bar(aes(x=Cultivar,y=Weight,fill=Date),stat=&apos;identity&apos;,position = &quot;dodge&quot;) library(ggplot2) library(gcookbook) diamonds &lt;- diamonds set.seed(100) diamonds &lt;-diamonds[sample(nrow(diamonds),1000),] summary(diamonds) str(diamonds) head(diamonds) #names(diamonds) #价格和克拉的关系 ggplot(diamonds)+geom_point(aes(x=carat,y=price,color=color)) #价格分布 ggplot(diamonds)+geom_histogram(aes(x=price,fill=cut),position=&quot;dodge&quot;) #透明度分布 ggplot(diamonds)+geom_bar(aes(x=clarity,fill=cut)) #价格概率分布 ggplot(diamonds)+geom_density(aes(x=price,color=cut)) #不同切工下的价格分布 ggplot(diamonds)+geom_boxplot(aes(x=cut,y=price,fill=color)) #坐标变换 ggplot(diamonds)+geom_point(aes(x=carat,y=price,color=color,shape=cut))+scale_y_log10()+labs(x=&apos;克拉&apos;,y=&apos;价格&apos;,title=&apos;中文标题&apos;)]]></content>
      <categories>
        <category>R语言</category>
      </categories>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10+Tensorflow+keras+GPU环境配置]]></title>
    <url>%2F2017%2F05%2F10%2FWin10-Tensorflow-keras-GPU%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[配置：Windows 10专业版（版本10.0.15063）Intel Core i3-3420 @ 3.4GHzGeForce GTX 1060 6GBRAM 16GB 需要文件：vs2015.3.pro_chs.isocuda_8.0.61_win10.execudnn-8.0-windows10-x64-v5.0-ga.zipAnaconda3-4.2.0-Windows-x86_64.exe 1,安装VS2015或20132,安装CUDA8.0配置环境变量CUDA_HOME:C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0 添加path变量:C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\lib\x64 C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0\extras\CUPTI\libx64 3,安装cudnn解压cudnn-8.0-windows10-x64-v5.0-ga.zip复制粘贴lib,include,bin文件夹到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0下 4,安装Anaconda5,安装tensorfowconda create -n tensorflow activate tensorflow pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow- 1.1.0-cp35-cp35m-win_amd64.wh 6,安装keraspip install keras 7,我装的是VS2013(安装速度比2015快太多)装完后import tensorflow 没有显示成功加载CUDA库，但是使用的时候，执行会话使用的是GPU。所以不影响使用。]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回去吧]]></title>
    <url>%2F2017%2F05%2F06%2F%E5%9B%9E%E5%8E%BB%E5%90%A7%2F</url>
    <content type="text"><![CDATA[美丽的钱塘江静静地流轻轻的晚风吹呀吹年轻的人儿你怎么不开心别叹息现实就是这般模样离开吧没关系你已不再年少去往你该去的地方]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[要走了]]></title>
    <url>%2F2017%2F05%2F03%2F%E8%A6%81%E8%B5%B0%E4%BA%86%2F</url>
    <content type="text"><![CDATA[Episode1要去实习了，突然发现以前很讨厌的这个地方也没有那么差，可能是梓苑的菜最近不错吃的也多了。人是会变的真的，不知不觉中，或是因为某个人或是因为某件事。从来不吃香菜的一个人，后来觉得也没什么，别人能吃，你为什么不能。 Episode2选择。选择什么样的人，什么样的城市，什么样的工作…做出决定的时候是很普通的，但你回过头去看，或是多年以后回过来看现在，每个小的决定将影响你的一生，比如考研。前段时间特别纠结，其实不用，跟着内心走，希望多年以后回忆起来不是感叹当时为什么不那样，而是说这就是我最想做的事情。 Episode3女生不是追来的，是靠你的特点吸引来的。当你费尽心思的去追一个人讨好她的时候，那已经不是真正的你。如果你的能力，特质等留不住她，她凭什么要跟你。女性地位越来越高的现在，男同胞们，我们只剩下一条路，就是勇往直前，这是男人的使命，不断的努力才能把她留在身边。在一起的那刻开始就注定你不能像以前那样，恣意而为，怎么快活怎么来。 Episode4不要为别人而轻易改变自己。以前也觉得自己话太少，努力去多说话，可是自己会觉得累，后来就放弃了。自己怎么舒服就怎么来，而且发现也不是在所有场合话都少，在和别人用社交软件聊天的时候，自己话特别多。保持最真的自己，总有一天你会找到那个能理解你这一切的人。同别人交往，自己感觉舒服的同时，不要让对方觉得尴尬难受，就够了。自己平时就喜欢安静的做喜欢的事情，待朋友真诚，认真生活，看喜欢的球赛，和基友开黑，和她一起跑步…这样也很酷。 Episode5人有牵挂的时候会很脆弱。下午飞机起飞的时候抖得厉害，透过窗口往下看，脑子不知道想些什么，越想越害怕。之前都没有像今天这样的感觉。 Episode6彪哥马上毕业，最后一顿饭都没吃成。不知道下次再见会是什么时候（召唤师峡谷哈哈），祝好！]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>心情</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建博客小结]]></title>
    <url>%2F2017%2F04%2F30%2FHexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Nodejs安装，直接用命令。压缩包软链接创建会失败12345$ sudo apt-get install nodejs &amp; npm$ node -v v6.3.1$ npm -v 4.4.2 配置ssh12$ git config --global user.name "你的GitHub用户名"$ git config --global user.email "你的GitHub注册邮箱" 生成ssh密匙 &amp; 添加到github上1$ ssh-keygen -t rsa -C "你的GitHub注册邮箱" 验证是否连接成功12$ ssh -T git@github.comHi ynuwm! You've successfully authenticated, but GitHub does not provide shell access. 执行hexo d输入github用户名密码hexo d 出现错误，找不到命令1$ npm install hexo-deployer-git --save hexo博客部署到github上，冒号后面注意空格123456789# Deployment 注释## Docs: https://hexo.io/docs/deployment.htmldeploy: # 类型 type: git # 仓库 repo: git@github.com:ynuwm/ynuwm.github.io.git # 分支 branch: master 添加标签页。/sources目录下修改index.md1$ hexo new page tags 参考:https://hexo.io/http://ibruce.info/http://volcfamily.cn/https://notes.wanghao.work/http://theme-next.iissnan.com/https://www.waerfa.com/wordmarkhttps://www.zhihu.com/question/21193762http://www.cnblogs.com/zhvon/p/5351043.htmlhttp://www.cnblogs.com/zhaoyu1995/p/6239950.htmlhttps://www.zhihu.com/question/24422335/answer/46357100]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>技术</tag>
      </tags>
  </entry>
</search>